<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Mrli"><meta name="renderer" content="webkit"><meta name="copyright" content="Mrli"><meta name="keywords" content="Mrli's Blog"><meta name="description" content="想和你讲，说了会心动 ，缄默会心安。"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>机器学习——python实践.笔记 · Mr.li's Blog</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/favicon.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="https://avatars1.githubusercontent.com/u/31088082?s=400&amp;u=7a99ff83916afb3f4c5312bd78a1be17fe0e34ed&amp;v=4"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Mrli</div><div class="profile-signature">别装作很努力,<br>因为结局不会陪你演戏。</div><div class="contacts"><div>Contacts:</div><span><a href="http://sighttp.qq.com/msgrd?v=1&amp;uin=1063052964" target="_black">QQ</a></span><span><a href="https://www.cnblogs.com/nymrli/" target="_black">博客园</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 60vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">Mr.li's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">机器学习——python实践.笔记</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-edit"></i><span>2019/01/21</span></span><span class="busuanzi-pv" id="busuanzi_container_page_pv"><i class="post-intro-calendar fa fa-user-o"></i><span id="busuanzi_value_page_pv"></span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="机器学习"> 机器学习</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="Python"> Python</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">3,683</span> | Reading time: <span class="post-count">15</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><h1 id="机器学习python实践"><a class="markdownIt-Anchor" href="#机器学习python实践"></a> 机器学习——Python实践</h1>
<ul>
<li>Numpy:
<ul>
<li>python开源数值计算拓展,用来存储和处理大型矩阵,提供了许多高级的数值编程工具,如 矩阵数据类型、矢量处理、精密的运算库
<ul>
<li>利用Numpy数组来准备机器学习算法的数据</li>
</ul>
</li>
</ul>
</li>
<li>matplotlib:
<ul>
<li>python中最著名的2D绘图库,适合交互式的进行制图;也可作为绘图空间,嵌入GUI应用程序中
<ul>
<li>创建图表,展示数据</li>
</ul>
</li>
</ul>
</li>
<li>Pandas:
<ul>
<li>基于Numpy的工具,为了解决数据分析任务而创建的.~纳入了大量库和标准的数据模型,提供了操作大型数据集的工具,和快速便捷处理数据的函数和方法
<ul>
<li>导入、展示数据，以便挣钱对数据的理解和数据清洗、转换等工作</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id=""><a class="markdownIt-Anchor" href="#"></a> </h5>
<p><strong>预测模型所需的六个步骤:</strong></p>
<p><img src="/2018/11/23/机器学习——Python实践-笔记/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Python%E5%AE%9E%E8%B7%B5-%E7%AC%94%E8%AE%B0%5C%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E6%89%80%E9%9C%80%E7%9A%84%E5%85%AD%E4%B8%AA%E6%AD%A5%E9%AA%A4.jpg" alt="预测模型所需的六个步骤"></p>
<hr>
<h2 id="第一章"><a class="markdownIt-Anchor" href="#第一章"></a> 第一章：</h2>
<h3 id="鸢尾花iris-flower"><a class="markdownIt-Anchor" href="#鸢尾花iris-flower"></a> 鸢尾花(Iris Flower)</h3>
<blockquote>
<ul>
<li>
<p>所有特征数据都是数字,不需要考虑如何导入和处理数据—&gt;有的图表有标题等的,需要处理-</p>
</li>
<li>
<p>分类问题===&gt;监督学习算法</p>
</li>
<li>
<p>多分类问题,可能需要一些特殊处理</p>
</li>
<li>
<p>所有特征的数值采用相同单位,不需要进行尺度转换</p>
</li>
</ul>
</blockquote>
<p>步骤:</p>
<p><img src="/2018/11/23/机器学习——Python实践-笔记/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Python%E5%AE%9E%E8%B7%B5-%E7%AC%94%E8%AE%B0%5C%E9%A1%B9%E7%9B%AE%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4.jpg" alt="项目具体步骤"></p>
<h5 id="1导入数据集"><a class="markdownIt-Anchor" href="#1导入数据集"></a> 1.导入数据集</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'iris.data.csv'</span></span><br><span class="line">names = [<span class="string">'separ-length'</span>,<span class="string">'separ-width'</span>,<span class="string">'petal-length'</span>,<span class="string">'petal-width'</span>,<span class="string">'class'</span>]</span><br><span class="line"><span class="comment"># 花萼长度+宽度 , 花瓣长度+宽度</span></span><br><span class="line">dataset = read_csv(filename,names = names)</span><br><span class="line">print(dataset)  <span class="comment"># 150 * 5</span></span><br></pre></td></tr></table></figure>
<h5 id="2概述数据"><a class="markdownIt-Anchor" href="#2概述数据"></a> 2.概述数据</h5>
<blockquote>
<p>从下列角度审查数据:</p>
<ul>
<li>
<p>数据的维度</p>
</li>
<li>
<p>查看数据的自身</p>
</li>
<li>
<p>统计描述所有的数据特征</p>
</li>
<li>
<p>数据分类的分布情况</p>
</li>
</ul>
</blockquote>
<h6 id="1数据的维度"><a class="markdownIt-Anchor" href="#1数据的维度"></a> 1.数据的维度</h6>
<p>了解数据集中有多少行数据,数据有几个属性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'数据的维度: 行 %s , 列 %s'</span> % (dataset.shape))</span><br><span class="line"><span class="comment">#&gt;&gt;&gt;数据的维度: 行 150 , 列 5</span></span><br></pre></td></tr></table></figure>
<h6 id="2参看数据本身"><a class="markdownIt-Anchor" href="#2参看数据本身"></a> 2.参看数据本身</h6>
<p>直观的看到数据的特征,数据的类型,以及大概的数据分布范围</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(dataset.head(<span class="number">5</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">   separ-length  separ-width     ...       petal-width        <span class="class"><span class="keyword">class</span></span></span><br><span class="line"><span class="class">0           5.1          3.5     ...               0.2  <span class="title">Iris</span>-<span class="title">setosa</span></span></span><br><span class="line"><span class="class">1           4.9          3.0     ...               0.2  <span class="title">Iris</span>-<span class="title">setosa</span></span></span><br><span class="line"><span class="class">2           4.7          3.2     ...               0.2  <span class="title">Iris</span>-<span class="title">setosa</span></span></span><br><span class="line"><span class="class">3           4.6          3.1     ...               0.2  <span class="title">Iris</span>-<span class="title">setosa</span></span></span><br><span class="line"><span class="class">4           5.0          3.6     ...               0.2  <span class="title">Iris</span>-<span class="title">setosa</span></span></span><br></pre></td></tr></table></figure>
<h6 id="3统计描述数据"><a class="markdownIt-Anchor" href="#3统计描述数据"></a> 3.统计描述数据</h6>
<p>数据特征的统计描述信息包括数据的<code>行数</code>、<code>中位值</code>、<code>最大值</code>、<code>最小值</code>、<code>均值</code>、<code>四分位值</code>等统计数据信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">print(dataset.describe())</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">       separ-length  separ-width  petal-length  petal-width</span><br><span class="line">count    <span class="number">150.000000</span>   <span class="number">150.000000</span>    <span class="number">150.000000</span>   <span class="number">150.000000</span></span><br><span class="line">mean       <span class="number">5.843333</span>     <span class="number">3.054000</span>      <span class="number">3.758667</span>     <span class="number">1.198667</span></span><br><span class="line">std        <span class="number">0.828066</span>     <span class="number">0.433594</span>      <span class="number">1.764420</span>     <span class="number">0.763161</span></span><br><span class="line">min        <span class="number">4.300000</span>     <span class="number">2.000000</span>      <span class="number">1.000000</span>     <span class="number">0.100000</span></span><br><span class="line"><span class="number">25</span>%        <span class="number">5.100000</span>     <span class="number">2.800000</span>      <span class="number">1.600000</span>     <span class="number">0.300000</span></span><br><span class="line"><span class="number">50</span>%        <span class="number">5.800000</span>     <span class="number">3.000000</span>      <span class="number">4.350000</span>     <span class="number">1.300000</span></span><br><span class="line"><span class="number">75</span>%        <span class="number">6.400000</span>     <span class="number">3.300000</span>      <span class="number">5.100000</span>     <span class="number">1.800000</span></span><br><span class="line">max        <span class="number">7.900000</span>     <span class="number">4.400000</span>      <span class="number">6.900000</span>     <span class="number">2.500000</span></span><br></pre></td></tr></table></figure>
<h6 id="4数据分类分布"><a class="markdownIt-Anchor" href="#4数据分类分布"></a> 4.数据分类分布</h6>
<p>了解数据在不同分类的分布情况…==&gt;每个分类数据量的绝对数值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(dataset.groupby(<span class="string">'class'</span>).size())</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span></span></span><br><span class="line"><span class="class"><span class="title">Iris</span>-<span class="title">setosa</span>        50</span></span><br><span class="line"><span class="class"><span class="title">Iris</span>-<span class="title">versicolor</span>    50</span></span><br><span class="line"><span class="class"><span class="title">Iris</span>-<span class="title">virginica</span>     50</span></span><br><span class="line"><span class="class"><span class="title">dtype</span>:</span> int64</span><br></pre></td></tr></table></figure>
<p>▲ 如果数据分布不平衡,可能会影响到模型的准确性,…==&gt;不平衡时,需要对数据进行调整,方法有:</p>
<ul>
<li>扩大数据样本
<ul>
<li>通常容易被忽略的选择…但往往找到更大的数据集就有可能挖掘出更平衡的方面提高算法准确度</li>
</ul>
</li>
<li>数据的重新抽样
<ul>
<li>过抽样(复制少数类样本)…数据少时考虑</li>
<li>欠抽样(删除多数类样本)…数据多时考虑</li>
</ul>
</li>
<li>尝试生成人工样本
<ul>
<li>从少数类的实例中随机抽样特征属性,生成更多数据</li>
</ul>
</li>
<li>异常检测和变化检测
<ul>
<li>尝试从不同观点思考,异常检测是对罕见事件的检测,将小类作为异常值类</li>
</ul>
</li>
</ul>
<h4 id="3数据可视化"><a class="markdownIt-Anchor" href="#3数据可视化"></a> 3.数据可视化</h4>
<blockquote>
<p>单变量图表: 理解每一个特征属性</p>
<p>多变量图表: 理解不同特征属性之间的关系</p>
</blockquote>
<h5 id="单变量图"><a class="markdownIt-Anchor" href="#单变量图"></a> 单变量图:</h5>
<blockquote>
<p>箱线图: 一种用作显示一组数据分散情况资料的统计图。因形状如箱子而得名。</p>
<p>主要用于反映原始数据分布的特征，还可以进行多组数据分布特征的比 较。箱线图的绘制方法是：先找出一组数据的最大值、最小值、中位数和两个四分位数；然后， 连接两个四分位数画出箱子；再将最大值和最小值与箱子相连接，中位数在箱子中间。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#箱线图 , 因为每个特征属性都是数字 , 所以 用箱线图展示 属性与中位值的离散程度</span></span><br><span class="line">dataset.plot(kind=<span class="string">'box'</span>,subplots = <span class="keyword">True</span>,layout = (<span class="number">2</span>,<span class="number">2</span>) , sharex = <span class="keyword">False</span> , sharey = <span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 参数说明 : box箱线 , subplots 允许多个子图, layout 布局为2*2 , sharex.sharey 不共享x,y</span></span><br><span class="line">pyplot.show()</span><br><span class="line"><span class="comment">#---</span></span><br><span class="line"><span class="comment">#直方图 , x轴为值 , y轴为数量</span></span><br><span class="line">dataset.hist()</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure>
<h5 id="多变量图"><a class="markdownIt-Anchor" href="#多变量图"></a> 多变量图:</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas.plotting <span class="keyword">import</span> scatter_matrix</span><br><span class="line"><span class="comment">#散点矩阵图</span></span><br><span class="line">scatter_matrix(dataset)</span><br><span class="line">pyplot.show()</span><br><span class="line"><span class="comment">#pyplot.savefig("scatter_matrix.png")</span></span><br></pre></td></tr></table></figure>
<p><img src="/2018/11/23/机器学习——Python实践-笔记/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Python%E5%AE%9E%E8%B7%B5-%E7%AC%94%E8%AE%B0%5Cscatter_matrix.png" alt="scatter_matrix"></p>
<h4 id="4评估算法"><a class="markdownIt-Anchor" href="#4评估算法"></a> 4.评估算法</h4>
<blockquote>
<ul>
<li>分离出评估数据集</li>
<li>采用<strong>10折交叉验证</strong>来评估算法模型</li>
<li>生成6个不同的模型来预测新数据</li>
<li>选择最优模型</li>
</ul>
</blockquote>
<h5 id="1分离出评估数据集"><a class="markdownIt-Anchor" href="#1分离出评估数据集"></a> 1.分离出评估数据集</h5>
<p>❤️要想知道算法模型对真是数据的准确度,所以保留一部分数据来评估算法模型.</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">array = dataset.values</span><br><span class="line">X = array[:,<span class="number">0</span>:<span class="number">4</span>]    <span class="comment">#除了最后的class类别</span></span><br><span class="line">Y = array[:,<span class="number">4</span>]      <span class="comment">#类别,目标</span></span><br><span class="line">validation_size = <span class="number">0.2</span>   <span class="comment">#验证比例</span></span><br><span class="line">seed = <span class="number">7</span>                <span class="comment">#随机种子</span></span><br><span class="line">X_train , X_validation , Y_train , Y_validation = \</span><br><span class="line">    train_test_split(X,Y,test_size=validation_size,random_state=seed)</span><br></pre></td></tr></table></figure>
<h5 id="2评估模式"><a class="markdownIt-Anchor" href="#2评估模式"></a> 2.评估模式</h5>
<p>采用10折交叉验证来分离训练数据集 :</p>
<blockquote>
<p>随机将数据分成10份,9份用来训练模型,1份用来评估算法</p>
</blockquote>
<h5 id="3创建模型"><a class="markdownIt-Anchor" href="#3创建模型"></a> 3.创建模型</h5>
<p><strong>线性</strong></p>
<ul>
<li>线性回归(LR)</li>
<li>线性判别分析(LDA)</li>
</ul>
<p><strong>非线性</strong></p>
<ul>
<li>K近邻(KNN)</li>
<li>分类与回归树(CART)</li>
<li>贝叶斯分类器(NB)</li>
<li>支持向量机(SVM)</li>
</ul>
<p>▲ 在每次对算法进行评估前都会重新设置随机数种子,以保证每次对算法的评估都是用相同的数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression,LinearRegression <span class="comment">#LR</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier                      <span class="comment">#CART</span></span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis <span class="comment">#LDA</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB                           <span class="comment">#NB</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier                   <span class="comment">#KNN</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC                                          <span class="comment">#SVM</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold,cross_val_score</span><br><span class="line">models = &#123;&#125;</span><br><span class="line">models[<span class="string">'LR'</span>] = LogisticRegression()</span><br><span class="line">models[<span class="string">'LDA'</span>] = LinearDiscriminantAnalysis()</span><br><span class="line">models[<span class="string">'CART'</span>] = DecisionTreeClassifier()</span><br><span class="line">models[<span class="string">'NB'</span>] = GaussianNB()</span><br><span class="line">models[<span class="string">'KNN'</span>] = KNeighborsClassifier()</span><br><span class="line">models[<span class="string">'SVM'</span>] = SVC()</span><br><span class="line"><span class="comment">#评估算法</span></span><br><span class="line">resutls  = []</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> models:</span><br><span class="line">    kfold = KFold(n_splits=<span class="number">10</span>,random_state=seed)</span><br><span class="line">    cv_result  = cross_val_score(models[key],X_train,Y_train,cv = kfold , scoring = <span class="string">'accuracy'</span>)</span><br><span class="line">    <span class="comment">#cross_val_score将交叉验证的整个过程连接起来，不用再进行手动的分割数据, cv参数用于规定将原始数据分成多少份</span></span><br><span class="line">    resutls.append(cv_result)</span><br><span class="line">    print(<span class="string">"%s: %f (%f)"</span> % (key,cv_result.mean(),cv_result.std()))	<span class="comment">#均值,标准差</span></span><br></pre></td></tr></table></figure>
<h5 id="4选择最优模型"><a class="markdownIt-Anchor" href="#4选择最优模型"></a> 4.选择最优模型</h5>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;</span></span><br><span class="line">LR: 0.966667 (0.040825)</span><br><span class="line">LDA: 0.975000 (0.038188)</span><br><span class="line">CART: 0.966667 (0.040825)</span><br><span class="line">NB: 0.975000 (0.053359)</span><br><span class="line">KNN: 0.983333 (0.033333)</span><br><span class="line">SVM: 0.991667 (0.025000)</span><br></pre></td></tr></table></figure>
<h5 id="为什么使用-10折交叉验证"><a class="markdownIt-Anchor" href="#为什么使用-10折交叉验证"></a> 为什么使用 10折交叉验证?</h5>
<p>进行模型验证的一个重要目的是要选出一个最合适的模型，对于监督学习而言，我们希望模型对于未知数据的<u>泛化能力</u>强，所以就需要模型验证这一过程来体现不同的模型对于未知数据的表现效果。</p>
<h6 id="训练准确度测试准确度"><a class="markdownIt-Anchor" href="#训练准确度测试准确度"></a> 训练准确度==&gt;测试准确度</h6>
<p>最先我们用<strong>训练准确度</strong>（用全部数据进行训练和测试）来衡量模型的表现，这种方法会导致模型<u>过拟合(方差大)</u>；===&gt;&gt;为了解决这一问题，我们将所有数据分成训练集和测试集两部分，我们用<code>训练集</code>进行模型训练，得到的模型再用<code>测试集</code>来衡量模型的预测表现能力，这种度量方式叫<strong>测试准确度</strong>，这种方式可以有效避免过拟合。</p>
<h6 id="测试准确度10折交叉验证"><a class="markdownIt-Anchor" href="#测试准确度10折交叉验证"></a> 测试准确度==&gt;10折交叉验证</h6>
<p><strong>测试准确度的一个缺点</strong>是其样本准确度是一个<strong>高方差估计</strong>（high variance estimate）, 所以该样本准确度会依赖不同的测试集，其表现效果不尽相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1</span>,<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"random_state is "</span>, i,<span class="string">", and accuracy score is:"</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=i)</span><br><span class="line"></span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line">    knn.fit(X_train, y_train)</span><br><span class="line">    y_pred = knn.predict(X_test)</span><br><span class="line">    <span class="keyword">print</span> metrics.accuracy_score(y_test, y_pred)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">random_state <span class="keyword">is</span>  <span class="number">1</span> , <span class="keyword">and</span> accuracy score <span class="keyword">is</span>:<span class="number">1.0</span></span><br><span class="line">random_state <span class="keyword">is</span>  <span class="number">2</span> , <span class="keyword">and</span> accuracy score <span class="keyword">is</span>:<span class="number">1.0</span></span><br><span class="line">random_state <span class="keyword">is</span>  <span class="number">3</span> , <span class="keyword">and</span> accuracy score <span class="keyword">is</span>:<span class="number">0.947368421053</span></span><br><span class="line">random_state <span class="keyword">is</span>  <span class="number">4</span> , <span class="keyword">and</span> accuracy score <span class="keyword">is</span>:<span class="number">0.973684210526</span></span><br></pre></td></tr></table></figure>
<p>上面的测试准确率可以看出，<u>不同的训练集、测试集分割的方法</u>导致其准确率不同，而交叉验证的基本思想是：<u>1.将数据集进行一系列分割，生成一组不同的训练测试集</u>，<u>2.然后分别训练模型并计算测试准确率</u>，<u>3.最后对结果进行<strong>平均处理</strong></u>。这样来有效降低测试准确率的差异。</p>
<h5 id="k折交叉验证"><a class="markdownIt-Anchor" href="#k折交叉验证"></a> K折交叉验证:</h5>
<ol>
<li>将数据集平均分割成K个等份子集</li>
<li>使用1份数据(子集)作为测试数据，其余(K-1)份作为训练数据</li>
<li>计算测试准确率</li>
<li>使用不同的测试集，重复2、3步骤</li>
<li>对测试准确率做<strong>平均</strong>，作为对未知数据预测准确率的估计  ==&gt; <code>cross_val_score.mean()</code></li>
</ol>
<blockquote>
<p>不同的训练集、测试集分割的方法导致其准确率不同，而交叉验证的基本思想是：将数据集进行一系列分割，生成一组不同的训练测试集，然后分别训练模型并计算测试准确率，最后对结果进行平均处理。这样来有效降低测试准确率的差异。</p>
</blockquote>
<p><img src="/2018/11/23/机器学习——Python实践-笔记/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Python%E5%AE%9E%E8%B7%B5-%E7%AC%94%E8%AE%B0%5C%E6%95%B0%E5%AD%A6%E6%A6%82%E5%BF%B5%5CKFold.png" alt="KFold"></p>
<p>来自周志华&lt;&lt;机器学习&gt;&gt;:</p>
<p><img src="/2018/11/23/机器学习——Python实践-笔记/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Python%E5%AE%9E%E8%B7%B5-%E7%AC%94%E8%AE%B0%5C%E6%95%B0%E5%AD%A6%E6%A6%82%E5%BF%B5%5CK%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E6%B3%95.jpg" alt="K交叉验证法"></p>
<h6 id="分割方法"><a class="markdownIt-Anchor" href="#分割方法"></a> 分割方法</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面代码演示了K-fold交叉验证是如何进行数据分割的</span></span><br><span class="line"><span class="comment"># simulate splitting a dataset of 25 observations into 5 folds</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> KFold</span><br><span class="line">kf = KFold(<span class="number">25</span>, n_folds=<span class="number">5</span>, shuffle=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cv_result  = cross_val_score(models[key],X_train,Y_train,cv = <span class="number">10</span> , scoring = <span class="string">'accuracy'</span>)</span><br><span class="line"><span class="comment">#cross_val_score将交叉验证的整个过程连接起来，不用再进行手动的分割数据, cv参数用于规定将原始数据分成多少份</span></span><br></pre></td></tr></table></figure>
<h5 id="5实施预测"><a class="markdownIt-Anchor" href="#5实施预测"></a> 5.实施预测</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用评估数据集评估算法</span></span><br><span class="line">svm = SVC()</span><br><span class="line">svm.fit(X = X_train ,y = Y_train)		<span class="comment">#参数为 X , y</span></span><br><span class="line">predictions = svm.predict(X_validation)</span><br><span class="line">print(accuracy_score(Y_validation,predictions))		<span class="comment">#测试集结果 与 预测结果 相比</span></span><br><span class="line">print(confusion_matrix(Y_validation,predictions))</span><br><span class="line">print(classification_report(Y_validation,predictions))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="number">0.9333333333333333</span></span><br><span class="line">[[ <span class="number">7</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"> [ <span class="number">0</span> <span class="number">10</span>  <span class="number">2</span>]		<span class="comment">#混淆矩阵只出现了两个错误,2</span></span><br><span class="line"> [ <span class="number">0</span>  <span class="number">0</span> <span class="number">11</span>]]</span><br><span class="line">                 precision(精确度)    recall(召回率)  f1-score(F1值)   support(总和)</span><br><span class="line"></span><br><span class="line">    Iris-setosa       <span class="number">1.00</span>      	<span class="number">1.00</span>      		<span class="number">1.00</span>         	<span class="number">7</span></span><br><span class="line">Iris-versicolor       <span class="number">1.00</span>      	<span class="number">0.83</span>      		<span class="number">0.91</span>        	<span class="number">12</span></span><br><span class="line"> Iris-virginica       <span class="number">0.85</span>      	<span class="number">1.00</span>      		<span class="number">0.92</span>        	<span class="number">11</span></span><br><span class="line"></span><br><span class="line">    avg / total       <span class="number">0.94</span>      	<span class="number">0.93</span>      		<span class="number">0.93</span>        	<span class="number">30</span></span><br></pre></td></tr></table></figure>
<h6 id="召回率recall-rate也叫查全率"><a class="markdownIt-Anchor" href="#召回率recall-rate也叫查全率"></a> 召回率(Recall Rate,也叫<a href="https://baike.baidu.com/item/%E6%9F%A5%E5%85%A8%E7%8E%87" target="_blank" rel="noopener">查全率</a>)</h6>
<blockquote>
<p>是<u>检索出的<strong>相关文档数</strong></u>和<u>文档库中所有的<strong>相关</strong>文档数</u>的比率，衡量的是检索系统的<a href="https://baike.baidu.com/item/%E6%9F%A5%E5%85%A8%E7%8E%87" target="_blank" rel="noopener">查全率</a>；</p>
</blockquote>
<h6 id="精度precise"><a class="markdownIt-Anchor" href="#精度precise"></a> 精度(Precise)</h6>
<blockquote>
<p>是<u>检索出的<strong>相关文档数</strong></u>与<u>检索出的文档<strong>总数</strong></u>的比率，衡量的是<a href="https://baike.baidu.com/item/%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F" target="_blank" rel="noopener">检索系统</a>的<a href="https://baike.baidu.com/item/%E6%9F%A5%E5%87%86%E7%8E%87" target="_blank" rel="noopener">查准率</a>。</p>
</blockquote>
<ul>
<li>
<p>TP: 预测为正，实际为正		(第一个是实际T或F,第二个是预测P或N)</p>
</li>
<li>
<p>FP: 预测为正，实际为负</p>
</li>
<li>
<p>TN:预测为负，实际为负</p>
</li>
<li>
<p>FN: 预测为负，实际为正</p>
</li>
</ul>
<p>精确率、准确率：<code>Accuracy=(TP+TN)/(TP+TN+FN+FP)</code></p>
<p>//精准率、查准率：<code>P = TP/ (TP+FP)</code></p>
<p>召回率、查全率：<code>R = TP/ (TP+FN)</code></p>
<p><code>F1-score</code>: <code>2*TP/(2*TP + FP + FN)</code></p>
<p>◆. 精确度是“搜索结果有多大用处”，而召回是“结果如何完整”。</p>
<hr>
<h6 id="f1分数"><a class="markdownIt-Anchor" href="#f1分数"></a> F1分数:</h6>
<blockquote>
<p>概述 : 统计学中用来衡量<strong>二分类模型精确度</strong>的一种指标。它同时兼顾了分类模型的<a href="https://baike.baidu.com/item/%E5%87%86%E7%A1%AE%E7%8E%87/5165407" target="_blank" rel="noopener">准确率</a>和<a href="https://baike.baidu.com/item/%E5%8F%AC%E5%9B%9E%E7%8E%87/560642" target="_blank" rel="noopener">召回率</a>。F1分数可以看作是模型<a href="https://baike.baidu.com/item/%E5%87%86%E7%A1%AE%E7%8E%87/5165407" target="_blank" rel="noopener">准确率</a>和<a href="https://baike.baidu.com/item/%E5%8F%AC%E5%9B%9E%E7%8E%87/560642" target="_blank" rel="noopener">召回率</a>的一种加权平均，它的最大值是1，最小值是0。</p>
</blockquote>
<p>人们通常使用准确率和召回率这两个指标，来评价<strong>二分类模型</strong>的分析效果。</p>
<p>但是当这两个指标发生冲突时，我们很难在模型之间进行比较。比如，我们有如下两个模型A、B，A模型的召回率高于B模型，但是B模型的准确率高于A模型，A和B这两个模型的综合性能，哪一个更优呢？</p>
<table>
<thead>
<tr>
<th></th>
<th>准确率</th>
<th>召回率</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>80%</td>
<td>90%</td>
</tr>
<tr>
<td>B</td>
<td>90%</td>
<td>80%</td>
</tr>
</tbody>
</table>
<p>为了解决这个问题，人们提出了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mrow><mi>β</mi></mrow></msub></mrow><annotation encoding="application/x-tex">F_{\beta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> 分数。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mrow><mi>β</mi></mrow></msub></mrow><annotation encoding="application/x-tex">F_{\beta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>的物理意义就是将准确率和召回率这两个分值合并为一个分值，在合并的过程中，召回率的权重是准确率的  <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span>倍。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mrow><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">F_{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>分数认为召回率和准确率<strong>同等重要</strong>， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mrow><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">F_{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> 分数认为<strong>召回率的重要程度是准确率的2倍</strong>，而分<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mrow><mn>0</mn><mi mathvariant="normal">.</mi><mn>5</mn></mrow></msub></mrow><annotation encoding="application/x-tex">F_{0.5}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">5</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>数认为<strong>召回率的重要程度是准确率的一半</strong>。</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mrow><mi>β</mi></mrow></msub><mo>=</mo><mrow><mo fence="true">(</mo><mn>1</mn><mo>+</mo><msup><mi>β</mi><mrow><mn>2</mn></mrow></msup><mo fence="true">)</mo></mrow><mo>⋅</mo><mfrac><mrow><mtext><mtext> </mtext><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi><mtext> </mtext><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">l</mi></mtext></mrow><mrow><mrow><mo fence="true">(</mo><msup><mi>β</mi><mrow><mn>2</mn></mrow></msup><mo>⋅</mo><mtext><mtext> </mtext><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi><mtext> </mtext></mtext><mo fence="true">)</mo></mrow><mo>+</mo><mtext><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">l</mi></mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">F _ { \beta } = \left( 1 + \beta ^ { 2 } \right) \cdot \frac { \text { precision recall} } { \left( \beta ^ { 2 } \cdot \text { precision } \right) + \text {recall} }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.9322159999999999em;"></span><span class="strut bottom" style="height:1.452216em;vertical-align:-0.52em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="minner textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord mathrm">1</span><span class="mbin">+</span><span class="mord"><span class="mord mathit" style="margin-right:0.05278em;">β</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">2</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mbin">⋅</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.34500000000000003em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0.7em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="minner scriptstyle cramped"><span class="style-wrap reset-scriptstyle scriptstyle uncramped" style="top:0em;">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.05278em;">β</span><span class="vlist"><span style="top:-0.289em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">⋅</span><span class="text mord scriptstyle cramped"><span class="mord mspace"> </span><span class="mord mathrm">p</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">c</span><span class="mord mathrm">i</span><span class="mord mathrm">s</span><span class="mord mathrm">i</span><span class="mord mathrm">o</span><span class="mord mathrm">n</span><span class="mord mspace"> </span></span><span class="style-wrap reset-scriptstyle scriptstyle uncramped" style="top:0em;">)</span></span><span class="mbin">+</span><span class="text mord scriptstyle cramped"><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">c</span><span class="mord mathrm">a</span><span class="mord mathrm">l</span><span class="mord mathrm">l</span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0.7em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.44610799999999995em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0.7em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="text mord scriptstyle uncramped"><span class="mord mspace"> </span><span class="mord mathrm">p</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">c</span><span class="mord mathrm">i</span><span class="mord mathrm">s</span><span class="mord mathrm">i</span><span class="mord mathrm">o</span><span class="mord mathrm">n</span><span class="mord mspace"> </span><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">c</span><span class="mord mathrm">a</span><span class="mord mathrm">l</span><span class="mord mathrm">l</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0.7em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi><mo>=</mo><mfrac><mrow><mtext><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">l</mi><mtext> </mtext><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi></mtext></mrow><mrow><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">y</mi><mtext> </mtext><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi></mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">\beta = \frac { \text {recall rate} } { \text {accuracy rate} }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8801079999999999em;"></span><span class="strut bottom" style="height:1.3612159999999998em;vertical-align:-0.481108em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05278em;">β</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">a</span><span class="mord mathrm">c</span><span class="mord mathrm">c</span><span class="mord mathrm">u</span><span class="mord mathrm">r</span><span class="mord mathrm">a</span><span class="mord mathrm">c</span><span class="mord mathrm" style="margin-right:0.01389em;">y</span><span class="mord mspace"> </span><span class="mord mathrm">r</span><span class="mord mathrm">a</span><span class="mord mathrm">t</span><span class="mord mathrm">e</span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="text mord scriptstyle uncramped"><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">c</span><span class="mord mathrm">a</span><span class="mord mathrm">l</span><span class="mord mathrm">l</span><span class="mord mspace"> </span><span class="mord mathrm">r</span><span class="mord mathrm">a</span><span class="mord mathrm">t</span><span class="mord mathrm">e</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></p>
<p>▲ 如何计算<code>Precise</code>、<code>Recall</code>、<code>F1-score</code>见博客https://blog.csdn.net/akadiao/article/details/78788864</p>
<hr>
<h3 id="confusion_martrix混淆矩阵"><a class="markdownIt-Anchor" href="#confusion_martrix混淆矩阵"></a> Confusion_martrix(混淆矩阵)</h3>
<p>混淆矩阵: 一种特定的矩阵用来呈现算法性能的可视化效果，通常是监督学习（非监督学习，通常用匹配矩阵：matching matrix）</p>
<blockquote>
<p>其每一列代表预测值，每一行代表的是实际的类别。这个名字来源于它可以非常容易的表明多个类别是否有混淆（也就是一个class被预测成另一个class）。</p>
</blockquote>
<h4 id="example样例说明"><a class="markdownIt-Anchor" href="#example样例说明"></a> Example样例说明:</h4>
<p>假设有一个用来对<code>猫（cats）、狗（dogs）、兔子（rabbits）</code>进行分类的系统，混淆矩阵就是为了进一步分析性能而对该算法测试结果做出的总结。假设总共有 <strong>27</strong> 只动物：8只猫， 6条狗， 13只兔子。结果的混淆矩阵如下图：</p>
<p><img src="/2018/11/23/机器学习——Python实践-笔记/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Python%E5%AE%9E%E8%B7%B5-%E7%AC%94%E8%AE%B0%5C%E6%95%B0%E5%AD%A6%E6%A6%82%E5%BF%B5%5Cconfusion_matrix.png" alt="confusion_matrix"></p>
<p>在这个混淆矩阵中，实际有 8只猫，但是系统将其中3只预测成了狗；对于 6条狗，其中有 1条被预测成了兔子，2条被预测成了猫。从混淆矩阵中我们可以看出系统对于区分猫和狗存在一些问题，但是<strong>区分兔子和其他动物的效果还是不错的</strong>。<strong><u>所有正确的预测结果都在对角线上</u></strong>，所以从混淆矩阵中可以很方便直观的看出哪里有错误，因为他们呈现在对角线外面。****</p>
</article><!-- lincense--><div class="license-wrapper"><p> <span>Author:  </span><a href="https://nymrli.top">Mrli</a></p><p> <span>Link:  </span><a href="https://nymrli.top/2018/11/23/机器学习——Python实践-笔记/">https://nymrli.top/2018/11/23/机器学习——Python实践-笔记/</a></p><p> <span>Copyright:  </span><span>All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/3.0">CC BY-NC-SA 3.0</a> unless stating additionally.</span></p></div><div class="post-paginator"><a class="prevSlogan" href="/2018/11/24/机器学习——数学概念/" title="机器学习——数学概念"><span>< PreviousPost</span><br><span class="prevTitle">机器学习——数学概念</span></a><a class="nextSlogan" href="/2018/11/23/Pycharm里无法查看Sqlite数据表/" title="Pycharm里无法查看Sqlite数据表"><span>NextPost ></span><br><span class="nextTitle">Pycharm里无法查看Sqlite数据表</span></a><div class="clear"></div></div><div id="comment"><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  id: '机器学习——python实践.笔记',
  owner: 'Freedomisgood',
  repo: 'Freedomisgood.github.io',
  oauth: {
    client_id: 'bc5a81fe36017dcd8b63',
    client_secret: '949cec3a1b91742c6249c47259791e4b80a6fa69',
  },
})
gitment.render('container')</script></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 60vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#机器学习python实践"><span class="toc-number">1.</span> <span class="toc-text"> 机器学习——Python实践</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#"><span class="toc-number">1.0.0.0.1.</span> <span class="toc-text"> </span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#第一章"><span class="toc-number">1.1.</span> <span class="toc-text"> 第一章：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#鸢尾花iris-flower"><span class="toc-number">1.1.1.</span> <span class="toc-text"> 鸢尾花(Iris Flower)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1导入数据集"><span class="toc-number">1.1.1.0.1.</span> <span class="toc-text"> 1.导入数据集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2概述数据"><span class="toc-number">1.1.1.0.2.</span> <span class="toc-text"> 2.概述数据</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1数据的维度"><span class="toc-number">1.1.1.0.2.1.</span> <span class="toc-text"> 1.数据的维度</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2参看数据本身"><span class="toc-number">1.1.1.0.2.2.</span> <span class="toc-text"> 2.参看数据本身</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3统计描述数据"><span class="toc-number">1.1.1.0.2.3.</span> <span class="toc-text"> 3.统计描述数据</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#4数据分类分布"><span class="toc-number">1.1.1.0.2.4.</span> <span class="toc-text"> 4.数据分类分布</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3数据可视化"><span class="toc-number">1.1.1.1.</span> <span class="toc-text"> 3.数据可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#单变量图"><span class="toc-number">1.1.1.1.1.</span> <span class="toc-text"> 单变量图:</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#多变量图"><span class="toc-number">1.1.1.1.2.</span> <span class="toc-text"> 多变量图:</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4评估算法"><span class="toc-number">1.1.1.2.</span> <span class="toc-text"> 4.评估算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1分离出评估数据集"><span class="toc-number">1.1.1.2.1.</span> <span class="toc-text"> 1.分离出评估数据集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2评估模式"><span class="toc-number">1.1.1.2.2.</span> <span class="toc-text"> 2.评估模式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3创建模型"><span class="toc-number">1.1.1.2.3.</span> <span class="toc-text"> 3.创建模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4选择最优模型"><span class="toc-number">1.1.1.2.4.</span> <span class="toc-text"> 4.选择最优模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#为什么使用-10折交叉验证"><span class="toc-number">1.1.1.2.5.</span> <span class="toc-text"> 为什么使用 10折交叉验证?</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#训练准确度测试准确度"><span class="toc-number">1.1.1.2.5.1.</span> <span class="toc-text"> 训练准确度==&gt;测试准确度</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#测试准确度10折交叉验证"><span class="toc-number">1.1.1.2.5.2.</span> <span class="toc-text"> 测试准确度==&gt;10折交叉验证</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#k折交叉验证"><span class="toc-number">1.1.1.2.6.</span> <span class="toc-text"> K折交叉验证:</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#分割方法"><span class="toc-number">1.1.1.2.6.1.</span> <span class="toc-text"> 分割方法</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5实施预测"><span class="toc-number">1.1.1.2.7.</span> <span class="toc-text"> 5.实施预测</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#召回率recall-rate也叫查全率"><span class="toc-number">1.1.1.2.7.1.</span> <span class="toc-text"> 召回率(Recall Rate,也叫查全率)</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#精度precise"><span class="toc-number">1.1.1.2.7.2.</span> <span class="toc-text"> 精度(Precise)</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#f1分数"><span class="toc-number">1.1.1.2.7.3.</span> <span class="toc-text"> F1分数:</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#confusion_martrix混淆矩阵"><span class="toc-number">1.1.2.</span> <span class="toc-text"> Confusion_martrix(混淆矩阵)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#example样例说明"><span class="toc-number">1.1.2.1.</span> <span class="toc-text"> Example样例说明:</span></a></li></ol></li></ol></li></ol></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>