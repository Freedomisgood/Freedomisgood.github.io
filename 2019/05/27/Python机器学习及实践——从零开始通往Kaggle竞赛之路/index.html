<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Mrli"><meta name="renderer" content="webkit"><meta name="copyright" content="Mrli"><meta name="keywords" content="Mrli's Blog"><meta name="description" content="想和你讲，说了会心动 ，缄默会心安。"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>Python机器学习及实践——从零开始通往Kaggle竞赛之路 · Mr.li's Blog</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/favicon.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="https://avatars1.githubusercontent.com/u/31088082?s=400&amp;u=7a99ff83916afb3f4c5312bd78a1be17fe0e34ed&amp;v=4"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Mrli</div><div class="profile-signature">别装作很努力,<br>因为结局不会陪你演戏。</div><div class="contacts"><div>Contacts:</div><span><a href="http://sighttp.qq.com/msgrd?v=1&amp;uin=1063052964" target="_black">QQ</a></span><span><a href="https://www.cnblogs.com/nymrli/" target="_black">博客园</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 60vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">Mr.li's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">Python机器学习及实践——从零开始通往Kaggle竞赛之路</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-edit"></i><span>2019/09/15</span></span><span class="busuanzi-pv" id="busuanzi_container_page_pv"><i class="post-intro-calendar fa fa-user-o"></i><span id="busuanzi_value_page_pv"></span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="机器学习"> 机器学习</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="Python"> Python</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">1,523</span> | Reading time: <span class="post-count">6</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><h1 id="Python机器学习及实践——从零开始通往Kaggle竞赛之路"><a href="#Python机器学习及实践——从零开始通往Kaggle竞赛之路" class="headerlink" title="Python机器学习及实践——从零开始通往Kaggle竞赛之路"></a>Python机器学习及实践——从零开始通往Kaggle竞赛之路</h1><h1 id="监督学习模型"><a href="#监督学习模型" class="headerlink" title="监督学习模型"></a>监督学习模型</h1><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><h4 id="Logistic"><a href="#Logistic" class="headerlink" title="Logistic"></a>Logistic</h4><p>逻辑回归函数 ： $g(z)=\frac {1}{1+_e^{-z}} $</p>
<p>逻辑回归模型$h_{w,b}(x)=g(f(w,x,b)) = \frac{1}{1+e^{-z}} = \frac {1}{1+e^{-(w^{T}x+b)}}$</p>
<h5 id="处理缺省值的方法："><a href="#处理缺省值的方法：" class="headerlink" title="处理缺省值的方法："></a>处理缺省值的方法：</h5><p>1.缺省值较少时直接删除数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = data.replace(to_replace=<span class="string">'? '</span> ,value = np.nan)</span><br><span class="line">data = data.dropna(how=<span class="string">'any'</span>)</span><br></pre></td></tr></table></figure>
<h5 id="分割数据"><a href="#分割数据" class="headerlink" title="分割数据"></a>分割数据</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train , Y_train , X_test , Y_test = train_test_split(data[names[<span class="number">1</span>:<span class="number">10</span>]],data[names[<span class="number">10</span>]],test_size = <span class="number">0.25</span>,random_state = <span class="number">33</span>)</span><br><span class="line"><span class="comment"># 参数说明 ： $1X，$2Y，$3分割规模，$4分割种子</span></span><br></pre></td></tr></table></figure>
<h5 id="标准化数据"><a href="#标准化数据" class="headerlink" title="标准化数据"></a>标准化数据</h5><blockquote>
<p>保证每个维度的特征数据方差为1，均值为0，使得预测结果不回被某些维度过大的特征值主导</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">s = StandardScaler()</span><br><span class="line">X_train = s.fit_transform(X_train)</span><br><span class="line">X_test = s.transform(X_test)</span><br></pre></td></tr></table></figure>
<h5 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h5><blockquote>
<p>….</p>
</blockquote>
<p>:dart:逻辑回归对参数的计算采用精确解析的方式，时间长性能高；SGD（随机梯度上升）分类估计模型参数，时间短性能低。数据规模在10W量级时考虑对时间的消耗，更推荐使用随机梯度方法对模型参数进行估计。</p>
<p>:memo:说明用例：良恶性肿瘤</p>
<h4 id="支持向量机（​​分类）"><a href="#支持向量机（​​分类）" class="headerlink" title="支持向量机（​​分类）"></a>支持向量机（​​分类）</h4><p>决定直线位置的并不是所有数据，而是其中 <em>两个空间间隔最小</em> 的 <em>两个不同类别</em> 的数据点，把着中国真正帮助决策最有线性分类模型的数据点叫做“<strong>支持向量</strong>” 。同时要指出的是，logistic模型考虑了所有数据样本对参数的影响，所以不一定能获得最佳的分类器。</p>
<h5 id="多分类的SVM"><a href="#多分类的SVM" class="headerlink" title="多分类的SVM"></a>多分类的SVM</h5><p>将其中一类看作是阳性（正）样本，其余的全看成负样本。如有10个类别，则创造10个二分类任务。</p>
<p>:dart:SVM具有精妙的模型假设，可以在高维度的数据中筛选对预测任务有效的少数训练样本，不仅节省模型学习需要的数据内存，还提高了模型的预测性能，但是同时是以CPU资源和计算时间为代价的。</p>
<p>:memo:手写体数据​： 需要知道的是，经典模型没有对结构性信息学习的能力，所以这边对图片的处理其实是将2D图片像素矩阵（scikit-learn数据集里的是8*8）逐行首尾拼接为1D的像素特征向量。</p>
<h3 id="非线性模型"><a href="#非线性模型" class="headerlink" title="非线性模型"></a>非线性模型</h3><h4 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h4><blockquote>
<p>单独考量每一个维度特征被分类的条件概论，进而综合这些概率并对其所在的特征向量做出分类预测。</p>
<p><strong>数学假设</strong>：各个维度上的特征被分类的条件概率之间是相互独立的。</p>
</blockquote>
<p>:dart:具有较强的特征条件独立假设，使得模型预测所需要估计的参数规模从幂指数量级向线性量级减少，极大​节约了内存消耗和计算时间。但也受限与这种强假设的限制，训练时<strong>无法将各个特征之间的联系</strong>考量在内，使得该模型再其他数据特征关联性较强的分类任务上的性能表现不佳。</p>
<p>:memo:文本分类：互联网新闻分类、垃圾邮件筛选</p>
<h4 id="K近邻（KNN）"><a href="#K近邻（KNN）" class="headerlink" title="K近邻（KNN）"></a>K近邻（KNN）</h4><p>设有一个测试样本点，以及已经分好类（带有标签）的训练样本。那么该样本点的类别判定会根据在特征空间中最近的K个已标记样本作为参考。因此模型的性能很大程度上取决于K值的设定，但是K值不属于训练数据后学习的参数，而是模型初始化时需要提前确定的。</p>
<p>:dart:是无参数模型中最简单的一种，由于需要对预先加载在内存的训练样本进行遍历，逐一计算相似度、排序且选择K个最近邻训练样本的标记。所以是O(N2)的算法复杂度，一旦数据量大，可能会花费更多的时间（另一种理解为空间换时间，可探讨）</p>
<p>:memo: iris鸢尾花数据集​ : ▲在对数据进行分割时请保证随机取样！<br>由于scikit-learn获得的Iris数据集是根据类别依次排列的，所以如果只采样前25%那么所有采样的样本都是同一个类别，同时由于训练样本是<strong>不平衡的</strong>(Unbalanced)，这样取得的结果存在偏差，且可信度低。因此随机采样<code>train_test_split(data[names[1:10]],data[names[10]],test_size = 0.25,random_state = 33)</code>设置随机种子是必要的。</p>
<h4 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h4><blockquote>
<p>每个节点可以看作是二分类任务，根据不同特征组合搭建多层决策树，在学习时需要考虑特征节点的选择顺序。（度量方式：信息熵、基尼不纯性）</p>
</blockquote>
<h5 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h5><h6 id="缺省值："><a href="#缺省值：" class="headerlink" title="缺省值："></a>缺省值：</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = titannic[[<span class="string">'pclass'</span>,<span class="string">'age'</span>,<span class="string">'sex'</span>]]</span><br><span class="line">...</span><br><span class="line"><span class="comment"># 补充age里的数据，使用平均数或者中位数都是对模型偏离成都造成最小影响的策略</span></span><br><span class="line">X[<span class="string">'age'</span>].fillna(X[<span class="string">'age'</span>].mean(),inplace = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<h6 id="特征值转换"><a href="#特征值转换" class="headerlink" title="特征值转换"></a>特征值转换</h6><p>类别性特征值转换为数值特征，用0/1代替</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line">vec = DictVectorizer(sparse = <span class="keyword">False</span>)</span><br><span class="line">X_train = vec.fit_transform(X_train.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line">print(vec.feature_names)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="string">'age'</span>,<span class="string">'plcass=1st'</span>,<span class="string">'pclass=2nd'</span>, <span class="string">'sex=female'</span>,<span class="string">'sex=male'</span>]</span><br></pre></td></tr></table></figure>
<p>:dart:决策树在模型描述上有巨大的优势，推断逻辑非常直观，有清晰的可解释性，也方便模型的可视化。同时这些特性也保证了使用决策树时，无需考虑对数据量化甚至标准化。决策树属于有参数模型，需要花费更多时间在训练数据上面</p>
<p>:memo:泰坦尼克号沉船事故</p>
<h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><h3 id="线性回归器"><a href="#线性回归器" class="headerlink" title="线性回归器"></a>线性回归器</h3><p>:dart:美国波士顿地区房价</p>
<h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><h3 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h3><h3 id="集成模型"><a href="#集成模型" class="headerlink" title="集成模型"></a>集成模型</h3><h1 id="无监督学习模型"><a href="#无监督学习模型" class="headerlink" title="无监督学习模型"></a>无监督学习模型</h1><h2 id="数据聚类"><a href="#数据聚类" class="headerlink" title="数据聚类"></a>数据聚类</h2><h3 id="K均值（K-means）"><a href="#K均值（K-means）" class="headerlink" title="K均值（K-means）"></a>K均值（K-means）</h3><h2 id="特征降维"><a href="#特征降维" class="headerlink" title="特征降维"></a>特征降维</h2><h3 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h3></article><!-- lincense--><div class="license-wrapper"><p> <span>Author:  </span><a href="https://nymrli.top">Mrli</a></p><p> <span>Link:  </span><a href="https://nymrli.top/2019/05/27/Python机器学习及实践——从零开始通往Kaggle竞赛之路/">https://nymrli.top/2019/05/27/Python机器学习及实践——从零开始通往Kaggle竞赛之路/</a></p><p> <span>Copyright:  </span><span>All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/3.0">CC BY-NC-SA 3.0</a> unless stating additionally.</span></p></div><div class="post-paginator"><a class="prevSlogan" href="/2019/05/27/Keras使用——图像增强/" title="Keras使用——图像增强"><span>< PreviousPost</span><br><span class="prevTitle">Keras使用——图像增强</span></a><a class="nextSlogan" href="/2019/04/25/ACM-强连通分量/" title="ACM-强连通分量"><span>NextPost ></span><br><span class="nextTitle">ACM-强连通分量</span></a><div class="clear"></div></div><div id="comment"><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  id: 'Python机器学习及实践——从零开始通往Kaggle竞赛之路',
  owner: 'Freedomisgood',
  repo: 'Freedomisgood.github.io',
  oauth: {
    client_id: 'bc5a81fe36017dcd8b63',
    client_secret: '949cec3a1b91742c6249c47259791e4b80a6fa69',
  },
})
gitment.render('container')</script></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><p class="beian"><span>备案号:苏ICP备18015439号</span></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 60vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Python机器学习及实践——从零开始通往Kaggle竞赛之路"><span class="toc-number">1.</span> <span class="toc-text">Python机器学习及实践——从零开始通往Kaggle竞赛之路</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#监督学习模型"><span class="toc-number">2.</span> <span class="toc-text">监督学习模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#分类"><span class="toc-number">2.1.</span> <span class="toc-text">分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性模型"><span class="toc-number">2.1.1.</span> <span class="toc-text">线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Logistic"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">Logistic</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#处理缺省值的方法："><span class="toc-number">2.1.1.1.1.</span> <span class="toc-text">处理缺省值的方法：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#分割数据"><span class="toc-number">2.1.1.1.2.</span> <span class="toc-text">分割数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#标准化数据"><span class="toc-number">2.1.1.1.3.</span> <span class="toc-text">标准化数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#混淆矩阵"><span class="toc-number">2.1.1.1.4.</span> <span class="toc-text">混淆矩阵</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#支持向量机（​​分类）"><span class="toc-number">2.1.1.2.</span> <span class="toc-text">支持向量机（​​分类）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#多分类的SVM"><span class="toc-number">2.1.1.2.1.</span> <span class="toc-text">多分类的SVM</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#非线性模型"><span class="toc-number">2.1.2.</span> <span class="toc-text">非线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#朴素贝叶斯"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">朴素贝叶斯</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#K近邻（KNN）"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">K近邻（KNN）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#决策树"><span class="toc-number">2.1.2.3.</span> <span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#数据处理"><span class="toc-number">2.1.2.3.1.</span> <span class="toc-text">数据处理</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#缺省值："><span class="toc-number">2.1.2.3.1.1.</span> <span class="toc-text">缺省值：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#特征值转换"><span class="toc-number">2.1.2.3.1.2.</span> <span class="toc-text">特征值转换</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#回归"><span class="toc-number">2.2.</span> <span class="toc-text">回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性回归器"><span class="toc-number">2.2.1.</span> <span class="toc-text">线性回归器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KNN"><span class="toc-number">2.2.2.</span> <span class="toc-text">KNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#支持向量机"><span class="toc-number">2.2.3.</span> <span class="toc-text">支持向量机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#回归树"><span class="toc-number">2.2.4.</span> <span class="toc-text">回归树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#集成模型"><span class="toc-number">2.2.5.</span> <span class="toc-text">集成模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#无监督学习模型"><span class="toc-number">3.</span> <span class="toc-text">无监督学习模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#数据聚类"><span class="toc-number">3.1.</span> <span class="toc-text">数据聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K均值（K-means）"><span class="toc-number">3.1.1.</span> <span class="toc-text">K均值（K-means）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#特征降维"><span class="toc-number">3.2.</span> <span class="toc-text">特征降维</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#主成分分析（PCA）"><span class="toc-number">3.2.1.</span> <span class="toc-text">主成分分析（PCA）</span></a></li></ol></li></ol></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>