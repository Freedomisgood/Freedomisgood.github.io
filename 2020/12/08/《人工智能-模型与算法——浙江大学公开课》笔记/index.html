<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Mrli"><meta name="renderer" content="webkit"><meta name="copyright" content="Mrli"><meta name="keywords" content="Mrli's Blog"><meta name="description" content="想和你讲，说了会心动 ，缄默会心安。"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>《人工智能:模型与算法——浙江大学公开课》笔记 · Mr.li's Blog</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/favicon.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="https://avatars1.githubusercontent.com/u/31088082?s=400&amp;u=7a99ff83916afb3f4c5312bd78a1be17fe0e34ed&amp;v=4"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Mrli</div><div class="profile-signature">别装作很努力,<br>因为结局不会陪你演戏。</div><div class="contacts"><div>Contacts:</div><span><a href="http://sighttp.qq.com/msgrd?v=1&amp;uin=1063052964" target="_black">QQ</a></span><span><a href="https://www.cnblogs.com/nymrli/" target="_black">博客园</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 60vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">Mr.li's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">《人工智能:模型与算法——浙江大学公开课》笔记</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-edit"></i><span>2021/09/12</span></span><span class="busuanzi-pv" id="busuanzi_container_page_pv"><i class="post-intro-calendar fa fa-user-o"></i><span id="busuanzi_value_page_pv"></span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="ZJU"> ZJU</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="ML"> ML</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">7,917</span> | Reading time: <span class="post-count">29</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><html><head></head><body><blockquote>
<p>课程笔记来源：<a href="https://www.bilibili.com/video/BV1c7411n7EY?from=search&amp;seid=11992803505928094014&amp;spm_id_from=333.337.0.0" target="_blank" rel="noopener">2020公开课【人工智能：模型与算法】-浙江大学</a></p>
</blockquote>
<h2 id="P11-1可计算思想起源与发展"><a href="#P11-1可计算思想起源与发展" class="headerlink" title="P11.1可计算思想起源与发展"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=1" target="_blank" rel="noopener">P11.1可计算思想起源与发展</a></h2><p>智能： 从感知、到理解、到认知、到决策与行动</p>
<p>计算的诞生：从可计算到不可计算-&gt;20世纪初，人们发现有许多问题无法找到解决的方法。于是开始怀疑，是否对这些问题来说，根本就不存在算法，即不可计算。</p>
<p>人工智能：以机器为载体的人类智能或生物智能</p>
<p>算术公理的相容性：</p>
<ul>
<li>完备性：所有能够从该形式化系统推导出来的命题，都可以从这个形式化系统推导出来。</li>
<li>一致性：一个命题不可能同时为真或为假</li>
<li>可判定性：算法在有限步内判定命题的真伪</li>
</ul>
<p>哥德尔不完全性定理：任何表达力足够强的（递归可枚举）形式系统都不可能同时具有一致性和完备性</p>
<p>图灵测试：指测试者与被测试者（一个人和一台机器）隔开的情况下，通过一些装置（如键盘）向被测试者随意提问。进行多次测试后，如果机器让平均每个参与者做出超过30%的误判，那么这台机器就通过了测试，并被认为具有人类智能。</p>
<p>摩尔定律：（计算机速度1年半增长1倍），亿级晶体管、千亿指令/秒</p>
<h2 id="P21-2人工智能的发展简史"><a href="#P21-2人工智能的发展简史" class="headerlink" title="P21.2人工智能的发展简史"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=2" target="_blank" rel="noopener">P21.2人工智能的发展简史</a></h2><p>人工智能发展中的主流方法（1）：符号主义人工智能（SymbolicAl）为核心的逻辑推理</p>
<p>人工智能发展中的主流方法（2）：数据驱动（data-driven）为核心的机器学习</p>
<p>人工智能发展中的主流方法（3）：在“探索（未知空间）与利用（已有经验）（exploration vs.exploitation）”之间取得平衡为核心的强化学习</p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/AI三种主流方法区别.jpg" alt="AI三种主流方法区别" style="zoom:67%;"></p>
<h2 id="P31-3人工智能研究的基本内容"><a href="#P31-3人工智能研究的基本内容" class="headerlink" title="P31.3人工智能研究的基本内容"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=3" target="_blank" rel="noopener">P31.3人工智能研究的基本内容</a></h2><blockquote>
<p>人工智能特点：至小有内、至大无外，多学科交叉内禀</p>
</blockquote>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/人工智能课程.jpg" alt="人工智能课程"></p>
<p>从模拟人类智能角度而言，人工智能应具备如下能力：</p>
<ul>
<li><strong>具备视觉感知和语言交流的能力</strong>。即能够识别和理解外界信息（计算机视觉研究范畴）、能够与人通过语言交流（自然语言理解研究范畴）。</li>
<li>具备推理与问题求解能力。即基于已有知识，对所见事物和现象进行演绎推理以解决问题。</li>
<li><strong>具备协同控制能力</strong>。即将视觉（看）、语言（说）、推理（悟）等能力统一协调，加以控制，这是常见的机器人研究领域内容。</li>
<li><strong>具备遵守伦理道德能力</strong>。即模拟人类智能的智能体在社会环境中要遵从一定的伦理道德。阿西莫夫在科幻小说中按照优先级定义了机器人需要遵从的三条伦理原则：不得伤人，或弃人于危难；需服从人；在不违反上述两条原则情况下，保护机器人自己。</li>
<li><strong>具备从数据中进行归纳总结的能力</strong>。即需要从数据中进行知识、规律和模式学习的模型和方法，这是机器学习研究范畴。</li>
</ul>
<h3 id="授课基本内容："><a href="#授课基本内容：" class="headerlink" title="授课基本内容："></a>授课基本内容：</h3><p>人工智能概述</p>
<ul>
<li>1.1可计算思想起源与发展</li>
<li>1-2人工智能的发展简史</li>
<li>1.3人工智能研究的基本内容</li>
</ul>
<p>搜索求解</p>
<ul>
<li>2.1启发式搜索</li>
<li>2.2对抗搜索（Minimax及Alpha-Beta剪枝搜索）</li>
<li>2.3蒙特卡洛树搜索</li>
</ul>
<p>逻辑与推理</p>
<ul>
<li>3.1命题逻辑</li>
<li>3.2谓词逻辑</li>
<li>3.3兴国格推理</li>
<li>3.4因果推理</li>
</ul>
<p>统计机器学习|监督学习</p>
<ul>
<li>4.1机器学习基本概念</li>
<li>4.2线性回归与分类</li>
<li>4.3Ada Boosting</li>
<li>4.4线性区别分析</li>
</ul>
<p>统计机器学习|非监督学习</p>
<ul>
<li>5.1K-means</li>
<li>5.2主成分分析</li>
<li>5.3特征人脸方法</li>
<li>5.4期望极大算法（EM）</li>
</ul>
<p>深度学习(监督学习+端到端)</p>
<ul>
<li>6.1前馈神经网络（误差后向传播）</li>
<li>6.2卷积神经网络</li>
<li>6.3自然语言理解与视觉分析</li>
</ul>
<p>强化学习</p>
<ul>
<li>7.1马尔科夫决策过程</li>
<li>7.2强化学习中策略优化与策略评估</li>
<li>7.3Q-Learning</li>
<li>7.4深度强化学习</li>
</ul>
<p>人工智能博弈</p>
<ul>
<li>8.1博弈相关概念（纳什均衡）</li>
<li>8.2遗憾最小化算法</li>
<li>8.3虚拟遗憾最小化算法</li>
</ul>
<h2 id="搜索求解"><a href="#搜索求解" class="headerlink" title="搜索求解"></a>搜索求解</h2><h3 id="P42-1启发式搜索"><a href="#P42-1启发式搜索" class="headerlink" title="P42.1启发式搜索"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=4" target="_blank" rel="noopener">P42.1启发式搜索</a></h3><blockquote>
<p>给定搜索目标，设计启发函数，来保证搜索目标最优化的求解</p>
</blockquote>
<h3 id="P52-2对抗搜索"><a href="#P52-2对抗搜索" class="headerlink" title="P52.2对抗搜索"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=5" target="_blank" rel="noopener">P52.2对抗搜索</a></h3><blockquote>
<p>在游戏里搜索一种解决方案，但在搜索过程中对手会阻止我们，这种情况下我们能获得最大收益的搜索方式。本文中主要讲解Minmax搜索+alpha-beta剪枝搜索</p>
<p>对抗搜索（Adversarial Search）也称为博弈搜索（Game Search），在一个竞争的环境中，智能体（agents）之间通过竞争实现相反的利益，一方最大化这个利益，另外一方最小化这个利益。</p>
<p>本课程目前主要讨论在确定的、全局可观察的、竞争对手轮流行动、零和游戏（zero-sum）下的对抗搜索</p>
<ul>
<li><strong>零和博弈</strong>是博弈论的一个概念，属非合作博弈。指参与博弈的各方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”，双方不存在合作的可能。</li>
<li>与“零和”对应，“<strong>双赢博弈</strong>”的基本理论就是“利己”不“损人”，通过谈判、合作达到皆大欢喜的结果。</li>
</ul>
</blockquote>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/对抗搜索内容.jpg" alt="对抗搜索内容"></p>
<h4 id="最大最小搜索"><a href="#最大最小搜索" class="headerlink" title="最大最小搜索"></a>最大最小搜索</h4><p>给定一个游戏搜索树，minimax算法通过每个节点的minimax值来决定最优策略。当然，MAX节点希望最大化minimax值，而MIN节点则相反，希望最小化minimax值—-&gt;让自己的收益最大，让对方的收益or己方的损失最小</p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/minmax.jpg" alt="minmax"></p>
<p>优点：</p>
<ul>
<li>算法是一种简单有效的对抗搜索手段</li>
<li>在对手也“尽力而为”前提下，算法可返回最优结果</li>
</ul>
<p>缺点：</p>
<ul>
<li>如果搜索树极大，则无法在有效时间内返回结果</li>
</ul>
<p>改善：</p>
<ul>
<li>使用alpha-beta pruning算法来减少搜索节点</li>
<li>对节点进行采样、而非逐一搜索（ie.，MCTS）</li>
</ul>
<h4 id="alpha-beta剪枝搜索"><a href="#alpha-beta剪枝搜索" class="headerlink" title="alpha-beta剪枝搜索"></a>alpha-beta剪枝搜索</h4><blockquote>
<p>一种对最小最大搜索进行改进的算法，即在搜索过程中可剪除无需搜索的分支节点，且不影响搜索结果。.</p>
</blockquote>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/abpruncing.jpg" alt="abpruncing"></p>
<h4 id="P62-3蒙特卡洛树搜索"><a href="#P62-3蒙特卡洛树搜索" class="headerlink" title="P62.3蒙特卡洛树搜索"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=6" target="_blank" rel="noopener">P62.3蒙特卡洛树搜索</a></h4><blockquote>
<p>alphaGo三大法宝：深度学习、强化学习、MCTS</p>
<p>通过采样而非穷举方法来实现搜索，从而跟上述两种搜索有本质上的区别。</p>
<p>多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。</p>
<ul>
<li>利用（exploitation）：保证在过去决策中得到最佳回报</li>
<li>探索（exploration）：寄希望在未来能够得到更大回报</li>
</ul>
</blockquote>
<h5 id="exploitation-component-利用"><a href="#exploitation-component-利用" class="headerlink" title="exploitation component(利用)"></a>exploitation component(利用)</h5><p>第一部分是<span class="mjpage">undefined</span>​ ，也称作exploitation component。 Q(Vi)为子节点获胜次数，N(Vi)为子节点参与模拟的次数</p>
<p>可以看做是子节点Vi的胜率估计（总收益/总次数=平均每次的收益）。但是不能只选择胜率高的下一步，因为这种贪婪方式的搜索会很快导致游戏结束，这往往会导致搜索不充分，错过最优解。</p>
<p>举个简单的例子。现在假设MCTS的UCT函数只用了探索成分，从根节点开始，我们对所有子节点进行了一次模拟，然后在下一步中只访问至少赢了一次的子节点。那么在第一次模拟中那些不幸未被选中的节点（实际中rollout策略函数通常是随机的）将会被立刻抛弃</p>
<h5 id="exploration-component-探索"><a href="#exploration-component-探索" class="headerlink" title="exploration component(探索)"></a>exploration component(探索)</h5><span class="mjpage mjpage__block"></span><p>G_{m}(x): X \rightarrow{-1,1}</p>
<span class="mjpage mjpage__block">undefined</span><p>   f(x)=\sum<em>{i=1}^{M} \alpha</em>{m} G_{m}(x)</p>
<span class="mjpage mjpage__block">undefined</span><p>   G(x)=\operatorname{sign}(f(x))=\operatorname{sign}\left(\sum<em>{i=1}^{M} \alpha</em>{m} G_{m}(x)\right)</p>
<span class="mjpage mjpage__block"></span><p>d\left(x<em>{i}, x</em>{j}\right)=\sqrt{\left(x<em>{i 1}-x</em>{j 1}\right)^{2}+\left(x<em>{i 2}-x</em>{j 2}\right)^{2}+\cdots+\left(x<em>{i m}-x</em>{j m}\right)^{2}}</p>
<span class="mjpage mjpage__block"></span><p>f\left(w<em>{t}, w</em>{t-1}, \ldots, w<em>{t-n+2}, w</em>{t-n+1}\right)=p\left(w_{t} \mid \text { context }\right)</p>
<span class="mjpage mjpage__block">undefined</span><p>J=\max <em>{\theta}\left(\log f\left(w</em>{t}, w<em>{t-1}, \ldots, w</em>{t-n+2}, w_{t-n+1} ; \theta\right)+R(\theta)\right)</p>
<span class="mjpage mjpage__block"></span><p>其中衰退系数（ decay factor） <span class="mjpage">undefined</span>，来表示当前的奖励越是重要，远的奖励虽然需要考虑，但是重要程度是衰减的。<br>假设 <span class="mjpage">undefined</span><br>(1,1,0,0)$: G<em>{0}=1+0.99 \times 1+0.99^{2} \times 0+0.99^{3} \times 0=1.99<span class="mjpage">undefined</span>: G</em>{0}=0+0.99 \times 0+0.99^{2} \times 1+0.99^{3} \times 1=1.9504$</p>
<p>可见，前一种反馈的累加更大，虽然(1,1,0,0)更好。</p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/MRP2.jpg" alt="MRP2" style="zoom:67%;"></p>
<h5 id="马尔可夫决策过程（Markov-Decision-Process）"><a href="#马尔可夫决策过程（Markov-Decision-Process）" class="headerlink" title="马尔可夫决策过程（Markov Decision Process）"></a>马尔可夫决策过程（Markov Decision Process）</h5><p>马尔可夫决策过程（Markov Decision Process）：引入动作</p>
<p>在强化学习问题中，智能主体与环境交互过程中可自主决定所采取的动作，不同动作会对环境产生不同影响，为此：</p>
<ul>
<li>定义智能主体能够采取的动作集合为A</li>
<li>由于不同的动作对环境造成的影响不同，因此状态转移概率定义为<span class="mjpage">undefined</span>，其中atE A为第t步采取的动作</li>
<li>奖励可能受动作的影响，因此修改奖励函数为$R（S<em>t，at，S</em>{t+1}）$</li>
</ul>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/MDP3.jpg" alt="MDP3" style="zoom:67%;"></p>
<h5 id="相关术语"><a href="#相关术语" class="headerlink" title="相关术语"></a>相关术语</h5><p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/RL相关术语.jpg" alt="RL相关术语" style="zoom:67%;"></p>
<h4 id="啥是策略"><a href="#啥是策略" class="headerlink" title="啥是策略"></a>啥是策略</h4><p>策略函数：</p>
<ul>
<li>策略函数π：S×A→[0，1]，其中π（s，a）的值表示在状态s下采取动作a的概率。</li>
<li>策略函数的输出可以是确定的，即给定s情况下，只有一个动作a使得概率π（s，a）取值为1。<br>对于确定的策略，记为a=π（s）。</li>
</ul>
<p>如何进行策略学习：一个好的策略是在当前状态下采取了一个行动后，该行动能够在未来收到最大化的反馈：</p>
<p>$\pi^{*} = argmax<em>\pi G</em>{t}=R<em>{t+1}+\gamma R</em>{t+2}+\gamma^{2} R_{t+3}+\cdots$</p>
<p>为了对策略函数工进行评估，定义。</p>
<ul>
<li>价值函数（Value Function）<span class="mjpage">undefined</span> 其中 $V<em>{\pi}(s)=\mathbb{E}</em>{\pi}\left[G<em>{t} \mid S</em>{t}=s\right]$，即在第t步状态为s时，按照策略π行动后在未来所获得反馈值的期望.(由马尔可夫性，未来的状态和奖励只与当前状态相关，与t无关。因此t取任意值该等式均成立，如“逢山开路，遇水搭桥”。)</li>
<li>动作-价值函数（Action-Value Function）<span class="mjpage">undefined</span> 其中 $q<em>{\pi}(s, a)=\mathbb{E}</em>{\pi}\left[G<em>{t} \mid S</em>{t}=s, A_{t}=a\right]$<br>表示在第t步状态为s时，按照策略π采取动作a后，在未来所获得反馈值的期望</li>
</ul>
<p>==&gt;这样，策略学习转换为如下优化问题：寻找一个最优策略$π<em><span class="mjpage">undefined</span>V_\pi^{</em>}（s）$值最大</p>
<p>价值函数与动作-价值函数的关系：对策略进行评估</p>
<p>$\begin{aligned} V<em>{\pi}(s) &amp;=\mathbb{E}</em>{\pi}\left[R<em>{t+1}+\gamma R</em>{t+2}+\gamma^{2} R<em>{t+3}+\cdots \mid S</em>{t}=s\right] \ &amp;=\mathbb{E}<em>{a \sim \pi(s,)}\left[\mathbb{E}</em>{\pi}\left[R<em>{t+1}+\gamma R</em>{t+2}+\gamma^{2} R<em>{t+3}+\cdots \mid S</em>{t}=s, A<em>{t}=a\right]\right] \ &amp;=\sum</em>{a \in A} \pi(s, a) q<em>{\pi}(s, a) \ q</em>{\pi}(s, a)=&amp; \mathbb{E}<em>{\pi}\left[R</em>{t+1}+\gamma R<em>{t+2}+\gamma^{2} R</em>{t+3}+\cdots \mid S<em>{t}=s, A</em>{t}=a\right] \=&amp; \mathbb{E}<em>{s^{\prime} \sim \operatorname{Pr}(\mid s, a)}\left[R\left(s, a, s^{\prime}\right)+\gamma \mathbb{E}</em>{\pi}\left[R<em>{t+2}+\gamma R</em>{t+3}+\cdots \mid S<em>{t+1}=s^{\prime}\right]\right] \=&amp; \sum</em>{s^{\prime} \in S} \operatorname{Pr}\left(s^{\prime} \mid s, a\right)\left[R\left(s, a, s^{\prime}\right)+\gamma V_{\pi}\left(s^{\prime}\right)\right] \end{aligned}$</p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/RL_demo.jpg" alt="RL_demo" style="zoom:67%;"></p>
<h4 id="贝尔曼方程（Bellman-Equation）："><a href="#贝尔曼方程（Bellman-Equation）：" class="headerlink" title="贝尔曼方程（Bellman Equation）："></a>贝尔曼方程（Bellman Equation）：</h4><p>刻画了价值函数和行动-价值函数自身以及两者相互之间的递推关系</p>
<span class="mjpage mjpage__block">undefined</span><p>将右式带入左式，得到价值函数的贝尔曼方程</p>
<span class="mjpage mjpage__block">undefined</span><p>将左式带入右式，得到行动-价值函数的贝尔曼方程</p>
<span class="mjpage mjpage__block">undefined</span><p>将利用贝尔曼方程进行策略评估，进而进行策略优化</p>
<h3 id="P2710-2策略优化与策略评估"><a href="#P2710-2策略优化与策略评估" class="headerlink" title="P2710.2策略优化与策略评估"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=27" target="_blank" rel="noopener">P2710.2策略优化与策略评估</a></h3><p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/RL问题求解.jpg" alt="RL问题求解" style="zoom:67%;"></p>
<h4 id="基于价值的求解方法："><a href="#基于价值的求解方法：" class="headerlink" title="基于价值的求解方法："></a>基于价值的求解方法：</h4><p>第一部分：策略优化；</p>
<p>第二部分：策略评估</p>
<p>通过迭代计算贝尔曼方程进行策略评估</p>
<ul>
<li><p>动态规划</p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/DP.jpg" alt="DP" style="zoom:67%;"></p>
<ul>
<li>动态规划法的缺点：<br>1）智能主体需要事先知道<strong>状态转移概率</strong>（model-base）；<br>2）<strong>无法处理状态集合大小无限</strong>的情况</li>
</ul>
</li>
<li><p>蒙特卡洛采样</p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/MTCL.jpg" alt="MTCL" style="zoom:67%;"></p>
<p>蒙特卡洛采样法的优点</p>
<ul>
<li>智能主体不必知道状态转移概率·</li>
<li>容易扩展到无限状态集合的问题中</li>
</ul>
<p>蒙特卡洛采样法的缺点</p>
<ul>
<li>状态集合比较大时，一个状态在轨迹可能非常稀疏，不利于估计期望</li>
<li>在实际问题中，最终反馈需要在终止状态才能知晓，导致<strong>反馈周期较长</strong></li>
</ul>
</li>
<li><p>时序差分（Temporal Difference）</p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/TD.jpg" alt="TD" style="zoom:67%;"></p>
</li>
</ul>
<h3 id="P2810-3强化学习求解QLearning"><a href="#P2810-3强化学习求解QLearning" class="headerlink" title="P2810.3强化学习求解QLearning"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=28" target="_blank" rel="noopener">P2810.3强化学习求解QLearning</a></h3><blockquote>
<p> 基于时序差分的方法-Q学习（Q-Learning）[Q:quality]</p>
</blockquote>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/Qlearning.jpg" alt="Qlearning" style="zoom:67%;"></p>
<h4 id="无探索的Qlearning"><a href="#无探索的Qlearning" class="headerlink" title="无探索的Qlearning"></a>无探索的Qlearning</h4><p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/无探索Qlearning.jpg" alt="无探索Qlearning" style="zoom:67%;"></p>
<h4 id="使用e贪心策略的Q学习"><a href="#使用e贪心策略的Q学习" class="headerlink" title="使用e贪心策略的Q学习"></a>使用e贪心策略的Q学习</h4><p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/使用e贪心策略的Q学习.jpg" alt="使用e贪心策略的Q学习" style="zoom:67%;"></p>
<h3 id="P2910-4深度强化学习"><a href="#P2910-4深度强化学习" class="headerlink" title="P2910.4深度强化学习"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=29" target="_blank" rel="noopener">P2910.4深度强化学习</a></h3><blockquote>
<p>用神经网络拟合（行动）价值函数</p>
</blockquote>
<p><strong>问题：</strong></p>
<ul>
<li>状态数量太多时，有些状态可能始终无法采样到，因此对这些状态的q函数进行估计是很困难的</li>
<li>状态数量无限时，不可能用一张表（数组）来记录q函数的值</li>
</ul>
<p><strong>解决思路：</strong><br>将q函数参数化（parametrize），用一个非线性回归模型来拟合q函数，例如（深度）神经网络</p>
<ul>
<li>能够用有限的参数刻画无限的状态</li>
<li>由于回归函数的连续性，没有探索过的状态也可通过周围的状态来估计</li>
</ul>
<h4 id="深度Q学习与梯度下降法"><a href="#深度Q学习与梯度下降法" class="headerlink" title="深度Q学习与梯度下降法"></a>深度Q学习与梯度下降法</h4><p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/深度Q学习与梯度下降法.jpg" alt="深度Q学习与梯度下降法" style="zoom:67%;"></p>
<h5 id="深度Q学习的两个不稳定因素-gt-DQN"><a href="#深度Q学习的两个不稳定因素-gt-DQN" class="headerlink" title="深度Q学习的两个不稳定因素->DQN"></a>深度Q学习的两个不稳定因素-&gt;DQN</h5><ul>
<li>样本相关性太强</li>
<li>在损失函数中，q函数的值既用来估计目标值，又用来计算当前值。现在这两处的q函数通过e有所关联，可能导致优化时不稳定</li>
</ul>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/DRL不稳定因素.jpg" alt="DRL不稳定因素" style="zoom:67%;"></p>
<h4 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h4><h5 id="经验重现"><a href="#经验重现" class="headerlink" title="经验重现"></a>经验重现</h5><blockquote>
<p>样本相关性太强=&gt;经验重现</p>
</blockquote>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/ER.jpg" alt="ER" style="zoom:67%;"></p>
<h5 id="目标网络"><a href="#目标网络" class="headerlink" title="目标网络"></a>目标网络</h5><blockquote>
<p>在损失函数中，q函数的值既用来估计目标值，又用来计算当前值。现在这两处的q函数通过e有所关联，可能导致优化时不稳定-&gt;目标网络</p>
</blockquote>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/TN.jpg" alt="TN" style="zoom:67%;"></p>
<h2 id="P3011-1博弈相关概念——人工智能博弈"><a href="#P3011-1博弈相关概念——人工智能博弈" class="headerlink" title="P3011.1博弈相关概念——人工智能博弈"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=30" target="_blank" rel="noopener">P3011.1博弈相关概念</a>——人工智能博弈</h2><blockquote>
<p>博弈行为：带有相互竞争性质的主体，为了达到各自目标和利益，采取的带有对抗性质的行为。</p>
<p>博弈论主要研究博弈行为中<strong>最优的对抗策略</strong>及其<strong>稳定局势</strong>，协助人们在一定规则范围内寻求最合理的行为方式。</p>
</blockquote>
<p>​    <strong>博弈的要素</strong></p>
<ul>
<li>参与者或玩家（player）：参与博弈的决策主体</li>
<li>策略（strategy）：参与者可以采取的行动方案，是一整套在采取行动之前就已经准备好的完整方案。<ul>
<li>某个参与者可采纳策略的全体组合形成了策略集（strategy set）</li>
<li>所有参与者各自采取行动后形成的状态被称为局势（outcome）</li>
<li>如果参与者可以通过一定概率分布来选择若干个不同的策略，这样的策略称为<u>混合策略</u>（mixed strategy）。若参与者每次行动都选择某个确定的策略，这样的策略称为<u>纯策略</u>（pure strategy）</li>
</ul>
</li>
<li>收益（payoff）：各个参与者在不同局势下得到的利益</li>
<li>混合策略意义下的收益应为期望收益（expected payoff）规则（rule）：对参与者行动的先后顺序、参与者获得信息多少等内容的规定</li>
</ul>
<p><strong>囚徒困境（prisoner’s dilemma）</strong></p>
<p>数学家阿尔伯特·塔克：警方逮捕了共同犯罪的甲、乙两人，由于警方没有掌握充分的证据，所以将两人分开审讯：</p>
<ul>
<li>若一人认罪并指证对方，而另一方保持沉默，则此人会被当即释放，沉默者会被判监禁10年</li>
<li>若两人都保持沉默，则根据已有的犯罪事实（无充分证据）两人各判半年</li>
<li>若两人都认罪并相互指证，则两人各判5年</li>
</ul>
<p>在囚徒困境中，最优解为两人同时沉默，但是两人实际倾向于选择同时认罪（均衡解）</p>
<p><strong>囚徒困境产生的原因：</strong></p>
<ul>
<li>对甲而言，若乙沉默，自己认罪的收益为0，而自己也沉默则收益为-0.5；若乙认罪，自己认罪则收益为-5，自己沉默则收益为-10</li>
<li>对乙而言，若甲沉默，自己认罪的收益为0，而自己也沉默则收益为-0.5；若甲认罪，自己认罪的收益为-5，自己沉默则收益为-10</li>
<li>即对个人而言，认罪的收益在任何情况下都比沉默的收益高，所以两人同时认罪是一个稳定的局势，其他三种情况都不是稳定局势</li>
</ul>
<p>▲.囚徒困境表明稳定局势并不一定是最优局势</p>
<h4 id="博弈的分类"><a href="#博弈的分类" class="headerlink" title="博弈的分类"></a>博弈的分类</h4><p>合作博弈与非合作博弈</p>
<ul>
<li>合作博弈（cooperative game）：部分参与者可以组成联盟以获得更大的收益</li>
<li>非合作博弈（non-cooperative game）：参与者在决策中都彼此独立，不事先达成合作</li>
</ul>
<p>意向静态博弈与动态博弈</p>
<ul>
<li>静态博弈（static game）：所有参与者同时决策，或参与者互相不知道对方的决策</li>
<li>动态博弈（dynamic game）：参与者所采取行为的先后顺序由规则决定，且后行动者知道先行动者所采取的行为</li>
</ul>
<p>完全信息博弈与不完全信息博弈</p>
<ul>
<li>完全信息（complete information）：所有参与者均了解其他参与者的策略集、收益等信息</li>
<li>不完全信息（incomplete information）：并非所有参与者均掌握了所有信息</li>
</ul>
<p>囚徒困境是一种非合作、不完全信息的静态博弈</p>
<h4 id="纳什均衡"><a href="#纳什均衡" class="headerlink" title="纳什均衡"></a>纳什均衡</h4><p>博弈的稳定局势即为纳什均衡（Nash equilibrium）：<br>指的是参与者所作出的这样一种策略组合，在该策略组合上，任何参与者单独改变策略都不会得到好处。换句话说，<mark>如果在一个策略组合上，当所有其他人都不改变策略时，没有人会改变自己的策略，则该策略组合就是一个纳什均衡。</mark></p>
<p><strong>Nash定理</strong>：若<u>参与者有限</u>，每位参与者的<u>策略集有限</u>，收益函数为实值函数，则博弈<strong>必存在</strong><u>混合策略</u>意义下的纳什均衡。</p>
<p>囚徒困境中两人同时认罪就是这一问题的纳什均衡。</p>
<p>another Example:</p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/纳什博弈.jpg" alt="纳什博弈" style="zoom:67%;"></p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/纳什概率求解.jpg" alt="纳什概率求解" style="zoom:67%;"></p>
<h3 id="P3111-2遗憾最小化算法"><a href="#P3111-2遗憾最小化算法" class="headerlink" title="P3111.2遗憾最小化算法"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=31" target="_blank" rel="noopener">P3111.2遗憾最小化算法</a></h3><p>博弈论与计算机科学的交叉领域非常多</p>
<ul>
<li>理论计算机科学：算法博弈论</li>
<li>人工智能：多智能体系统、AI游戏玩家、人机交互、机器学习、广告推荐</li>
<li>互联网：互联网经济、共享经济</li>
<li>分布式系统：区块链</li>
</ul>
<p>人工智能与博弈论相互结合，形成了两个主要研究方向</p>
<ul>
<li>博弈策略的求解<ul>
<li>为什么引入博弈论的动机<br>·博弈论提供了许多问题的数学模型<br>·纳什定理确定了博弈过程问题存在解<br>·人工智能的方法可用来求解均衡局面或者最优策略</li>
<li>应用领域<br>·大规模搜索空间的问题求解：围棋<br>·非完全信息博弈问题求解：德州扑克<br>·网络对战游戏智能：Dota、星球大战<br>·动态博弈的均衡解：厂家竞争、信息安全</li>
</ul>
</li>
<li>博弈规则的设计<ul>
<li>问题描述<br>·假设博弈的参与者都是足够理性的<br>·如何设计一个博弈规则能确保公正性或者达到设计者的最大利益</li>
<li>挑战<br>·规则复杂<br>·计算量大</li>
<li>应用领域<br>·拍卖竞价：互联网广告投放、车牌竞价<br>·供需匹配：污染权、学校录取<br>·公正选举：选举制度、表决制度、议席分配</li>
</ul>
</li>
</ul>
<p>RM算法若干定义</p>
<ul>
<li>假设一共有N个玩家。玩家 <span class="mjpage">undefined</span> 所采用的策略表示为 <span class="mjpage">undefined</span> 。</li>
<li>对于每个信息集 $I<em>{i} \in \xi</em>{i}, \sigma<em>{i}\left(I</em>{i}\right): A\left(I<em>{i}\right) \rightarrow[0,1]<span class="mjpage">undefined</span>A\left(I</em>{i}\right)<span class="mjpage">undefined</span>i<span class="mjpage">undefined</span>\Sigma_{i}$ 表示。</li>
<li>一个策略组包含所有玩家策略，用 $\sigma=\left(\sigma<em>{1}, \sigma</em>{2}, \ldots, \sigma_{|N|}\right)$</li>
<li>$\sigma<em>{-i}<span class="mjpage">undefined</span>\sigma<span class="mjpage">undefined</span>\sigma</em>{i}<span class="mjpage">undefined</span>i<span class="mjpage">undefined</span>)$</li>
<li>在博亦对决中，不同玩家在不同时刻会采取相应策略以及行动。策略\sigma下对应的行动序列 <span class="mjpage">undefined</span> 发生的概率表示为 <span class="mjpage">undefined</span> 。于是, $\pi^{\sigma}(h)=\prod<em>{i \in N} \pi</em>{i}^{\sigma}(h),<span class="mjpage">undefined</span>\pi<em>{i}^{\sigma}(h)<span class="mjpage">undefined</span>i<span class="mjpage">undefined</span>\sigma</em>{i}<span class="mjpage">undefined</span>h<span class="mjpage">undefined</span>i<span class="mjpage">undefined</span>: \pi<em>{-i}^{\sigma}(h)=\prod</em>{j \in N \backslash{i}} \pi_{j}^{\sigma}(h)$</li>
<li>对于每个玩家 <span class="mjpage">undefined</span> 表示玩家 <span class="mjpage">undefined</span> 的收益函数，即在到达终止序列集合Z中某个终 止序列时，玩家 <span class="mjpage">undefined</span> 所得到的收益。</li>
<li>玩家 <span class="mjpage">undefined</span> 在给定策略 <span class="mjpage">undefined</span> 下所能得到的期望收益可如下计算: $u<em>{i}(\sigma)=\sum</em>{h \in Z} u_{i}(h) \pi^{\sigma}(h)$</li>
</ul>
<p>悔值：</p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/RM.jpg" alt="RM" style="zoom:67%;"></p>
<h4 id="遗憾最小化算法：策略选择介绍"><a href="#遗憾最小化算法：策略选择介绍" class="headerlink" title="遗憾最小化算法：策略选择介绍"></a>遗憾最小化算法：策略选择介绍</h4><ul>
<li>遗憾最小化算法是一种根据过去博将中的遗憾程度来决定将来动作选择的方法</li>
<li>在博亦中，玩家i在第T轮次（每一轮表示一次博将完成）采取策略 <span class="mjpage">undefined</span> 的遗憾值定义如<br>下（累加遗憾）:</li>
</ul>
<span class="mjpage mjpage__block">undefined</span><ul>
<li>通常遗憾值为负数的策略被认为不能提升下一时刻收益，所以这里考虑的遗憾值均为<br>正数或0</li>
<li>计算得到玩家 <span class="mjpage">undefined</span> 在第T轮次采取策略 <span class="mjpage">undefined</span> 的遗憾值后，在第 <span class="mjpage">undefined</span> 轮次玩家 <span class="mjpage">undefined</span> 选择策略 <span class="mjpage">undefined</span> 的概<br>率如下（悔值越大、越选择，即亡羊补牢）</li>
</ul>
<span class="mjpage mjpage__block">undefined</span><h4 id="demo石头剪刀布"><a href="#demo石头剪刀布" class="headerlink" title="demo石头剪刀布"></a>demo石头剪刀布</h4><p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/demo石头剪刀布.jpg" alt="demo石头剪刀布" style="zoom:67%;"></p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/demo石头剪刀布2.jpg" alt="demo石头剪刀布2" style="zoom:67%;"></p>
<p>为了解决博弈状态空间大的问题-&gt;虚拟遗憾最小化算法</p>
<h3 id="P3211-3虚拟遗憾最小化算法"><a href="#P3211-3虚拟遗憾最小化算法" class="headerlink" title="P3211.3虚拟遗憾最小化算法"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=32" target="_blank" rel="noopener">P3211.3虚拟遗憾最小化算法</a></h3><ul>
<li><p>如果不能遍历计算所有节点的遗憾值，那么可以采用虚拟遗憾最小化算法来进行模拟计算<br>假设:</p>
<ul>
<li>集合 <span class="mjpage">undefined</span> 是博亦中所有玩家所能采用的行为集（如在石头-剪刀-布游戏中出石头、出剪刀或出布三种行 为 <span class="mjpage">undefined</span></li>
<li>I为信息集，包含了博亦的规则以及玩家采取的历史行动，在信息集I下所能采取的行为集合记为 <span class="mjpage">undefined</span></li>
</ul>
</li>
<li><p>玩家 <span class="mjpage">undefined</span> 在第 <span class="mjpage">undefined</span> 轮次采取的行动 $a<em>{i} \in A\left(I</em>{i}\right)<span class="mjpage">undefined</span>\sigma<em>{i}^{t}<span class="mjpage">undefined</span>i<span class="mjpage">undefined</span>a \in A(I)<span class="mjpage">undefined</span>\sigma</em>{\circ}^{t}$</p>
</li>
<li><p>在信息集I下采取行动a所反映的策略记为 <span class="mjpage">undefined</span> 。</p>
</li>
<li><p>在第t轮次所有玩家采取的行动是一条序列，记为 <span class="mjpage">undefined</span> 采取某个策略 <span class="mjpage">undefined</span> 计算行动序列 <span class="mjpage">undefined</span><br>出现的概率记为 <span class="mjpage">undefined</span></p>
</li>
<li><p>每个信息集I发生的概率 <span class="mjpage">undefined</span> 表示所有能够到达该信息集的行动序<br>列的概率累加。</p>
</li>
<li><p>给定博亦的终结局势z <span class="mjpage">undefined</span> 玩家 <span class="mjpage">undefined</span> 在游戏结束后的收益记作 <span class="mjpage">undefined</span></p>
</li>
<li><p>在策略组合 <span class="mjpage">undefined</span> 下，施加博亦行动序列 <span class="mjpage">undefined</span> 后达到最终局势z的概率为 <span class="mjpage">undefined</span></p>
</li>
<li><p>当采取策略\sigma时，其所对应的行动序列h的虚拟价值（Counterfactual Value）如下 计算(注：行动序列 <span class="mjpage">undefined</span> 未能使博亦进入终结局势):</p>
<span class="mjpage mjpage__block">undefined</span><p>玩家i采取行动a所得到的虚拟遗憾值:</p>
<span class="mjpage mjpage__block">undefined</span><p>行动序列 <span class="mjpage">undefined</span> 所对应的信息集I遗憾值为</p>
<span class="mjpage mjpage__block">undefined</span><p>玩家 <span class="mjpage">undefined</span> 在第T轮次采取行动a的遗憾值为 :</p>
<span class="mjpage mjpage__block">undefined</span><p>同样，对于<strong>遗憾值为负数的情况，我们不予考虑</strong>，记:</p>
<span class="mjpage mjpage__block">undefined</span><p>在 <span class="mjpage">undefined</span> 轮次，玩家 <span class="mjpage">undefined</span> 选择行动 <span class="mjpage">undefined</span> 的概率计算如下</p>
<span class="mjpage mjpage__block">undefined</span><p>玩家i根据遗憾值大小来选择下一时刻行为，如果遗憾值为负数，则随机挑选一种行 为进行博亦（由于规定regret不为负数，因此随机取的概率不会出现）</p>
</li>
</ul>
<h4 id="demo库恩扑克（Kunh’s-pocker）"><a href="#demo库恩扑克（Kunh’s-pocker）" class="headerlink" title="demo库恩扑克（Kunh’s pocker）"></a>demo库恩扑克（Kunh’s pocker）</h4><ul>
<li>库恩扑克是最简单的限注扑克游戏，由两名玩家进行游戏博弈，牌值只有1，2和3三种情况</li>
<li>每轮每位玩家各持一张手牌，根据各自判断来决定加定额赌注</li>
<li>游戏没有公共牌，摊牌阶段比较未弃牌玩家的底牌大小，底牌牌值最大的玩家即为胜者</li>
</ul>
<p>游戏规则定义：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>玩家A</th>
<th>玩家B</th>
<th>玩家A</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>过牌</td>
<td>过牌</td>
<td>\</td>
<td>牌值大的玩家+1</td>
</tr>
<tr>
<td>加注</td>
<td>加注</td>
<td>\</td>
<td>牌值大的玩家+2</td>
</tr>
<tr>
<td>过牌</td>
<td>加注</td>
<td>过牌</td>
<td>玩家B+1</td>
</tr>
<tr>
<td>过牌</td>
<td>加注</td>
<td>加注</td>
<td>牌值大的玩家+2</td>
</tr>
<tr>
<td>加注</td>
<td>过牌</td>
<td>\</td>
<td>玩家A+1</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/Kpocker.jpg" alt="Kpocker" style="zoom:67%;"></p>
<h5 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h5><p>该问题中进行策略选择的算法步骤如下：<br>1.初始化遗憾值和累加策略表为02.采用随机选择的方法来决定策略<br>3.利用当前策略与对手进行博弈4.计算每个玩家采取每次行为后的遗憾值5.根据博弈结果计算每个行动的累加遗憾值大小来更新策略<br>6.重复博弈若干次<br>7.根据重复博弈最终的策略，完成最终的动作选择</p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/Kpocker.jpg" alt="Kpocker" style="zoom:67%;"></p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/Kpocker解析.jpg" alt="Kpocker解析" style="zoom:67%;"></p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/Kpocker_end.jpg" alt="Kpocker_end" style="zoom:67%;"></p>
<h4 id="G-S算法（Gale-Shapley）"><a href="#G-S算法（Gale-Shapley）" class="headerlink" title="G-S算法（Gale-Shapley）"></a>G-S算法（Gale-Shapley）</h4><ul>
<li>在生活中，人们常常会碰到与资源匹配相关的决策问题（如求职就业、报考录取等），这些需要双向选择的情况被称为是<u>双边匹配问题</u>。在双边匹配问题中，需要双方互相满足对方的需求才会达成匹配</li>
<li>匹配的稳定是指没有任何人能从偏离稳定状态中获益。如果将匹配问题看做是一种合作博弈的话，稳定状态解就是纳什均衡解</li>
<li>1962年，美国数学家大卫·盖尔和博弈论学家沙普利提出了针对双边稳定匹配问题的解决算法，并将其应用于稳定婚姻问题的求解</li>
<li>稳定婚姻问题（stable marriage problem）是指在给定成员偏好的条件下，为两组成员寻找稳定匹配。由于这种匹配并不是简单地价高者得，所以匹配解法应考虑双方意愿</li>
<li>稳定婚姻问题的稳定解是指不存在未达成匹配的两个人都更倾向于选择对方胜过自己当前的匹配对象</li>
</ul>
<h4 id="最大交易圈算法（Top-Trading-Cycle-algorithm）"><a href="#最大交易圈算法（Top-Trading-Cycle-algorithm）" class="headerlink" title="最大交易圈算法（Top-Trading Cycle algorithm）"></a>最大交易圈算法（Top-Trading Cycle algorithm）</h4><ul>
<li>在匹配问题中，还有一类交换不可分的标的物的匹配问题，被称为单边匹配问题，如远古时期以物易物、或者宿舍的床位分配</li>
<li>1974年，沙普利和斯卡夫提出了针对单边匹配问题的稳定匹配算法：最大交易圈算法（TTC），算法过程如下：<ul>
<li>首先每个交易者连接一条指向他最喜欢的标的物的边，并从每一个标的物连接到其占有者或是具有高优先权的交易者。</li>
<li>此时形成一张有向图，且必存在交易圈，对于交易圈中的交易者，将每人指向节点所代表的标的物赋予其，同时交易者放弃原先占有的标的物，占有者和匹配成功的标的物离开匹配市场。</li>
<li>接着从剩余的交易者和标的物之间重复进行交易圈匹配，直到无法形成交易圈，算法停止。</li>
</ul>
</li>
</ul>
<h2 id="P3311-4人工智能安全"><a href="#P3311-4人工智能安全" class="headerlink" title="P3311.4人工智能安全"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=33" target="_blank" rel="noopener">P3311.4人工智能安全</a></h2><h3 id="基于人工智能的信息安全技术：加密技术"><a href="#基于人工智能的信息安全技术：加密技术" class="headerlink" title="基于人工智能的信息安全技术：加密技术"></a>基于人工智能的信息安全技术：加密技术</h3><ul>
<li>将明文信息处理为难以读取的密文内容，使之不可读。</li>
<li>在网络环境中保障通信安全，保证数据的完整性</li>
<li>目前常用的加密算法有安全哈希算法（Secure Hash Algorithm，SHA）和高级加密标准（Advanced Encryption Standard，AES）</li>
</ul>
<p>使用神经网络的加密算法</p>
<ul>
<li>2016年谷歌大脑的研究团队提出了使用对抗生成网络GAN生成的一个加密算法，其使用了三个神经网络分别完成加密、解密和攻击的工作，以保证通信双方信息的无损传输以及第三方无法破译通信内容</li>
</ul>
<h3 id="基于人工智能的信息安全技术：数字水印"><a href="#基于人工智能的信息安全技术：数字水印" class="headerlink" title="基于人工智能的信息安全技术：数字水印"></a>基于人工智能的信息安全技术：数字水印</h3><ul>
<li>将特定信息（版权信息等）嵌入在数字信号中，数字信号可能是音频、视频、图片等。</li>
<li>当拷贝信息时，水印内容会被同时拷贝，所以水印内容可作为版权信息的证明，这样能避免或阻止数字媒体未经授权的复制和拷贝</li>
</ul>
<p>近年来通过神经网络来添加水印和提取水印信息的成为学术研究热点。</p>
<h3 id="人工智能的安全：数据安全与模型安全"><a href="#人工智能的安全：数据安全与模型安全" class="headerlink" title="人工智能的安全：数据安全与模型安全"></a>人工智能的安全：数据安全与模型安全</h3><p>人工智能很大程度是依靠数据驱动学习<br><strong>可用性</strong>（availability）</p>
<ul>
<li>训练数据是否充足且可靠</li>
<li>训练数据是否有足够的标注</li>
</ul>
<p><strong>完整性</strong>（completeness）</p>
<ul>
<li>数据是否具有代表性</li>
</ul>
<p><strong>隐私性</strong>（privacy）</p>
<ul>
<li>数据是否涉及隐私安全问题</li>
<li>如何保障数据不被窃取</li>
</ul>
<p>人工智能所使用的的模型是由有限的训练数据训练得到的<br><strong>鲁棒性</strong>（robustness）</p>
<ul>
<li>模型是否易于受到噪声干扰或攻击</li>
</ul>
<p><strong>正确性</strong>（correctness）</p>
<ul>
<li>模型是否正确</li>
</ul>
<p><strong>通用性</strong>（generality）</p>
<ul>
<li>模型是否能够应用于现实场景</li>
<li>模型对输入数据是否有过高的要求</li>
</ul>
<h3 id="人工智能的安全：对模型的攻击"><a href="#人工智能的安全：对模型的攻击" class="headerlink" title="人工智能的安全：对模型的攻击"></a>人工智能的安全：对模型的攻击</h3><p>对模型的攻击</p>
<ul>
<li>使用特定技术对输入样本进行微小的修改就可骗过模型而得到错误的结果</li>
<li>这种经过修改，使得模型判断错误的样本被称为对抗样本</li>
</ul>
<p>白盒攻击</p>
<ul>
<li><p>攻击者熟知人工智能模型的算法和模型参数，生成对抗样本的过程可以与模型的每一部分进行交互</p>
<p>对人工智能模型的白盒攻击通常会对模型的每一部分进行逐层分解，然后对每一部分添加一定的扰动，使得模型的结果逐步向误判目标类别偏移<br>这是一种非常隐蔽的攻击手段，通过限制扰动的大小可以使得对抗样本看起来与原样本差别很小</p>
<p>白盒攻击的防御策略：生成对抗网络</p>
</li>
</ul>
<p>黑盒攻击</p>
<ul>
<li><p>攻击者只能给定输入去获得模型输出，但并不知道被攻击模型所使用的算法和参数</p>
</li>
<li><p>黑盒攻击可以针对任何一个人工智能模型</p>
<p>常用的黑盒攻击防御策略有：<br>·数据压缩：通过对输入数据进行压缩或者降维，在保证识别准确率的情况下提升模型对干扰攻击的鲁棒性<br>·数据随机化：对训练数据进行随机缩放、增强等操作，提升模型的鲁棒性<br>·训练额外的网络来判断训练数据是否为攻击样本</p>
</li>
</ul>
<h2 id="P3412-1记忆驱动的智能计算"><a href="#P3412-1记忆驱动的智能计算" class="headerlink" title="P3412.1记忆驱动的智能计算"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=34" target="_blank" rel="noopener">P3412.1记忆驱动的智能计算</a></h2><h2 id="P3512-2可计算社会学"><a href="#P3512-2可计算社会学" class="headerlink" title="P3512.2可计算社会学"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=35" target="_blank" rel="noopener">P3512.2可计算社会学</a></h2><h2 id="P3612-3若干挑战"><a href="#P3612-3若干挑战" class="headerlink" title="P3612.3若干挑战"></a><a href="https://www.bilibili.com/video/BV1c7411n7EY?p=36" target="_blank" rel="noopener">P3612.3若干挑战</a></h2></body></html></article><!-- lincense--><div class="license-wrapper"><p> <span>Author:  </span><a href="https://nymrli.top">Mrli</a></p><p> <span>Link:  </span><a href="https://nymrli.top/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/">https://nymrli.top/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/</a></p><p> <span>Copyright:  </span><span>All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/3.0">CC BY-NC-SA 3.0</a> unless stating additionally.</span></p></div><div class="post-paginator"><a class="prevSlogan" href="/2020/12/09/浙大2020春夏-人工智能习题3——图像恢复/" title="浙大2020春夏-人工智能习题3——图像恢复"><span>< PreviousPost</span><br><span class="prevTitle">浙大2020春夏-人工智能习题3——图像恢复</span></a><a class="nextSlogan" href="/2020/12/05/枚举类的优雅写法Java-Python/" title="枚举类的优雅写法Java-&gt;Python"><span>NextPost ></span><br><span class="nextTitle">枚举类的优雅写法Java-&gt;Python</span></a><div class="clear"></div></div><div id="comment"><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  id: '《人工智能:模型与算法——浙江大学公开课》笔记',
  owner: 'Freedomisgood',
  repo: 'Freedomisgood.github.io',
  oauth: {
    client_id: 'bc5a81fe36017dcd8b63',
    client_secret: '949cec3a1b91742c6249c47259791e4b80a6fa69',
  },
})
gitment.render('container')</script></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><p class="beian"><span>备案号:苏ICP备18015439号</span></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 60vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#P11-1可计算思想起源与发展"><span class="toc-number">1.</span> <span class="toc-text">P11.1可计算思想起源与发展</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#P21-2人工智能的发展简史"><span class="toc-number">2.</span> <span class="toc-text">P21.2人工智能的发展简史</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#P31-3人工智能研究的基本内容"><span class="toc-number">3.</span> <span class="toc-text">P31.3人工智能研究的基本内容</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#授课基本内容："><span class="toc-number">3.1.</span> <span class="toc-text">授课基本内容：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#搜索求解"><span class="toc-number">4.</span> <span class="toc-text">搜索求解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#P42-1启发式搜索"><span class="toc-number">4.1.</span> <span class="toc-text">P42.1启发式搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#P52-2对抗搜索"><span class="toc-number">4.2.</span> <span class="toc-text">P52.2对抗搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#最大最小搜索"><span class="toc-number">4.2.1.</span> <span class="toc-text">最大最小搜索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#alpha-beta剪枝搜索"><span class="toc-number">4.2.2.</span> <span class="toc-text">alpha-beta剪枝搜索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#P62-3蒙特卡洛树搜索"><span class="toc-number">4.2.3.</span> <span class="toc-text">P62.3蒙特卡洛树搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#exploitation-component-利用"><span class="toc-number">4.2.3.1.</span> <span class="toc-text">exploitation component(利用)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#exploration-component-探索"><span class="toc-number">4.2.3.2.</span> <span class="toc-text">exploration component(探索)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#马尔可夫决策过程（Markov-Decision-Process）"><span class="toc-number">4.2.3.3.</span> <span class="toc-text">马尔可夫决策过程（Markov Decision Process）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#相关术语"><span class="toc-number">4.2.3.4.</span> <span class="toc-text">相关术语</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#啥是策略"><span class="toc-number">4.2.4.</span> <span class="toc-text">啥是策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#贝尔曼方程（Bellman-Equation）："><span class="toc-number">4.2.5.</span> <span class="toc-text">贝尔曼方程（Bellman Equation）：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#P2710-2策略优化与策略评估"><span class="toc-number">4.3.</span> <span class="toc-text">P2710.2策略优化与策略评估</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#基于价值的求解方法："><span class="toc-number">4.3.1.</span> <span class="toc-text">基于价值的求解方法：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#P2810-3强化学习求解QLearning"><span class="toc-number">4.4.</span> <span class="toc-text">P2810.3强化学习求解QLearning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#无探索的Qlearning"><span class="toc-number">4.4.1.</span> <span class="toc-text">无探索的Qlearning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用e贪心策略的Q学习"><span class="toc-number">4.4.2.</span> <span class="toc-text">使用e贪心策略的Q学习</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#P2910-4深度强化学习"><span class="toc-number">4.5.</span> <span class="toc-text">P2910.4深度强化学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#深度Q学习与梯度下降法"><span class="toc-number">4.5.1.</span> <span class="toc-text">深度Q学习与梯度下降法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#深度Q学习的两个不稳定因素-gt-DQN"><span class="toc-number">4.5.1.1.</span> <span class="toc-text">深度Q学习的两个不稳定因素-&gt;DQN</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DQN"><span class="toc-number">4.5.2.</span> <span class="toc-text">DQN</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#经验重现"><span class="toc-number">4.5.2.1.</span> <span class="toc-text">经验重现</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#目标网络"><span class="toc-number">4.5.2.2.</span> <span class="toc-text">目标网络</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#P3011-1博弈相关概念——人工智能博弈"><span class="toc-number">5.</span> <span class="toc-text">P3011.1博弈相关概念——人工智能博弈</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#博弈的分类"><span class="toc-number">5.0.1.</span> <span class="toc-text">博弈的分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#纳什均衡"><span class="toc-number">5.0.2.</span> <span class="toc-text">纳什均衡</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#P3111-2遗憾最小化算法"><span class="toc-number">5.1.</span> <span class="toc-text">P3111.2遗憾最小化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#遗憾最小化算法：策略选择介绍"><span class="toc-number">5.1.1.</span> <span class="toc-text">遗憾最小化算法：策略选择介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#demo石头剪刀布"><span class="toc-number">5.1.2.</span> <span class="toc-text">demo石头剪刀布</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#P3211-3虚拟遗憾最小化算法"><span class="toc-number">5.2.</span> <span class="toc-text">P3211.3虚拟遗憾最小化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#demo库恩扑克（Kunh’s-pocker）"><span class="toc-number">5.2.1.</span> <span class="toc-text">demo库恩扑克（Kunh’s pocker）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#算法步骤"><span class="toc-number">5.2.1.1.</span> <span class="toc-text">算法步骤</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#G-S算法（Gale-Shapley）"><span class="toc-number">5.2.2.</span> <span class="toc-text">G-S算法（Gale-Shapley）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#最大交易圈算法（Top-Trading-Cycle-algorithm）"><span class="toc-number">5.2.3.</span> <span class="toc-text">最大交易圈算法（Top-Trading Cycle algorithm）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#P3311-4人工智能安全"><span class="toc-number">6.</span> <span class="toc-text">P3311.4人工智能安全</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基于人工智能的信息安全技术：加密技术"><span class="toc-number">6.1.</span> <span class="toc-text">基于人工智能的信息安全技术：加密技术</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于人工智能的信息安全技术：数字水印"><span class="toc-number">6.2.</span> <span class="toc-text">基于人工智能的信息安全技术：数字水印</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#人工智能的安全：数据安全与模型安全"><span class="toc-number">6.3.</span> <span class="toc-text">人工智能的安全：数据安全与模型安全</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#人工智能的安全：对模型的攻击"><span class="toc-number">6.4.</span> <span class="toc-text">人工智能的安全：对模型的攻击</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#P3412-1记忆驱动的智能计算"><span class="toc-number">7.</span> <span class="toc-text">P3412.1记忆驱动的智能计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#P3512-2可计算社会学"><span class="toc-number">8.</span> <span class="toc-text">P3512.2可计算社会学</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#P3612-3若干挑战"><span class="toc-number">9.</span> <span class="toc-text">P3612.3若干挑战</span></a></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>