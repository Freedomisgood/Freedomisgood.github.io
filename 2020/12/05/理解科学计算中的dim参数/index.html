<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Mrli"><meta name="renderer" content="webkit"><meta name="copyright" content="Mrli"><meta name="keywords" content="Mrli's Blog"><meta name="description" content="想和你讲，说了会心动 ，缄默会心安。"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>理解科学计算(numpy,pytorch)中的dim参数 · Mr.li's Blog</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/favicon.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="https://avatars1.githubusercontent.com/u/31088082?s=400&amp;u=7a99ff83916afb3f4c5312bd78a1be17fe0e34ed&amp;v=4"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Mrli</div><div class="profile-signature">别装作很努力,<br>因为结局不会陪你演戏。</div><div class="contacts"><div>Contacts:</div><span><a href="http://sighttp.qq.com/msgrd?v=1&amp;uin=1063052964" target="_black">QQ</a></span><span><a href="https://www.cnblogs.com/nymrli/" target="_black">博客园</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 60vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">Mr.li's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">理解科学计算(numpy,pytorch)中的dim参数</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-edit"></i><span>2020/12/10</span></span><span class="busuanzi-pv" id="busuanzi_container_page_pv"><i class="post-intro-calendar fa fa-user-o"></i><span id="busuanzi_value_page_pv"></span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="计算机基础知识"> 计算机基础知识</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="ML"> ML</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">4,205</span> | Reading time: <span class="post-count">19</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><blockquote>
<p>😄理解numpy中array和pytorch中tensor的操作是开始科学运算的第一步!</p>
</blockquote>
<p>首先明白维度的感念：</p>
<h2 id="维度"><a class="markdownIt-Anchor" href="#维度"></a> 维度</h2>
<p>我们通常能听到的都是2D, 3D，其实这边的D就是dimension的含义即维度。2D，我们通常理解为是平面，如我们最熟悉的直角坐标系就是平面坐标系，还有极坐标系。而3D呢，就是在平面的基础上增加了一维——高度，从而使平面的物体立起来了，同样3D也有耳熟能详的坐标系——3维坐标系。</p>
<p>更官方的解释呢：维度（Dimension），又称为<a href="https://baike.baidu.com/item/%E7%BB%B4%E6%95%B0/6496548" target="_blank" rel="noopener">维数</a>，是<a href="https://baike.baidu.com/item/%E6%95%B0%E5%AD%A6/107037" target="_blank" rel="noopener">数学</a>中独立参数的<a href="https://baike.baidu.com/item/%E6%95%B0%E7%9B%AE/7720359" target="_blank" rel="noopener">数目</a>。在<a href="https://baike.baidu.com/item/%E7%89%A9%E7%90%86%E5%AD%A6/313183" target="_blank" rel="noopener">物理学</a>和<a href="https://baike.baidu.com/item/%E5%93%B2%E5%AD%A6/140608" target="_blank" rel="noopener">哲学</a>的领域内，指独立的时空坐标的数目。0维是一个无限小的点，没有长度。<strong>1维是一条无限长的线，只有长度。2维是一个平面，是由长度和宽度(或部分曲线)组成面积。3维是2维加上高度组成体积。4维分为时间上和空间上的4维，人们说的4维通常是指关于物体在时间线上的转移。</strong>（4维准确来说有两种。1.四维时空，是指三维空间加一维时间。2.四维空间，只指四个维度的空间。）四维运动产生了五维。</p>
<p>从哲学角度看，人们观察、思考与表述某事物的“<u>思维角度</u>”，简称“维度”。例如，人们观察与思考“月亮”这个事物，可以从月亮的“内容、时间、空间”三个思维角度去描述；也可以从月亮的“载体、能量、信息”三个思维角度去描述。这边的维度其实也可以理解为角度，从不同的方面去看待、确定一个事物。</p>
<p>所以代数上来说，维度其实是数学里在<strong>表示</strong>方面的一个重要的概念，它反映的是一个空间的本质性质。</p>
<h2 id="科学计算中维度的概念"><a class="markdownIt-Anchor" href="#科学计算中维度的概念"></a> 科学计算中维度的概念</h2>
<h3 id="从二维点位置-编程中的坐标系"><a class="markdownIt-Anchor" href="#从二维点位置-编程中的坐标系"></a> 从二维点位置-&gt;编程中的坐标系</h3>
<p>维度的考量主要集中在矩阵的运算上。首先我们来看一个元素：4，其实它就是一个点，可以被认为是0维的。但往往我们不会只有一个元素。我们最常见的是编程中的数组，如[1,2,3,4]，这个是由多个元素构成的，它的维度就是一维的，这个我们也比较好理解。</p>
<p>而二维是什么呢？我们能直观理解的二维是平面坐标系的那种：(1,3), (4,5)…即给一个x，一个y，那么在平面中就可以在直角坐标系下确定这个点（物）。现在我们规整下这些坐标点[ (1, 3), (4, 5) ]，从这个角度上离我们的矩阵，或是数组好像还是有点远。那么我们继续变形。</p>
<p>如果我们需要画出坐标系中有哪些点的话， 1.第一种做法就是跟上述一样， 把点都存一个vector中[ (1, 3), (4, 5) ]，然后遍历，再在坐标系中点出。2.第二种呢，就是在坐标系中把所有的位置都列出来，如果有点存在就把它标出来，即跟我们列出迷宫地图一样，先把地图画出来，然后再把宝藏标出来。所以上述的两个点可以理解为。在给出了<code>map[20][20]</code>的地图上，(1, 3)和(4, 5)位置为true， 即<code>map[1][3] = 1</code>, <code>map[4][5] = 1</code>，其他位置<code>map[x][y] = 0</code>，所以这样我们就从[(1, 3), (4, 5)]==&gt; 用map形式表现出了这两个点，两者成功在<strong>二维</strong>上进行了转换。接下来我们就来分析这个二维的map。</p>
<p>数组，在编程中，我们都不陌生，如<code>int arr[50][50]</code>，虽然可以通过这个二维的数组，根据val的不同来表示三维的量，但是我们这边不把它这么理解，仅是当做<code>bool arr[50][50]</code>来理解维度上的概念。===&gt;同样，面对numpy中的array我们也是这么个理解。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">x = np.random.randint(<span class="number">2</span>, size = (<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">y = torch.randint(<span class="number">2</span>, size = (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">print(y)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[[0 1 1]</span></span><br><span class="line"><span class="string"> [1 0 1]]</span></span><br><span class="line"><span class="string">tensor([[1, 0, 1],</span></span><br><span class="line"><span class="string">        [0, 0, 1]])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>我们从numpy的x上理解，这边是创建了一个2*3的矩阵，其中<code>x[0][1], x[0][2], x[1][0], x[0][2]</code>全1，其余为0，输入<code>x.shape</code>得到的结果是(2, 3)，有两项，跟我们从map的理解上是一致的，这个地图map拥有长和宽两个维度。</p>
<p>然后我们从编程中观察这个2*3的矩阵或是叫数组，可以发现<code>x[a][b]</code>第一个<code>[]</code>中的内容a范围是从0-1的，第一个<code>[]</code>中的内容b范围是从0-2的，0的话学编程的人都能很快的理解，而第二个的范围却不太那么肯定。为什么呢？因为它跟我们普通认知的直角坐标系不一致。下面我们把x画出来(不改变输出显示的形式，而是让坐标系去适应这种表现形式)。</p>
<p><img src="/2020/12/05/理解科学计算中的dim参数/%E5%9D%90%E6%A0%87%E7%B3%BB.png" alt="坐标系"></p>
<p>为什么是这样画的呢？首先明确的原则是，不改变输出显示的形式，而是让坐标系去适应这种输出形式，因此输出长啥样，我们坐标系只能去适应。由于我们碰到有x，有y的时候，习惯上把第一个出现的当作x，第二个当作y，所以就有了第一个[]为x，第二个[]为y。</p>
<p>好了，现在我们确定好坐标系长什么样了。接下来就是具体理解dim的含义了</p>
<h3 id="编程中坐标系-科学计算中array的dim"><a class="markdownIt-Anchor" href="#编程中坐标系-科学计算中array的dim"></a> 编程中坐标系-&gt;科学计算中array的dim</h3>
<blockquote>
<p>想必大家在学习numpy或者torch的时候都被各种函数方法中的dim参数折磨过，感觉怎么理解都有问题，不敢自己使用。因此，这边就是解决，这些函数中的dim到底是怎么确定的</p>
</blockquote>
<p>比如我们创建一个高维的array</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A = torch.randint(<span class="number">2</span>, size = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[[[0, 0, 1, 0],</span></span><br><span class="line"><span class="string">          [1, 1, 0, 1],</span></span><br><span class="line"><span class="string">          [0, 0, 0, 0]],</span></span><br><span class="line"><span class="string">         [[0, 0, 1, 1],</span></span><br><span class="line"><span class="string">          [1, 1, 1, 0],</span></span><br><span class="line"><span class="string">          [0, 1, 0, 0]]]])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>举个我自己最初理解dim的笨方法：硬记x为第一维(dim = 0)， y为第二维(dim = 1)</p>
<p>实际上这种记法是比较低效的，最好的方法是我们怎么定义这个array就怎么记，比如我们这边创建的是一个size=(1, 2, 3, 4)，输出<code>len(A.shape)</code>为4，可以看到这就是个4维的tensor，那么我们顺理成章地就把把各个维度依次定义出来了。如dim = 0地指的就是size = 1的那层，dim = 1就是指size = 2的那层，依次类推。这样说可能有点抽象，因此我们回归简单的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">B = torch.randint(<span class="number">2</span>, size = (<span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line">print(B)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[1, 0],</span></span><br><span class="line"><span class="string">        [1, 1],</span></span><br><span class="line"><span class="string">        [0, 0]])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>按照我们刚的定义，dim=0就是size=3的这一层，也就是我们坐标系中的X轴orX面。</p>
<p>好了，想必大家这个时候还不知道我在说什么。接下来就带大家来测试函数。</p>
<h4 id="测试dim在函数参数中的定义"><a class="markdownIt-Anchor" href="#测试dim在函数参数中的定义"></a> 测试dim在函数参数中的定义</h4>
<blockquote>
<p>提前指出把：要注意函数介绍中dim指的是&quot;<strong>沿着dim这个维度</strong>&quot;or&quot;<strong>删除、增加…dim这个维度(在dim这个维度上进行维度修改)</strong>&quot;</p>
</blockquote>
<h5 id="规约计算"><a class="markdownIt-Anchor" href="#规约计算"></a> 规约计算</h5>
<blockquote>
<p>一般是指分组聚合计算，表现结果就是会进行维度压缩</p>
</blockquote>
<h6 id="sum"><a class="markdownIt-Anchor" href="#sum"></a> sum</h6>
<blockquote>
<p>沿着dim累加元素</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">C = torch.randint(<span class="number">5</span>, size = (<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">print(C)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[2, 3, 3, 4, 0],</span></span><br><span class="line"><span class="string">        [1, 0, 2, 4, 4]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">print(C.sum(dim = <span class="number">0</span>))</span><br><span class="line">print(C.sum(dim = <span class="number">1</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([3, 3, 5, 8, 4])</span></span><br><span class="line"><span class="string">tensor([12, 11])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>可以看到sum就是比较典型的&quot;沿着dim&quot;的例子，当dim = 0时就沿着dim = 0即x轴进行累加，由于sum这个函数会进行维度的压缩，所以最后的结果为<code>tensor([3, 3, 5, 8, 4])</code></p>
<h6 id="cumprod"><a class="markdownIt-Anchor" href="#cumprod"></a> cumprod</h6>
<blockquote>
<p>通过dim指定沿着某个维度计算累积</p>
<p>其他的函数还有cumsum、prod、sum，实际上两者是相同的，还有mean、median、var、std、min、max</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line">x = torch.Tensor([</span><br><span class="line">    [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],</span><br><span class="line">    [<span class="number">9</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">6</span>,<span class="number">5</span>,]</span><br><span class="line">])</span><br><span class="line">print(torch.cumprod(x, dim = <span class="number">0</span>))</span><br><span class="line">print(torch.cumprod(x, dim = <span class="number">1</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[ 2.,  3.,  4.,  5.,  6.],</span></span><br><span class="line"><span class="string">        [18., 24., 28., 30., 30.]])</span></span><br><span class="line"><span class="string">tensor([[2.0000e+00, 6.0000e+00, 2.4000e+01, 1.2000e+02, 7.2000e+02],</span></span><br><span class="line"><span class="string">        [9.0000e+00, 7.2000e+01, 5.0400e+02, 3.0240e+03, 1.5120e+04]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># min</span></span><br><span class="line">x = torch.Tensor([</span><br><span class="line">    [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],</span><br><span class="line">    [<span class="number">9</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">6</span>,<span class="number">5</span>,]</span><br><span class="line">])</span><br><span class="line">print(torch.min(x, dim = <span class="number">0</span>))</span><br><span class="line">print(torch.min(x, dim = <span class="number">1</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">torch.return_types.min(</span></span><br><span class="line"><span class="string">    values=tensor([2., 3., 4., 5., 5.]),</span></span><br><span class="line"><span class="string">    indices=tensor([0, 0, 0, 0, 1]) )</span></span><br><span class="line"><span class="string">torch.return_types.min(</span></span><br><span class="line"><span class="string">    values=tensor([2., 5.]),</span></span><br><span class="line"><span class="string">    indices=tensor([0, 4]) )</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mean</span></span><br><span class="line">x = torch.Tensor([</span><br><span class="line">    [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],</span><br><span class="line">    [<span class="number">9</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">6</span>,<span class="number">5</span>,]</span><br><span class="line">])</span><br><span class="line">print(torch.mean(x, dim = <span class="number">0</span>))</span><br><span class="line">print(torch.mean(x, dim = <span class="number">1</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([5.5000, 5.5000, 5.5000, 5.5000, 5.5000])</span></span><br><span class="line"><span class="string">tensor([4., 7.])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h5 id="索引-切片-连接"><a class="markdownIt-Anchor" href="#索引-切片-连接"></a> 索引、切片、连接</h5>
<h6 id="squeezeunsqueeze"><a class="markdownIt-Anchor" href="#squeezeunsqueeze"></a> squeeze，unsqueeze</h6>
<blockquote>
<p>unsqueeze关键字：参数dim指定在第几个维度增加&quot;[]&quot;，以提升维度</p>
<p>squeeze: unsqueeze的逆操作，删除dim指定的维度</p>
</blockquote>
<p>unsqueeze</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">D = torch.Tensor( [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>] )</span><br><span class="line">y = D.unsqueeze(dim = <span class="number">0</span>)</span><br><span class="line">print(y, y.shape)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[1., 2., 3., 4., 5.]]) torch.Size([1, 5])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">y = D.unsqueeze(dim = <span class="number">1</span>)</span><br><span class="line">print(y.shape)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[1.],</span></span><br><span class="line"><span class="string">        [2.],</span></span><br><span class="line"><span class="string">        [3.],</span></span><br><span class="line"><span class="string">        [4.],</span></span><br><span class="line"><span class="string">        [5.]])  torch.Size([5, 1])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>可以看到的是dim = 0的时候就是在dim = 0 维度上增加了一维， 使得变成了[1, 5]。第二个是在dim=1的位置加了一维变成了[5, 1] （这也就是为什么很多书上会说其实就是在dim维度上加了1）</p>
<p>▲这个典型就是要区分： <strong>在dim维度上</strong> 和 <strong>沿着dim维度</strong>了</p>
<p>squeeze</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">F = torch.Tensor( [ [<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], </span><br><span class="line">                    [<span class="number">22</span>, <span class="number">33</span>, <span class="number">44</span> ,<span class="number">55</span>]</span><br><span class="line">                  ])</span><br><span class="line">y = torch.squeeze(F, dim = <span class="number">0</span>)</span><br><span class="line">print(y, y.shape)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[ 0.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [22., 33., 44., 55.]]) torch.Size([2, 4])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">y = torch.squeeze(F, dim = <span class="number">1</span>)</span><br><span class="line">print(y, y.shape)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[ 0.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [22., 33., 44., 55.]]) torch.Size([2, 4])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>这边变换不大的原因是因为dim上没有size=1可以删除</p>
<h6 id="split"><a class="markdownIt-Anchor" href="#split"></a> split</h6>
<blockquote>
<p><strong>按(沿着)dim维度</strong>将tensor分成n个部分</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>],</span><br><span class="line">    [<span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">])</span><br><span class="line">print(x)</span><br><span class="line">print(torch.split(x, <span class="number">5</span>, dim = <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 指定划分列表，表示依次有1,2,3,4个长度 (总和得跟dim维度上元素个数相同)</span></span><br><span class="line">print(torch.split(x, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], dim = <span class="number">1</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],</span></span><br><span class="line"><span class="string">        [10.,  9.,  8.,  7.,  6.,  5.,  4.,  3.,  2.,  1.]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">(tensor([								# 可以看到这个是在x[:][6]的地方将tensor切成了两个</span></span><br><span class="line"><span class="string">		[ 1.,  2.,  3.,  4.,  5.],</span></span><br><span class="line"><span class="string">        [10.,  9.,  8.,  7.,  6.]</span></span><br><span class="line"><span class="string">        ]), </span></span><br><span class="line"><span class="string">tensor([</span></span><br><span class="line"><span class="string">        [ 6.,  7.,  8.,  9., 10.],</span></span><br><span class="line"><span class="string">        [ 5.,  4.,  3.,  2.,  1.]</span></span><br><span class="line"><span class="string">        ]))</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">(tensor([[ 1.],</span></span><br><span class="line"><span class="string">        [10.]]), tensor([[2., 3.],</span></span><br><span class="line"><span class="string">        [9., 8.]]), tensor([[4., 5., 6.],</span></span><br><span class="line"><span class="string">        [7., 6., 5.]]), tensor([[ 7.,  8.,  9., 10.],</span></span><br><span class="line"><span class="string">        [ 4.,  3.,  2.,  1.]]))</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h6 id="unbind"><a class="markdownIt-Anchor" href="#unbind"></a> unbind</h6>
<blockquote>
<p>删除某个维度后，返回所有切片组成的<strong>元组</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">x = torch.rand(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># x = torch.rand(size=(1,2,3))</span></span><br><span class="line">print(x, x.shape)</span><br><span class="line">out = torch.unbind(x, dim = <span class="number">1</span>)</span><br><span class="line">print(out, len(out))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([</span></span><br><span class="line"><span class="string">		[</span></span><br><span class="line"><span class="string">            [0.3631, 0.6672, 0.9489],</span></span><br><span class="line"><span class="string">            [0.4944, 0.1606, 0.6122]</span></span><br><span class="line"><span class="string">		]</span></span><br><span class="line"><span class="string">         ])	torch.Size([1, 2, 3])</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">(</span></span><br><span class="line"><span class="string">    tensor([[0.3631, 0.6672, 0.9489]]), 		torch.Size([1, 3])</span></span><br><span class="line"><span class="string">    tensor([[0.4944, 0.1606, 0.6122]])			torch.Size([1, 3])</span></span><br><span class="line"><span class="string">)      2</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">x = torch.Tensor([</span><br><span class="line">        [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,],</span><br><span class="line">        [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],</span><br><span class="line">        [<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]],</span><br><span class="line">    </span><br><span class="line">        [[<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>],</span><br><span class="line">        [<span class="number">17</span>,<span class="number">18</span>,<span class="number">19</span>,<span class="number">20</span>],</span><br><span class="line">        [<span class="number">21</span>,<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>]]</span><br><span class="line">])</span><br><span class="line">print(x, x.shape)</span><br><span class="line">out = torch.unbind(x, dim = <span class="number">1</span>)</span><br><span class="line">print(out, len(out))</span><br><span class="line">print(out[<span class="number">0</span>].shape)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([</span></span><br><span class="line"><span class="string">        [[1,2,3,4,],</span></span><br><span class="line"><span class="string">        [5,6,7,8],</span></span><br><span class="line"><span class="string">        [9,10,11,12]],</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        [[13,14,15,16],</span></span><br><span class="line"><span class="string">        [17,18,19,20],</span></span><br><span class="line"><span class="string">        [21,22,23,24]]</span></span><br><span class="line"><span class="string">])										        torch.Size([2, 3, 4])</span></span><br><span class="line"><span class="string">(tensor([</span></span><br><span class="line"><span class="string">		[ 1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [13., 14., 15., 16.]]), </span></span><br><span class="line"><span class="string">tensor([</span></span><br><span class="line"><span class="string">		[ 5.,  6.,  7.,  8.],</span></span><br><span class="line"><span class="string">        [17., 18., 19., 20.]]), </span></span><br><span class="line"><span class="string">tensor([</span></span><br><span class="line"><span class="string">		[ 9., 10., 11., 12.],</span></span><br><span class="line"><span class="string">        [21., 22., 23., 24.]])) </span></span><br><span class="line"><span class="string">删除dim = 1, 把size[1] = 3的tensor拆成了3个tensor</span></span><br><span class="line"><span class="string">	不要记这个： 因为dim0为z轴， dim1为x轴， dim2为y轴，所以删除dim1就是删除x轴，最后得到的就是yOz平面</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h6 id="cat-stack"><a class="markdownIt-Anchor" href="#cat-stack"></a> cat、stack</h6>
<blockquote>
<p>通过关键字dim指定按哪个维度拼接</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randint(<span class="number">1</span>, <span class="number">100</span>, size=(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">print(x)</span><br><span class="line">y = torch.randint(<span class="number">1</span>, <span class="number">100</span>, size=(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">print(y)</span><br><span class="line">res = torch.cat((x, y), dim = <span class="number">1</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[71, 56, 44],</span></span><br><span class="line"><span class="string">        [64, 30, 87]])</span></span><br><span class="line"><span class="string">tensor([[39, 56, 63],</span></span><br><span class="line"><span class="string">        [68, 28, 65]])</span></span><br><span class="line"><span class="string">tensor([[71, 56, 44, 39, 56, 63],</span></span><br><span class="line"><span class="string">        [64, 30, 87, 68, 28, 65]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加强高维理解</span></span><br><span class="line">x = torch.randint(<span class="number">1</span>, <span class="number">100</span>, size=(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">print(x)</span><br><span class="line">y = torch.randint(<span class="number">1</span>, <span class="number">100</span>, size=(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">print(y)</span><br><span class="line">res = torch.cat((x, y), dim = <span class="number">1</span>)</span><br><span class="line">print(res)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[[81, 79, 10,  8],</span></span><br><span class="line"><span class="string">         [47, 30, 48, 35],</span></span><br><span class="line"><span class="string">         [10, 57, 68, 88]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[33, 51, 60, 97],</span></span><br><span class="line"><span class="string">         [27, 14, 83, 51],</span></span><br><span class="line"><span class="string">         [51, 54, 79, 65]]])</span></span><br><span class="line"><span class="string">tensor([[[85,  9, 95, 95],</span></span><br><span class="line"><span class="string">         [29, 99, 12,  8],</span></span><br><span class="line"><span class="string">         [32,  8,  3, 84]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[13, 24, 46, 20],</span></span><br><span class="line"><span class="string">         [86, 83, 72, 10],</span></span><br><span class="line"><span class="string">         [76, 33, 79, 48]]])</span></span><br><span class="line"><span class="string">tensor([[[81, 79, 10,  8],</span></span><br><span class="line"><span class="string">         [47, 30, 48, 35],</span></span><br><span class="line"><span class="string">         [10, 57, 68, 88],</span></span><br><span class="line"><span class="string">         [85,  9, 95, 95],</span></span><br><span class="line"><span class="string">         [29, 99, 12,  8],</span></span><br><span class="line"><span class="string">         [32,  8,  3, 84]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[33, 51, 60, 97],</span></span><br><span class="line"><span class="string">         [27, 14, 83, 51],</span></span><br><span class="line"><span class="string">         [51, 54, 79, 65],</span></span><br><span class="line"><span class="string">         [13, 24, 46, 20],</span></span><br><span class="line"><span class="string">         [86, 83, 72, 10],</span></span><br><span class="line"><span class="string">         [76, 33, 79, 48]]]) torch.Size([2, 6, 4])</span></span><br><span class="line"><span class="string">dim=1即沿元素为3的方向上延伸，所以结果变成了6</span></span><br><span class="line"><span class="string">	不要记：也可以理解为沿x轴方向</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>官方文档： <a href="https://pytorch.org/docs/stable/generated/torch.cat.html?highlight=cat#torch.cat" target="_blank" rel="noopener">https://pytorch.org/docs/stable/generated/torch.cat.html?highlight=cat#torch.cat</a></p>
<h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3>
<h4 id="正确理解姿势"><a class="markdownIt-Anchor" href="#正确理解姿势"></a> 正确理解姿势</h4>
<p>dim是指tensor在shape上的顺序(可以这么理解)，如x的shape是２ｘ３ｘ４，也就是[2, 3, 4]。故可以这样一一对应来。<br>
比如dim = 1就是按具有3个元素的那个轴操作，从而不用死记硬背那些dim = 0是对列操作还是对行操作了。</p>
<h4 id="强记三维"><a class="markdownIt-Anchor" href="#强记三维"></a> 强记三维</h4>
<p><img src="/2020/12/05/理解科学计算中的dim参数/3%E7%BB%B4.png" alt="3维"></p>
<p>但还是不提倡强记，因为一旦高维就理解不了了。</p>
<h2 id="附"><a class="markdownIt-Anchor" href="#附"></a> 附：</h2>
<h3 id="关于size的设置"><a class="markdownIt-Anchor" href="#关于size的设置"></a> 关于size的设置</h3>
<p>在ones、rand等函数上，size = (2,3,4)，我们在C++数组中<code>int arr[x][y][z]</code>的理解是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>∗</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">2*3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>然后z为4， 但实际上在科学运算中size = (2,3,4)的矩阵是有4个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mo>∗</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">3*4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span></span></span></span>的矩阵叠加而成，这边是要区分的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.random.randint(<span class="number">1</span>, <span class="number">100</span>, [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[[<span class="number">26</span>, <span class="number">36</span>, <span class="number">31</span>, <span class="number">21</span>],</span><br><span class="line">        [<span class="number">74</span>, <span class="number">59</span>, <span class="number">79</span>, <span class="number">32</span>],</span><br><span class="line">        [<span class="number">77</span>, <span class="number">94</span>, <span class="number">81</span>, <span class="number">32</span>]],</span><br><span class="line"></span><br><span class="line">       [[<span class="number">72</span>, <span class="number">76</span>, <span class="number">85</span>, <span class="number">93</span>],</span><br><span class="line">        [<span class="number">66</span>, <span class="number">34</span>, <span class="number">80</span>, <span class="number">12</span>],</span><br><span class="line">        [<span class="number">99</span>, <span class="number">17</span>, <span class="number">98</span>, <span class="number">23</span>]]])</span><br><span class="line"></span><br><span class="line">x = torch.randint(<span class="number">1</span>, <span class="number">100</span>, size=(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">print(x)</span><br><span class="line">print(x[<span class="number">1</span>][<span class="number">2</span>][<span class="number">3</span>])		<span class="comment"># 高度索引为1的, 在x = 2, y = 3的元素就是76</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[[63, 54, 57, 17],</span></span><br><span class="line"><span class="string">         [78, 64, 76, 44],</span></span><br><span class="line"><span class="string">         [96,  3, 59, 37]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[86,  3, 92, 84],</span></span><br><span class="line"><span class="string">         [89, 36,  8, 79],</span></span><br><span class="line"><span class="string">         [10, 87, 15, 76]]])</span></span><br><span class="line"><span class="string">tensor(76)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">x = torch.randint(<span class="number">1</span>, <span class="number">100</span>, size=(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>))</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[[[29, 50],</span></span><br><span class="line"><span class="string">          [50, 69],</span></span><br><span class="line"><span class="string">          [95, 70],</span></span><br><span class="line"><span class="string">          [21, 35]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[58, 65],</span></span><br><span class="line"><span class="string">          [15, 53],</span></span><br><span class="line"><span class="string">          [96, 25],</span></span><br><span class="line"><span class="string">          [11, 75]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[12, 71],</span></span><br><span class="line"><span class="string">          [36, 12],</span></span><br><span class="line"><span class="string">          [71, 92],</span></span><br><span class="line"><span class="string">          [87, 47]]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[[43, 89],</span></span><br><span class="line"><span class="string">          [88, 22],</span></span><br><span class="line"><span class="string">          [61, 56],</span></span><br><span class="line"><span class="string">          [47, 97]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[71,  7],</span></span><br><span class="line"><span class="string">          [44, 88],</span></span><br><span class="line"><span class="string">          [54, 32],</span></span><br><span class="line"><span class="string">          [15, 65]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[96, 22],</span></span><br><span class="line"><span class="string">          [90, 78],</span></span><br><span class="line"><span class="string">          [30, 85],</span></span><br><span class="line"><span class="string">          [65, 57]]]])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h3 id="关于如何看图"><a class="markdownIt-Anchor" href="#关于如何看图"></a> 关于如何看图</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">res = np.uint8(np.random.rand(<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>) * <span class="number">255</span>)</span><br><span class="line">print(res)</span><br><span class="line">plt.imshow(res)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[[ 19 228 231]</span></span><br><span class="line"><span class="string">  [106  28 252]</span></span><br><span class="line"><span class="string">  [191  98 139]</span></span><br><span class="line"><span class="string">  [171  71 202]</span></span><br><span class="line"><span class="string">  [105 101  93]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[155 232 228]</span></span><br><span class="line"><span class="string">  [ 89 119  13]</span></span><br><span class="line"><span class="string">  [142 158 200]</span></span><br><span class="line"><span class="string">  [226 169  55]</span></span><br><span class="line"><span class="string">  [137 187 249]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[202 217  96]</span></span><br><span class="line"><span class="string">  [214  44 133]</span></span><br><span class="line"><span class="string">  [144 253 213]</span></span><br><span class="line"><span class="string">  [ 82   4  28]</span></span><br><span class="line"><span class="string">  [172 242 238]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[106 103  68]</span></span><br><span class="line"><span class="string">  [236 252  63]</span></span><br><span class="line"><span class="string">  [ 53  49  66]</span></span><br><span class="line"><span class="string">  [ 48 121  62]</span></span><br><span class="line"><span class="string">  [ 64  61 209]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[158  80 221]</span></span><br><span class="line"><span class="string">  [220  12 199]</span></span><br><span class="line"><span class="string">  [214  40   3]</span></span><br><span class="line"><span class="string">  [169 142 144]</span></span><br><span class="line"><span class="string">  [140 112 148]]]</span></span><br><span class="line"><span class="string">  """</span></span><br></pre></td></tr></table></figure>
<p>画图效果如下：</p>
<p><img src="/2020/12/05/理解科学计算中的dim参数/%E5%9B%BE.png" alt="图"></p>
<p>可以得到的结果是针对<code>np.random.rand(5,5,3)</code>来说，三个通道被列成了三列，因此每一列就是一个通道。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">res = np.uint8(np.random.rand(<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>) * <span class="number">255</span>)</span><br><span class="line">[[[ <span class="number">19</span> <span class="number">228</span> <span class="number">231</span>]</span><br><span class="line">  [<span class="number">106</span>  <span class="number">28</span> <span class="number">252</span>]</span><br><span class="line">  [<span class="number">191</span>  <span class="number">98</span> <span class="number">139</span>]</span><br><span class="line">  [<span class="number">171</span>  <span class="number">71</span> <span class="number">202</span>]</span><br><span class="line">  [<span class="number">105</span> <span class="number">101</span>  <span class="number">93</span>]]</span><br><span class="line">  </span><br><span class="line">res = np.uint8(np.random.rand(<span class="number">5</span>,<span class="number">6</span>,<span class="number">3</span>) * <span class="number">255</span>)</span><br><span class="line">[[[<span class="number">217</span> <span class="number">210</span> <span class="number">138</span>]</span><br><span class="line">  [  <span class="number">1</span> <span class="number">148</span>  <span class="number">10</span>]</span><br><span class="line">  [ <span class="number">25</span> <span class="number">231</span> <span class="number">128</span>]</span><br><span class="line">  [<span class="number">158</span> <span class="number">216</span>  <span class="number">73</span>]</span><br><span class="line">  [<span class="number">220</span> <span class="number">208</span> <span class="number">165</span>]</span><br><span class="line">  [ <span class="number">73</span> <span class="number">119</span> <span class="number">122</span>]]</span><br><span class="line"><span class="comment"># 单元中竖着的行数表示了图片的长，可以看到横着的长是6</span></span><br><span class="line"><span class="comment"># 而有多少个单元就以为着图片的宽， 可以看到竖着的宽是5==&gt; 所以第几个单元意味着第几行</span></span><br><span class="line"><span class="comment"># 所以[0][5][0]=73标志着右上角那一块绿色的第一个通道的值</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可以设置后再绘图看看是不是右上角的图块变成白色的了</span></span><br><span class="line">res[<span class="number">0</span>][<span class="number">5</span>][<span class="number">0</span>] = res[<span class="number">0</span>][<span class="number">5</span>][<span class="number">1</span>] = res[<span class="number">0</span>][<span class="number">5</span>][<span class="number">2</span>] = <span class="number">255</span></span><br><span class="line">print(res[<span class="number">0</span>][<span class="number">5</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>总结： RGB图像的shape：(H, W, C)</p>
<h3 id="pytorchapi"><a class="markdownIt-Anchor" href="#pytorchapi"></a> pytorchAPI:</h3>
<p><a href="https://pytorch.org/docs/stable/torch.html#torch.arange" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torch.html#torch.arange</a></p>
</article><!-- lincense--><div class="license-wrapper"><p> <span>Author:  </span><a href="https://nymrli.top">Mrli</a></p><p> <span>Link:  </span><a href="https://nymrli.top/2020/12/05/理解科学计算中的dim参数/">https://nymrli.top/2020/12/05/理解科学计算中的dim参数/</a></p><p> <span>Copyright:  </span><span>All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/3.0">CC BY-NC-SA 3.0</a> unless stating additionally.</span></p></div><div class="post-paginator"><a class="prevSlogan" href="/2020/12/09/浙大2020春夏-人工智能习题3——图像恢复/" title="浙大2020春夏-人工智能习题3——图像恢复"><span>< PreviousPost</span><br><span class="prevTitle">浙大2020春夏-人工智能习题3——图像恢复</span></a><a class="nextSlogan" href="/2020/12/03/Lets-learn-设计模式/" title="Lets learn 设计模式"><span>NextPost ></span><br><span class="nextTitle">Lets learn 设计模式</span></a><div class="clear"></div></div><div id="comment"><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  id: '理解科学计算(numpy,pytorch)中的dim参数',
  owner: 'Freedomisgood',
  repo: 'Freedomisgood.github.io',
  oauth: {
    client_id: 'bc5a81fe36017dcd8b63',
    client_secret: '949cec3a1b91742c6249c47259791e4b80a6fa69',
  },
})
gitment.render('container')</script></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><p class="beian"><a href="https://beian.miit.gov.cn/"> <span>备案号:苏ICP备18015439号</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 60vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#维度"><span class="toc-number">1.</span> <span class="toc-text"> 维度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#科学计算中维度的概念"><span class="toc-number">2.</span> <span class="toc-text"> 科学计算中维度的概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#从二维点位置-编程中的坐标系"><span class="toc-number">2.1.</span> <span class="toc-text"> 从二维点位置-&gt;编程中的坐标系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编程中坐标系-科学计算中array的dim"><span class="toc-number">2.2.</span> <span class="toc-text"> 编程中坐标系-&gt;科学计算中array的dim</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#测试dim在函数参数中的定义"><span class="toc-number">2.2.1.</span> <span class="toc-text"> 测试dim在函数参数中的定义</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#规约计算"><span class="toc-number">2.2.1.1.</span> <span class="toc-text"> 规约计算</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#sum"><span class="toc-number">2.2.1.1.1.</span> <span class="toc-text"> sum</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#cumprod"><span class="toc-number">2.2.1.1.2.</span> <span class="toc-text"> cumprod</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#索引-切片-连接"><span class="toc-number">2.2.1.2.</span> <span class="toc-text"> 索引、切片、连接</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#squeezeunsqueeze"><span class="toc-number">2.2.1.2.1.</span> <span class="toc-text"> squeeze，unsqueeze</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#split"><span class="toc-number">2.2.1.2.2.</span> <span class="toc-text"> split</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#unbind"><span class="toc-number">2.2.1.2.3.</span> <span class="toc-text"> unbind</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#cat-stack"><span class="toc-number">2.2.1.2.4.</span> <span class="toc-text"> cat、stack</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总结"><span class="toc-number">2.3.</span> <span class="toc-text"> 总结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#正确理解姿势"><span class="toc-number">2.3.1.</span> <span class="toc-text"> 正确理解姿势</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#强记三维"><span class="toc-number">2.3.2.</span> <span class="toc-text"> 强记三维</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#附"><span class="toc-number">3.</span> <span class="toc-text"> 附：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#关于size的设置"><span class="toc-number">3.1.</span> <span class="toc-text"> 关于size的设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#关于如何看图"><span class="toc-number">3.2.</span> <span class="toc-text"> 关于如何看图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pytorchapi"><span class="toc-number">3.3.</span> <span class="toc-text"> pytorchAPI:</span></a></li></ol></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>