<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Mrli"><meta name="renderer" content="webkit"><meta name="copyright" content="Mrli"><meta name="keywords" content="Mrli's Blog"><meta name="description" content="想和你讲，说了会心动 ，缄默会心安。"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>深度学习+Pytorch学习笔记 · Mr.li's Blog</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/favicon.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="https://avatars1.githubusercontent.com/u/31088082?s=400&amp;u=7a99ff83916afb3f4c5312bd78a1be17fe0e34ed&amp;v=4"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Mrli</div><div class="profile-signature">别装作很努力,<br>因为结局不会陪你演戏。</div><div class="contacts"><div>Contacts:</div><span><a href="http://sighttp.qq.com/msgrd?v=1&amp;uin=1063052964" target="_black">QQ</a></span><span><a href="https://www.cnblogs.com/nymrli/" target="_black">博客园</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 60vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">Mr.li's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">深度学习+Pytorch学习笔记</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-edit"></i><span>2020/11/28</span></span><span class="busuanzi-pv" id="busuanzi_container_page_pv"><i class="post-intro-calendar fa fa-user-o"></i><span id="busuanzi_value_page_pv"></span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="机器学习"> 机器学习</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="读书笔记"> 读书笔记</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">6,133</span> | Reading time: <span class="post-count">29</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><blockquote>
<p>在课程设计实验周里总共读了3本书， 《深度学习算法实践》、《深度学习框架PyTorch快速开发与实战》、《PyTorch深度学习入门》。 其中《深度学习算法实践》、《深度学习框架PyTorch快速开发与实战》比较失望，质量比较差；《PyTorch深度学习入门》的代码质量高一点，但需要理论辅助，由于事先看了李宏毅老师的深度学习，因此还算容易入手。</p>
</blockquote>
<h1 id="深度学习框架pytorch快速开发与实战"><a class="markdownIt-Anchor" href="#深度学习框架pytorch快速开发与实战"></a> 《深度学习框架PyTorch快速开发与实战》</h1>
<p>网上关于《深度学习框架PyTorch快速开发与实战》的评价：</p>
<blockquote>
<h2 id="关于作者"><a class="markdownIt-Anchor" href="#关于作者"></a> 关于作者</h2>
<ul>
<li>作者一共三个，在封面内侧有简单的介绍, 三个作者的简绍里没有“深度学习”相关的内容（请注意书名）.单从这本书的作者介绍来说，这几个作者是很不专业的，换言之，这几个人可能未必够资格写这么一本书（就事论事，没有任何不尊重人的意思）</li>
<li>代码质量差到离谱
<ul>
<li>PyTorch版本问题（这个是技术相关书籍都会遇到的问题，可以接受）</li>
<li>代码不全，照着书上的代码敲下来，会发现莫名其妙就少了一部分</li>
</ul>
</li>
</ul>
</blockquote>
<p>我第一本代码书看的就是这本， 当时觉得还行， 但在看完《PyTorch深度学习入门》后打算也敲下这本书的代码，结果发现给的样例代码很多都跑不过，并且初看时对每个函数的详细介绍现在看来可能是因为没东西写了，内容非常重复。</p>
<p>▲.以后选书还是得看下作者是否相关专业</p>
<h3 id="常用距离公式"><a class="markdownIt-Anchor" href="#常用距离公式"></a> 常用距离公式</h3>
<p>（1）闵可夫斯基距离<br>
闵可夫斯基距离（Mlinkowski distance）是衡量数值点之间距离的一种非常常见的方法。<br>
闵可夫斯基距离定义为：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow><mo fence="true">(</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msup><mrow><mo fence="true">∣</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo fence="true">∣</mo></mrow><mi>p</mi></msup><mo fence="true">)</mo></mrow><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{p}\right)^{1 / p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3819020000000002em;vertical-align:-0.29971000000000003em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-3.2029000000000005em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.082192em;"><span style="top:-3.257192em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">/</span><span class="mord mathdefault mtight">p</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>该距离最常用的p是2和1，前者是欧几里得距离（Euclidean distance），后者是曼哈顿距离（Manhattan distance）。当p趋近于无穷大时，闵可夫斯基距离转化成切比雪夫距离（Chebyshev distance）：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mo><mi>lim</mi><mo>⁡</mo></mo><mrow><mi>p</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow></msub><msup><mrow><mo fence="true">(</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mo fence="true">∣</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo fence="true">∣</mo></mrow><mi>p</mi><mo fence="true">)</mo></mrow><mfrac><mn>1</mn><mi>p</mi></mfrac></msup><mo>=</mo><msubsup><mo><mi>max</mi><mo>⁡</mo></mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mo fence="true">∣</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo fence="true">∣</mo></mrow></mrow><annotation encoding="application/x-tex">\lim _{p \rightarrow \infty}\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right| p\right)^{\frac{1}{p}}=\max _{i=1}^{n}\left|x_{i}-y_{i}\right|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4479220000000002em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop">lim</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mrel mtight">→</span><span class="mord mtight">∞</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">p</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.148212em;"><span style="top:-3.5571919999999997em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443142857142858em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.48288571428571425em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.008664em;vertical-align:-0.258664em;"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span></span></span></span></p>
<p>（2）马氏距离（Mahalanobis distance）是由印度统计学家马哈拉诺比斯（PC.Mahalanobis）提出的，表示数据的协方差距离。它是一种有效地计算两个未知样本集的相似度的方法。与欧氏距离不同的是它考虑各种特性之间的联系（例如：一条关于身高的信息会带来一条关于体重的信息，因为两者是有关联的。），并且是尺度无关的（Scale-invariant），即独立于测量尺度。对于一个均值为u，协方差矩阵为的多变量向量，其马氏距离为（x-u）Z（-1）（xu）。</p>
<p>（3）余弦相似度，又称为余弦相似性。通过计算两个向量的夹角余弦值来评估它们的相似度。<br>
假设向量a、b的坐标分别为（×1，y1）、（×2，y2）。则：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>cos</mi><mo>⁡</mo><mi>θ</mi><mo>=</mo><mfrac><mrow><mi>a</mi><mo>∙</mo><mi>b</mi></mrow><mrow><mi mathvariant="normal">∥</mi><mi>a</mi><mi mathvariant="normal">∥</mi><mi mathvariant="normal">∥</mi><mi>b</mi><mi mathvariant="normal">∥</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\cos \theta=\frac{a \bullet b}{\|a\|\|b\|}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mop">cos</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.400108em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mathdefault mtight">a</span><span class="mord mtight">∥</span><span class="mord mtight">∥</span><span class="mord mathdefault mtight">b</span><span class="mord mtight">∥</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="mbin mtight">∙</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<h3 id="数据标准化处理"><a class="markdownIt-Anchor" href="#数据标准化处理"></a> 数据标准化处理</h3>
<p>数据标准化（归一化）处理，是为了消除指标之间的量纲影响，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。标准化就是一种对样本数据在不同维度上进行一个伸缩变化，也就是不改变原始数据的信息（分布）。这样的好处就是在进行特征提取时，忽略掉不同特征之间的一个度量，而保留样本在各个维度上的信息（分布）。</p>
<p>a.Min-Max标准化（Min-Max Normalization）也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0, 1]之间。转换函数如下：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>X</mi><mo>∗</mo></msup><mo>=</mo><mfrac><mrow><mi>X</mi><mo>−</mo><mi>min</mi><mo>⁡</mo></mrow><mrow><mi>max</mi><mo>⁡</mo><mo>−</mo><mi>min</mi><mo>⁡</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">X^{*}=\frac{X-\min }{\max -\min }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.688696em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.275662em;vertical-align:-0.403331em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight">max</span><span class="mspace mtight" style="margin-right:0.19516666666666668em;"></span><span class="mord mtight">−</span><span class="mspace mtight" style="margin-right:0.19516666666666668em;"></span><span class="mop mtight">min</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07847em;">X</span><span class="mbin mtight">−</span><span class="mop mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.403331em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。这种方法有个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。</p>
<p>b.Z-score标准化方法<br>
这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。经过处理的数据符合标准正态分布，即均值为0，标准差为1，转化函数为X*= \frac{X-μ}{σ}，其中u为所有样本数据的均值，σ为所有样本数据的标准差。</p>
<h3 id="正则化"><a class="markdownIt-Anchor" href="#正则化"></a> 正则化</h3>
<p>为了防止过拟合现象，我们加入了正则化项，常用的有 L1范数和 L2范数。<br>
常用的向量的范数如下。<br>
L0范数：||x||0为x向量各个非零元素的个数。<br>
L1范数：||x||1为x向量各个元素绝对值之和，也叫“稀疏规则算子”（Lasso Regularization）。<br>
L2范数：||x||2为 x 向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或者Frobenius范数。在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减（Weight Decay）”。<br>
Lp范数：||x||为x向量各个元素绝对值p次方和的1/p次方 […]</p>
<h3 id="成本函数优化函数"><a class="markdownIt-Anchor" href="#成本函数优化函数"></a> 成本函数（优化函数）</h3>
<p>在训练神经网络时，必须评估网络输出的正确性。众所周知，预期上正确的训练输出数据和实际的训练输出是可比拟的。成本函数便是测量实际和训练输出之间的差异。实际和预期输出之间的零成本将意味着训练神经网络成为可能。<br>
<strong>BGD即Batch Gradient Descent</strong>。在训练中，每一步迭代都使用训练集的所有内容。也就是利用现有参数对训练集中的每一个输入生成一个估计输出，然后跟实际输出比较，统计所有误差，求平均以后得到平均误差，以此来作为更新参数的依据。<br>
由于每一步都利用了训练集中的所有数据，因此当损失函数达到最小值以后，能够保证此时计算出的梯度为0，换句话说，就是能够收敛。因此，使用BGD时不需要逐渐减小学习速率。由于每一步都要使用所有数据，因此随着数据集的增大，运行速度会越来越慢。在批量梯度下降法中，因为每次都遍历了完整的训练集，其能保证结果为全局最优，但是也因为我们需要对每个参数求偏导，且在对每个参数求偏导的过程中还需要对训练集遍历一次，当训练集很大时，这个计算量是惊人的！<br>
所以，为了提高速度，减少计算量，提出了SGD随机梯度下降的方法，该方法每次随机选取一个样本进行梯度计算，大大降低了计算成本。<strong>SGD全名Stochastic Gradient Descent</strong>，即随机梯度下降。即随机抽取一批样本，以此为根据来更新参数。随机梯度下降算法和批量梯度下降的不同点在于其梯度是根据随机选取的训练集样本来决定的，其每次对theta的更新，都是针对单个样本数据，并没有遍历完整的参数。当样本数据很大时，可能到迭代完成，也只不过遍历了样本中的一小部分。因此，其速度较快，但是其每次的优化方向不一定是全局最优的，但最终的结果是在全局最优解的附近。</p>
<p><strong>Momentum Momentum</strong><br>
借用了物理中的动量概念，即前几次的梯度也会参与运算。<br>
为了表示动量，引入了一个新的变量v（velocity）。v是之前的梯度的累加，但是每回合都有一定的衰减。前后梯度方向一致时，能够加速学习。前后梯度方向不一致时，能够抑制震荡。<br>
Nesterov Momentum这是对之前的Momentum的一种改进，大概思路就是，先对参数进行估计，然后使用估计后的参数来计算误差。</p>
<p><strong>AdaGrad AdaGrad</strong><br>
可以自动变更学习速率，只需要设定一个全局的学习速率，但是这并非是实际学习速率，实际的速率是与以往参数的模之和的开方成反比的。<br>
它能够实现学习率的自动更改。如果这次梯度大，那么学习速率衰减就快一些；如果这次梯度小，那么学习速率衰减就慢一些。但仍然要设置一个变量。<br>
经验表明，在普通算法中也许效果不错，但在深度学习中，深度过深时会造成训练提前结束。</p>
<p><strong>RMSProp</strong><br>
RMSProp 通过引入一个衰减系数r，让r每回合都衰减一定比例，类似于Momentum中的做法。相比于AdaGrad，这种方法很好地解决了深度学习中过早结束的问题。</p>
<h1 id="pytorch深度学习入门"><a class="markdownIt-Anchor" href="#pytorch深度学习入门"></a> 《PyTorch深度学习入门》</h1>
<p>入门coding：</p>
<h2 id="线性回归代码"><a class="markdownIt-Anchor" href="#线性回归代码"></a> 线性回归代码</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Produce_X</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 增加一维全1给bias</span></span><br><span class="line">    x0 = torch.ones( x.numpy().size)</span><br><span class="line">    <span class="comment"># dim = 0, 添加行； dim = 1, 添加列</span></span><br><span class="line">    X = torch.stack( (x, x0), dim = <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = torch.Tensor([ <span class="number">1.4</span>, <span class="number">5</span>, <span class="number">11</span>, <span class="number">16</span>, <span class="number">21</span> ])</span><br><span class="line">y = torch.Tensor( [<span class="number">14.4</span>, <span class="number">29.6</span>, <span class="number">62</span>, <span class="number">85.5</span>, <span class="number">113.4</span>])</span><br><span class="line"></span><br><span class="line">X = Produce_X(x)</span><br><span class="line"></span><br><span class="line">inputs = X</span><br><span class="line">targets = y</span><br><span class="line">w = torch.rand(<span class="number">2</span>, requires_grad = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(outputs, loss)</span>:</span></span><br><span class="line">    plt.cla()</span><br><span class="line">    plt.scatter( x.numpy(), y.numpy() )</span><br><span class="line">    plt.plot( x.numpy(), outputs.data.numpy(), <span class="string">'r-'</span>, lw = <span class="number">5</span>)</span><br><span class="line">    plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">'loss=%s'</span> % (loss.item()), fontdict = &#123;</span><br><span class="line">        <span class="string">'size'</span>:<span class="number">20</span>, </span><br><span class="line">        <span class="string">'color'</span>:<span class="string">'red'</span></span><br><span class="line">    &#125;)</span><br><span class="line">    plt.pause(<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epochs = <span class="number">1</span>, lr = <span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> range(epochs):</span><br><span class="line">        outputs = inputs.mv(w)</span><br><span class="line">        print(outputs.size())</span><br><span class="line">        print(targets.shape)</span><br><span class="line">        loss = (outputs - targets).pow(<span class="number">2</span>).sum()</span><br><span class="line">        loss.backward()</span><br><span class="line">        w.data = w.data - lr * w.grad</span><br><span class="line">        w.grad.zero_()<span class="comment">#更新完grad后, 必须清空grad, 否则会累加</span></span><br><span class="line"><span class="comment">#         if epochs % 80 == 0:</span></span><br><span class="line"><span class="comment">#             draw(outputs, loss)</span></span><br><span class="line">    <span class="keyword">return</span> w, loss</span><br><span class="line"></span><br><span class="line">w, loss = train(<span class="number">1</span>, lr = <span class="number">1e-4</span>)</span><br><span class="line">print(<span class="string">"final loss:"</span>, loss.item())</span><br><span class="line">print(<span class="string">"weights:"</span>, w.data)</span><br></pre></td></tr></table></figure>
<h2 id="人工神经元-线性回归"><a class="markdownIt-Anchor" href="#人工神经元-线性回归"></a> 人工神经元-线性回归</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Produce_X</span><span class="params">(x)</span>:</span></span><br><span class="line">	x0 = torch.ones(x.numpy().size) <span class="comment">#用ones产生初始值为1，大小与x相同的向量</span></span><br><span class="line">	X = torch.stack((x,x0),dim=<span class="number">1</span>)   <span class="comment">#stack函数将两个向量拼合</span></span><br><span class="line">	<span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = torch.linspace(<span class="number">-3</span>,<span class="number">3</span>,<span class="number">100000</span>)<span class="comment">#用linspace产生（-3，3）区间内的100000个点</span></span><br><span class="line">X = Produce_X(x)</span><br><span class="line">y = x +<span class="number">1.2</span>*torch.rand(x.size())<span class="comment">#假设真实函数是y=x，我们在上面增加一些误差，更加符合实际情况</span></span><br><span class="line">w = torch.rand(<span class="number">2</span>) <span class="comment">#定义权重w的变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意这边的x是x， 而不是X， X是神经网络的输入， 被加上了bias的</span></span><br><span class="line">plt.scatter(x.numpy(),y.numpy(),s=<span class="number">0.001</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CUDA = torch.cuda.is_available()</span><br><span class="line"><span class="keyword">if</span> CUDA:</span><br><span class="line">    inputs = X.cuda()</span><br><span class="line">    targets = Y.cuda()</span><br><span class="line">    w = w.cuda()</span><br><span class="line">    w.requires_grad = <span class="keyword">True</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    inputs = X</span><br><span class="line">    targets = Y</span><br><span class="line">    w = w</span><br><span class="line">    w.requires_grad = Truue</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(outputs, loss)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> CUDA:</span><br><span class="line">        outputs = outputs.cpu()</span><br><span class="line">    plt.cla()</span><br><span class="line">    plt.scatter( x.numpy(), y.numpy() )</span><br><span class="line">    plt.plot( x.numpy(), outputs.data.numpy(), <span class="string">'r-'</span>, lw = <span class="number">5</span>)</span><br><span class="line">    plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">'loss=%s'</span> % (loss.item()), fontdict = &#123;</span><br><span class="line">        <span class="string">'size'</span>:<span class="number">20</span>, </span><br><span class="line">        <span class="string">'color'</span>:<span class="string">'red'</span></span><br><span class="line">    &#125;)</span><br><span class="line">    plt.pause(<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epochs = <span class="number">1</span>, lr = <span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> range(epochs):</span><br><span class="line">        outputs = inputs.mv(w)</span><br><span class="line">        loss = (outputs - targets).pow(<span class="number">2</span>).sum()</span><br><span class="line">        loss.backward()</span><br><span class="line">        w.data = w.data - lr * w.grad</span><br><span class="line">        w.grad.zero_()<span class="comment">#更新完grad后, 必须清空grad, 否则会累加</span></span><br><span class="line"><span class="comment">#         if epochs % 80 == 0:</span></span><br><span class="line"><span class="comment">#             draw(outputs, loss)</span></span><br><span class="line">    <span class="keyword">return</span> w, loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> perf_counter</span><br><span class="line">start = perf_counter()</span><br><span class="line">w, loss = train(<span class="number">1000</span>, lr = <span class="number">1e-4</span>)</span><br><span class="line">finish = perf_counter()</span><br><span class="line">t = finish - start</span><br><span class="line">print(<span class="string">"计算时间:"</span>, t)</span><br><span class="line">print(<span class="string">"final loss:"</span>, loss.item())</span><br><span class="line">print(<span class="string">"w:"</span>, w.data)</span><br><span class="line">print(type(loss))</span><br></pre></td></tr></table></figure>
<h3 id="人工神经元-线性回归-module"><a class="markdownIt-Anchor" href="#人工神经元-线性回归-module"></a> 人工神经元-线性回归-module</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> perf_counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Any</span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">1000</span>), dim = <span class="number">1</span>)</span><br><span class="line">y = x + <span class="number">1.2</span> * torch.rand(x.size())</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LR</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LR, self).__init__()</span><br><span class="line">        self.liner = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x: Any)</span>:</span></span><br><span class="line">        out = self.liner(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果支持CUDA，则采用CUDA加速</span></span><br><span class="line">CUDA = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> CUDA:</span><br><span class="line">    LR_model = LR().cuda()</span><br><span class="line">    inputs = x.cuda()</span><br><span class="line">    target = y.cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    LR_model = LR()</span><br><span class="line">    inputs = x</span><br><span class="line">    target = y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(LR_model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(output, loss)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> CUDA:</span><br><span class="line">        output = output.cpu()</span><br><span class="line">    plt.cla()</span><br><span class="line">    plt.scatter(x.numpy(), y.numpy(), marker=<span class="string">'.'</span>)</span><br><span class="line">    plt.plot(x.numpy(), output.data.numpy(), <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br><span class="line">    plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">'Loss=%.8s'</span> % (loss.item()), fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>: <span class="string">'red'</span>&#125;)</span><br><span class="line">    plt.pause(<span class="number">0.005</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, criterion, optimizer, epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> range(epochs):</span><br><span class="line">        output = model(inputs)</span><br><span class="line">        loss = criterion(output, target)</span><br><span class="line">        optimizer.zero_grad()<span class="comment"># 清空权重的grad</span></span><br><span class="line">        loss.backward()<span class="comment"># 计算梯度</span></span><br><span class="line">        optimizer.step()<span class="comment"># 利用梯度更新权重</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ep % <span class="number">80</span> == <span class="number">0</span>:</span><br><span class="line">            draw(output, loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model, loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">start = perf_counter()</span><br><span class="line">LR_model,loss = train(LR_model,criterion,optimizer,<span class="number">10000</span>)</span><br><span class="line">finish = perf_counter()</span><br><span class="line">time = finish-start</span><br><span class="line">print(<span class="string">"计算时间:%s"</span> % time)</span><br><span class="line">print(<span class="string">"final loss:"</span>,loss.item())</span><br><span class="line">print(<span class="string">"weights:"</span>,list(LR_model.parameters()))</span><br></pre></td></tr></table></figure>
<h2 id="非线性回归"><a class="markdownIt-Anchor" href="#非线性回归"></a> 非线性回归</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> perf_counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Any</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(output, loss)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> CUDA:</span><br><span class="line">        output = output.cpu()</span><br><span class="line">    plt.cla()</span><br><span class="line">    plt.scatter(x.numpy(), y.numpy(), marker=<span class="string">'.'</span>)</span><br><span class="line">    plt.plot(x.numpy(), output.data.numpy(), <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br><span class="line">    plt.text(<span class="number">-2</span>, <span class="number">-10</span>, <span class="string">'Loss=%.6s'</span> % (loss.item()), fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>: <span class="string">'red'</span>&#125;)</span><br><span class="line">    plt.pause(<span class="number">0.005</span>)</span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">1000</span>), dim=<span class="number">1</span>)</span><br><span class="line">y = x.pow(<span class="number">3</span>) + <span class="number">0.3</span> * torch.rand(x.size())</span><br><span class="line"></span><br><span class="line">plt.scatter(x.numpy(), y.numpy())</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_feature, num_hidden, outpus)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.hidden = nn.Linear(input_feature, num_hidden)</span><br><span class="line">        self.out = nn.Linear(num_hidden, outpus)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = nn.functional.relu(self.hidden(x))</span><br><span class="line">        x = self.out(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果支持CUDA，则采用CUDA加速</span></span><br><span class="line">CUDA = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> CUDA:</span><br><span class="line">    net = Net(input_feature= <span class="number">1</span>, num_hidden= <span class="number">20</span>, outpus= <span class="number">1</span>).cuda()</span><br><span class="line">    inputs = x.cuda()</span><br><span class="line">    target = y.cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    net = Net(input_feature= <span class="number">1</span>, num_hidden= <span class="number">20</span>, outpus= <span class="number">1</span>)</span><br><span class="line">    inputs = x</span><br><span class="line">    target = y</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, criterion, optimizer, epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> range(epochs):</span><br><span class="line">        output = model(inputs)</span><br><span class="line">        loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ep % <span class="number">80</span> == <span class="number">0</span>:</span><br><span class="line">            draw(output, loss)</span><br><span class="line">    <span class="keyword">return</span> model, loss</span><br><span class="line"></span><br><span class="line">net, loss = train(net, criterion, optimizer, <span class="number">1</span>)</span><br><span class="line">print(type(loss))</span><br><span class="line">print(<span class="string">"final loss:"</span>, loss.item())</span><br></pre></td></tr></table></figure>
<h2 id="分类问题"><a class="markdownIt-Anchor" href="#分类问题"></a> 分类问题</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> perf_counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(output)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> CUDA:</span><br><span class="line">        output=output.cpu()</span><br><span class="line">    plt.cla()</span><br><span class="line">    <span class="comment"># torch.max[0]是取max的值， [1]是取索引值</span></span><br><span class="line">    output = torch.max((output), <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">    pred_y = output.data.numpy().squeeze()</span><br><span class="line">    target_y = y.numpy()</span><br><span class="line">    plt.scatter(x.numpy()[:, <span class="number">0</span>], x.numpy()[:, <span class="number">1</span>], c=pred_y, s=<span class="number">10</span>, lw=<span class="number">0</span>, cmap=<span class="string">'RdYlGn'</span>)</span><br><span class="line">    accuracy = sum(pred_y == target_y)/<span class="number">1000.0</span></span><br><span class="line">    plt.text(<span class="number">1.5</span>, <span class="number">-4</span>, <span class="string">'Accuracy=%s'</span> % (accuracy), fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</span><br><span class="line">    plt.pause(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">N = <span class="number">500</span></span><br><span class="line">cluster = torch.ones(N, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># mean, std的正态分布</span></span><br><span class="line">data0 = torch.normal(<span class="number">4</span> * cluster, <span class="number">2</span>)</span><br><span class="line">data1 = torch.normal(<span class="number">-4</span> * cluster, <span class="number">2</span>)</span><br><span class="line">label0 = torch.zeros(N)</span><br><span class="line">label1 = torch.ones(N)</span><br><span class="line"></span><br><span class="line">x = torch.cat((data0, data1), ).type(torch.FloatTensor)</span><br><span class="line">y = torch.cat((label0, label1), ).type(torch.LongTensor)</span><br><span class="line">plt.scatter(x.numpy()[:, <span class="number">0</span>], x.numpy()[:, <span class="number">1</span>], c = y, s=<span class="number">10</span>, lw=<span class="number">0</span>, cmap=<span class="string">'RdYlGn'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        x = torch.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">CUDA = torch.cuda.is_available()</span><br><span class="line"><span class="keyword">if</span> CUDA:</span><br><span class="line">    net = Net().cuda()</span><br><span class="line">    inputs = x.cuda()</span><br><span class="line">    targets = y.cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    net = Net()</span><br><span class="line">    inputs = x</span><br><span class="line">    targets = y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, criterion, optimizer, epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> range(epochs):</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, targets)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ep % <span class="number">80</span> == <span class="number">0</span>:</span><br><span class="line">            draw(outputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># return model, loss</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">train(net, criterion, optimizer, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>torch.randn(5,2)是标准正态分布取（5,2）数</li>
<li>data0 = torch.normal(4 * cluster, 2) , 第一个参数是mean, 第二个是std, 维度根据传入的tensor参数决定</li>
<li>torch.norm() 是求范数</li>
</ul>
<h2 id="多分类"><a class="markdownIt-Anchor" href="#多分类"></a> 多分类</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(output)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> CUDA:</span><br><span class="line">        output=output.cpu()</span><br><span class="line">    plt.cla()</span><br><span class="line">    output = torch.max((output), <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">    pred_y = output.data.numpy().squeeze()</span><br><span class="line">    target_y = y.numpy()</span><br><span class="line">    plt.scatter(x.numpy()[:, <span class="number">0</span>], x.numpy()[:, <span class="number">1</span>], c=pred_y, s=<span class="number">10</span>, lw=<span class="number">0</span>, cmap=<span class="string">'RdYlGn'</span>)</span><br><span class="line">    accuracy = sum(pred_y == target_y)/<span class="number">1500.0</span></span><br><span class="line">    plt.text(<span class="number">0.5</span>, <span class="number">-6</span>, <span class="string">'Accuracy=%.6s'</span> % (accuracy), fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</span><br><span class="line">    plt.pause(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">N = <span class="number">500</span></span><br><span class="line">cluster = torch.ones(N, <span class="number">2</span>)  <span class="comment"># 2是两个维度： x, y</span></span><br><span class="line">data0 = torch.normal(<span class="number">4</span>*cluster, <span class="number">2</span>)</span><br><span class="line">data1 = torch.normal(<span class="number">-4</span>*cluster, <span class="number">1</span>)</span><br><span class="line">data2 = torch.normal(<span class="number">-8</span>*cluster, <span class="number">1</span>)</span><br><span class="line">label0 = torch.zeros(N)</span><br><span class="line">label1 = torch.ones(N)</span><br><span class="line">label2 = label1 * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = torch.cat((data0, data1, data2), ).type(torch.FloatTensor)</span><br><span class="line">y = torch.cat((label0, label1, label2), ).type(torch.LongTensor)</span><br><span class="line"></span><br><span class="line">plt.scatter(x.numpy()[:, <span class="number">0</span>], x.numpy()[:, <span class="number">1</span>], c=y.numpy(), s=<span class="number">10</span>, lw=<span class="number">0</span>, cmap=<span class="string">'RdYlGn'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_feature, num_hidden, outputs)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.hidden = nn.Linear(input_feature, num_hidden)</span><br><span class="line">        self.out = nn.Linear(num_hidden, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x )</span>:</span></span><br><span class="line">        x = F.relu( self.hidden(x) )</span><br><span class="line">        x = F.softmax(self.out(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">CUDA = torch.cuda.is_available()</span><br><span class="line"><span class="keyword">if</span> CUDA:</span><br><span class="line">    net = Net(input_feature=<span class="number">2</span>, num_hidden=<span class="number">20</span>,outputs=<span class="number">3</span>).cuda()</span><br><span class="line">    inputs = x.cuda()</span><br><span class="line">    target = y.cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    net = Net(input_feature=<span class="number">2</span>, num_hidden=<span class="number">20</span>,outputs=<span class="number">3</span>)</span><br><span class="line">    inputs = x</span><br><span class="line">    target = y</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, criterion, optimizer, epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> range(epochs):</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ep % <span class="number">80</span> == <span class="number">0</span>:</span><br><span class="line">            draw(outputs)</span><br><span class="line"></span><br><span class="line">train(net, criterion, optimizer, <span class="number">10000</span>)</span><br></pre></td></tr></table></figure>
<h2 id="lenet-mnist手写识别"><a class="markdownIt-Anchor" href="#lenet-mnist手写识别"></a> LeNet-MNIST手写识别</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="comment"># 加载torchvision， 是专门做图形处理的一个库， datasets为加载数据集， transforms做图形预处理</span></span><br><span class="line"><span class="comment"># torch.utils.data.DataLoader做数据加载</span></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),                          <span class="comment"># 将数据转成tensor对象</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))      <span class="comment"># 将数据做归一化处理</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载并存储路径， 加载训练集or测试集、是否自动下载、指定数据预处理方式</span></span><br><span class="line">trainset = datasets.MNIST(<span class="string">'data'</span>, train=<span class="keyword">True</span>, download=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">testset = datasets.MNIST(<span class="string">'data'</span>, train=<span class="keyword">False</span>, download=<span class="keyword">True</span>, transform=transform)</span><br><span class="line"><span class="comment"># 下载下来的raw文件夹包含了数据和标签，processsed文件夹中为处理后的数据</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 定义Net的初始化函数，本函数定义了神经网络的基本结构</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 继承父类的初始化方法，即先运行nn.Module的初始化函数</span></span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line">        <span class="comment"># C1卷积层：输入1张灰度图片，输出6张特征图，卷积核5x5</span></span><br><span class="line">        self.c1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, (<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        <span class="comment"># C3卷积层：输入6张特征图，输出16张特征图，卷积核5x5</span></span><br><span class="line">        self.c3 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 全连接层S4-&gt;C5：从S4到C5是全连接，S4层中16*4*4个节点全连接到C5层的120个节点上</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">120</span>)</span><br><span class="line">        <span class="comment"># 全连接层C5-&gt;F6：C5层的120个节点全连接到F6的84个节点上</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        <span class="comment"># 全连接层F6-&gt;OUTPUT：F6层的84个节点全连接到OUTPUT层的10个节点上，10个节点的输出代表着0到9的不同分值。</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义向前传播函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 输入的灰度图片x经过c1的卷积之后得到6张特征图，然后使用relu函数，增强网络的非线性拟合能力，接着使用2x2窗口的最大池化，然后更新到x</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.c1(x)), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 输入x经过c3的卷积之后由原来的6张特征图变成16张特征图，经过relu函数，并使用最大池化后将结果更新到x</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.c3(x)), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 使用view函数将张量x（S4）变形成一维向量形式，总特征数不变，**为全连接层做准备**</span></span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</span><br><span class="line">        <span class="comment"># 输入S4经过全连接层fc1，再经过relu，更新到x</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        <span class="comment"># 输入C5经过全连接层fc2，再经过relu，更新到x</span></span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        <span class="comment"># 输入F6经过全连接层fc3，更新到x</span></span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算张量x的总特征量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 由于默认批量输入，第零维度的batch剔除</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]</span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">CUDA = torch.cuda.is_available()</span><br><span class="line"><span class="keyword">if</span> CUDA:</span><br><span class="line">    lenet = LeNet().cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    lenet = LeNet()</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(lenet.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用数据加载工具来加载训练数据, batch_size表示一次性加载的数据量， shuffle表示遍历不同批次数据时打乱顺序， num_workers表示用n个子进程来加载数据</span></span><br><span class="line"><span class="comment"># 后来把workers改成2还是出错，直到最后把workers改成0才不出错。电脑不行啊！当报错时应该考虑一下是不是设置的太大或者不支持多线程进行操作。</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>, shuffle=<span class="keyword">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>, shuffle=<span class="keyword">False</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, criterion, optimizer, epochs=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">            inputs, labels = data</span><br><span class="line">            <span class="keyword">if</span> CUDA:</span><br><span class="line">                inputs, labels = inputs.cuda(), labels.cuda()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            outputs = model(inputs)</span><br><span class="line"></span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">999</span>:</span><br><span class="line">                print(<span class="string">'[Epoch:%d, Batch:%5d] Loss: %.3f'</span> % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">1000</span>))</span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line">    print(<span class="string">'Finished Training'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(testloader, model)</span>:</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        <span class="keyword">if</span> CUDA:</span><br><span class="line">            images = images.cuda()</span><br><span class="line">            labels = labels.cuda()</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum()</span><br><span class="line">    print(<span class="string">'Accuracy on the test set: %d %%'</span> % (<span class="number">100</span> * correct / total))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_param</span><span class="params">(model, path)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(path):</span><br><span class="line">        model.load_state_dict(torch.load(path))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_param</span><span class="params">(model, path)</span>:</span></span><br><span class="line">    torch.save(model.state_dict(), path)</span><br><span class="line"></span><br><span class="line">load_param(lenet, <span class="string">'model.pkl'</span>)</span><br><span class="line">train(lenet, criterion, optimizer, epochs=<span class="number">2</span>)</span><br><span class="line">save_param(lenet, <span class="string">'model.pkl'</span>)</span><br><span class="line">test(testloader, lenet)</span><br></pre></td></tr></table></figure>
<h2 id="附录"><a class="markdownIt-Anchor" href="#附录"></a> 附录</h2>
<h3 id="torchsizeshape-numpynarraysizeshape-区别"><a class="markdownIt-Anchor" href="#torchsizeshape-numpynarraysizeshape-区别"></a> Torch.size()/shape, numpy.narray.size/shape 区别</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a = np.array([[1,2,3],[2,1,3]])</span></span><br><span class="line"><span class="comment"># print(a.shape, a.size)</span></span><br><span class="line"><span class="comment"># (2, 3) 6</span></span><br><span class="line"></span><br><span class="line">a = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>]])</span><br><span class="line">print(a.shape, a.size())</span><br><span class="line"><span class="comment"># torch.Size([2, 3]) torch.Size([2, 3])</span></span><br></pre></td></tr></table></figure>
<h3 id="unsqueeze用法"><a class="markdownIt-Anchor" href="#unsqueeze用法"></a> unsqueeze用法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">print(a, a.size())</span><br><span class="line"></span><br><span class="line">b = torch.unsqueeze(a, dim = <span class="number">1</span>)</span><br><span class="line">print(b, b.size())</span><br></pre></td></tr></table></figure>
<h3 id="关于求微分-autograd-variable"><a class="markdownIt-Anchor" href="#关于求微分-autograd-variable"></a> 关于求微分-&gt;autograd-&gt;Variable</h3>
<p>之前书上提供的代码需要求grad的时都是把tensor的requires_grad属性手动置为True(requires_grad默认为False的)， 而实际上需要求grad的tensor已经被封装成了可以自动求微分的Variable类型(from torch.autograd import Variable)`</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CUDA = torch.cuda.is_available()</span><br><span class="line"><span class="keyword">if</span> CUDA:</span><br><span class="line">    inputs = X.cuda()</span><br><span class="line">    targets = Y.cuda()</span><br><span class="line">    w = w.cuda()</span><br><span class="line">    w.requires_grad = <span class="keyword">True</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    inputs = X</span><br><span class="line">    targets = Y</span><br><span class="line">    w = w</span><br><span class="line">    w.requires_grad = <span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<p>(variable)的理解， pytorch中的变量variable有三个属性，分别是data表示变量中的具体值， grad表示这个变量反向传播的梯度，这个的计算方式下面有专门的一个演示程序， grad_fn表示是通过什么操作得到这个变量的例如( 加减乘除、卷积、反置卷积)</p>
<p><img src="https://img-blog.csdnimg.cn/20190628093314464.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzc2Nzgx,size_16,color_FFFFFF,t_70" alt=""></p>
<p>然后定义一个tensor(张量)，以及将tensor(张量)转化成variable(变量)。之所以需要将tensor转化成variable是因为<strong>pytorch中tensor(张量)只能放在CPU上运算</strong>，而(variable)变量是可以用GPU进行加速计算的。 所以说这就是为什么pytorch加载图像的时候一般都会使用(variable)变量. 下面一段代码演示的是tensor和variable(变量)之间的转化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 然后定义pytorch中的tensor 并将tensor转化成Variable的形式</span></span><br><span class="line">x_tensor = torch.ones(<span class="number">3</span>)</span><br><span class="line">print(<span class="string">'张量的类型以及具体值:\n'</span>, type(x_tensor), x_tensor)</span><br><span class="line">x_var = Variable(x_tensor, requires_grad = <span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">'变量的类型以及具体的值:\n'</span>, type(x_var), x_var)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">张量的类型以及具体值:</span></span><br><span class="line"><span class="string"> &lt;class 'torch.Tensor'&gt; tensor([1., 1., 1.])</span></span><br><span class="line"><span class="string">变量的类型以及具体的值:</span></span><br><span class="line"><span class="string"> &lt;class 'torch.Tensor'&gt; tensor([1., 1., 1.], requires_grad=True)</span></span><br><span class="line"><span class="string"> '''</span></span><br><span class="line"><span class="comment"># 可以看到这边多了一个requires_grad的属性</span></span><br></pre></td></tr></table></figure>
<p>Tensor也可以通过x_tensor.data来获得值，因此比较下来两者的区别就是requires_grad为True、False的关系</p>
<p>摘自： <a href="https://blog.csdn.net/qq_41776781/article/details/93967961" target="_blank" rel="noopener">https://blog.csdn.net/qq_41776781/article/details/93967961</a></p>
<h1 id="深度学习算法实践"><a class="markdownIt-Anchor" href="#深度学习算法实践"></a> 《深度学习算法实践》</h1>
<blockquote>
<p>这本书应该是我大二见到过最多的书了， 但说实话看完也比较一般， 比较多介绍的是深度学习应用介绍。</p>
</blockquote>
<p><img src="https://wkphoto.cdn.bcebos.com/500fd9f9d72a60597d613fca2534349b023bbae9.jpg" alt=""></p>
</article><!-- lincense--><div class="license-wrapper"><p> <span>Author:  </span><a href="https://nymrli.top">Mrli</a></p><p> <span>Link:  </span><a href="https://nymrli.top/2020/11/28/深度学习-Pytorch读书笔记/">https://nymrli.top/2020/11/28/深度学习-Pytorch读书笔记/</a></p><p> <span>Copyright:  </span><span>All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/3.0">CC BY-NC-SA 3.0</a> unless stating additionally.</span></p></div><div class="post-paginator"><a class="prevSlogan" href="/2020/11/28/熟悉Latex写作/" title="熟悉Latex写作"><span>< PreviousPost</span><br><span class="prevTitle">熟悉Latex写作</span></a><a class="nextSlogan" href="/2020/11/22/Linux服务器同步时钟/" title="Linux服务器同步时钟"><span>NextPost ></span><br><span class="nextTitle">Linux服务器同步时钟</span></a><div class="clear"></div></div><div id="comment"><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  id: '深度学习+Pytorch学习笔记',
  owner: 'Freedomisgood',
  repo: 'Freedomisgood.github.io',
  oauth: {
    client_id: 'bc5a81fe36017dcd8b63',
    client_secret: '949cec3a1b91742c6249c47259791e4b80a6fa69',
  },
})
gitment.render('container')</script></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><p class="beian"><span>备案号:苏ICP备18015439号</span></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 60vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#深度学习框架pytorch快速开发与实战"><span class="toc-number">1.</span> <span class="toc-text"> 《深度学习框架PyTorch快速开发与实战》</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#关于作者"><span class="toc-number">1.1.</span> <span class="toc-text"> 关于作者</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#常用距离公式"><span class="toc-number">1.1.1.</span> <span class="toc-text"> 常用距离公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据标准化处理"><span class="toc-number">1.1.2.</span> <span class="toc-text"> 数据标准化处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#正则化"><span class="toc-number">1.1.3.</span> <span class="toc-text"> 正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#成本函数优化函数"><span class="toc-number">1.1.4.</span> <span class="toc-text"> 成本函数（优化函数）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch深度学习入门"><span class="toc-number">2.</span> <span class="toc-text"> 《PyTorch深度学习入门》</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#线性回归代码"><span class="toc-number">2.1.</span> <span class="toc-text"> 线性回归代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#人工神经元-线性回归"><span class="toc-number">2.2.</span> <span class="toc-text"> 人工神经元-线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#人工神经元-线性回归-module"><span class="toc-number">2.2.1.</span> <span class="toc-text"> 人工神经元-线性回归-module</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#非线性回归"><span class="toc-number">2.3.</span> <span class="toc-text"> 非线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分类问题"><span class="toc-number">2.4.</span> <span class="toc-text"> 分类问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#多分类"><span class="toc-number">2.5.</span> <span class="toc-text"> 多分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lenet-mnist手写识别"><span class="toc-number">2.6.</span> <span class="toc-text"> LeNet-MNIST手写识别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#附录"><span class="toc-number">2.7.</span> <span class="toc-text"> 附录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#torchsizeshape-numpynarraysizeshape-区别"><span class="toc-number">2.7.1.</span> <span class="toc-text"> Torch.size()/shape, numpy.narray.size/shape 区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#unsqueeze用法"><span class="toc-number">2.7.2.</span> <span class="toc-text"> unsqueeze用法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#关于求微分-autograd-variable"><span class="toc-number">2.7.3.</span> <span class="toc-text"> 关于求微分-&gt;autograd-&gt;Variable</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#深度学习算法实践"><span class="toc-number">3.</span> <span class="toc-text"> 《深度学习算法实践》</span></a></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>