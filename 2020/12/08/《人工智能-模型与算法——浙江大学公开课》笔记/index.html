<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Mrli"><meta name="renderer" content="webkit"><meta name="copyright" content="Mrli"><meta name="keywords" content="Mrli's Blog"><meta name="description" content="想和你讲，说了会心动 ，缄默会心安。"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>《人工智能:模型与算法——浙江大学公开课》笔记 · Mr.li's Blog</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/favicon.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="https://avatars1.githubusercontent.com/u/31088082?s=400&amp;u=7a99ff83916afb3f4c5312bd78a1be17fe0e34ed&amp;v=4"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">Mrli</div><div class="profile-signature">别装作很努力,<br>因为结局不会陪你演戏。</div><div class="contacts"><div>Contacts:</div><span><a href="http://sighttp.qq.com/msgrd?v=1&amp;uin=1063052964" target="_black">QQ</a></span><span><a href="https://www.cnblogs.com/nymrli/" target="_black">博客园</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 60vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">Mr.li's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">《人工智能:模型与算法——浙江大学公开课》笔记</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-edit"></i><span>2020/12/22</span></span><span class="busuanzi-pv" id="busuanzi_container_page_pv"><i class="post-intro-calendar fa fa-user-o"></i><span id="busuanzi_value_page_pv"></span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="ZJU"> ZJU</a><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="ML"> ML</a></span></div><div class="post-intro-read"><span> Word count: <span class="post-count">10,011</span> | Reading time: <span class="post-count">37</span>min</span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><html><head></head><body><h2 id="p111可计算思想起源与发展"><a class="markdownIt-Anchor" href="#p111可计算思想起源与发展"></a> <a href="https://www.bilibili.com/video/BV1c7411n7EY?p=1" target="_blank" rel="noopener">P11.1可计算思想起源与发展</a></h2>
<p>智能： 从感知、到理解、到认知、到决策与行动</p>
<p>计算的诞生：从可计算到不可计算-&gt;20世纪初，人们发现有许多问题无法找到解决的方法。于是开始怀疑，是否对这些问题来说，根本就不存在算法，即不可计算。</p>
<p>人工智能：以机器为载体的人类智能或生物智能</p>
<p>算术公理的相容性：</p>
<ul>
<li>完备性：所有能够从该形式化系统推导出来的命题，都可以从这个形式化系统推导出来。</li>
<li>一致性：一个命题不可能同时为真或为假</li>
<li>可判定性：算法在有限步内判定命题的真伪</li>
</ul>
<p>哥德尔不完全性定理：任何表达力足够强的（递归可枚举）形式系统都不可能同时具有一致性和完备性</p>
<p>图灵测试：指测试者与被测试者（一个人和一台机器）隔开的情况下，通过一些装置（如键盘）向被测试者随意提问。进行多次测试后，如果机器让平均每个参与者做出超过30%的误判，那么这台机器就通过了测试，并被认为具有人类智能。</p>
<p>摩尔定律：（计算机速度1年半增长1倍），亿级晶体管、千亿指令/秒</p>
<h2 id="p212人工智能的发展简史"><a class="markdownIt-Anchor" href="#p212人工智能的发展简史"></a> <a href="https://www.bilibili.com/video/BV1c7411n7EY?p=2" target="_blank" rel="noopener">P21.2人工智能的发展简史</a></h2>
<h2 id="p313人工智能研究的基本内容"><a class="markdownIt-Anchor" href="#p313人工智能研究的基本内容"></a> <a href="https://www.bilibili.com/video/BV1c7411n7EY?p=3" target="_blank" rel="noopener">P31.3人工智能研究的基本内容</a></h2>
<blockquote>
<p>人工智能特点：至小有内、至大无外，多学科交叉内禀</p>
</blockquote>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%AF%BE%E7%A8%8B.jpg" alt="人工智能课程"></p>
<p>从模拟人类智能角度而言，人工智能应具备如下能力：</p>
<ul>
<li><strong>具备视觉感知和语言交流的能力</strong>。即能够识别和理解外界信息（计算机视觉研究范畴）、能够与人通过语言交流（自然语言理解研究范畴）。</li>
<li>具备推理与问题求解能力。即基于已有知识，对所见事物和现象进行演绎推理以解决问题。</li>
<li><strong>具备协同控制能力</strong>。即将视觉（看）、语言（说）、推理（悟）等能力统一协调，加以控制，这是常见的机器人研究领域内容。</li>
<li><strong>具备遵守伦理道德能力</strong>。即模拟人类智能的智能体在社会环境中要遵从一定的伦理道德。阿西莫夫在科幻小说中按照优先级定义了机器人需要遵从的三条伦理原则：不得伤人，或弃人于危难；需服从人；在不违反上述两条原则情况下，保护机器人自己。</li>
<li><strong>具备从数据中进行归纳总结的能力</strong>。即需要从数据中进行知识、规律和模式学习的模型和方法，这是机器学习研究范畴。</li>
</ul>
<h3 id="授课基本内容"><a class="markdownIt-Anchor" href="#授课基本内容"></a> 授课基本内容：</h3>
<p>人工智能概述</p>
<ul>
<li>1.1可计算思想起源与发展</li>
<li>1-2人工智能的发展简史</li>
<li>1.3人工智能研究的基本内容</li>
</ul>
<p>搜索求解</p>
<ul>
<li>2.1启发式搜索</li>
<li>2.2对抗搜索（Minimax及Alpha-Beta剪枝搜索）</li>
<li>2.3蒙特卡洛树搜索</li>
</ul>
<p>逻辑与推理</p>
<ul>
<li>3.1命题逻辑</li>
<li>3.2谓词逻辑</li>
<li>3.3兴国格推理</li>
<li>3.4因果推理</li>
</ul>
<p>统计机器学习|监督学习</p>
<ul>
<li>4.1机器学习基本概念</li>
<li>4.2线性回归与分类</li>
<li>4.3Ada Boosting</li>
<li>4.4线性区别分析</li>
</ul>
<p>统计机器学习|非监督学习</p>
<ul>
<li>5.1K-means</li>
<li>5.2主成分分析</li>
<li>5.3特征人脸方法</li>
<li>5.4期望极大算法（EM）</li>
</ul>
<p>深度学习(监督学习+端到端)</p>
<ul>
<li>6.1前馈神经网络（误差后向传播）</li>
<li>6.2卷积神经网络</li>
<li>6.3自然语言理解与视觉分析</li>
</ul>
<p>强化学习</p>
<ul>
<li>7.1马尔科夫决策过程</li>
<li>7.2强化学习中策略优化与策略评估</li>
<li>7.3Q-Learning</li>
<li>7.4深度强化学习</li>
</ul>
<p>人工智能博弈</p>
<ul>
<li>8.1博弈相关概念（纳什均衡）</li>
<li>8.2遗憾最小化算法</li>
<li>8.3虚拟遗憾最小化算法</li>
</ul>
<h2 id="搜索求解"><a class="markdownIt-Anchor" href="#搜索求解"></a> 搜索求解</h2>
<h3 id="p421启发式搜索"><a class="markdownIt-Anchor" href="#p421启发式搜索"></a> <a href="https://www.bilibili.com/video/BV1c7411n7EY?p=4" target="_blank" rel="noopener">P42.1启发式搜索</a></h3>
<blockquote>
<p>给定搜索目标，设计启发函数，来保证搜索目标最优化的求解</p>
</blockquote>
<h3 id="p522对抗搜索"><a class="markdownIt-Anchor" href="#p522对抗搜索"></a> <a href="https://www.bilibili.com/video/BV1c7411n7EY?p=5" target="_blank" rel="noopener">P52.2对抗搜索</a></h3>
<blockquote>
<p>在游戏里搜索一种解决方案，但在搜索过程中对手会阻止我们，这种情况下我们能获得最大收益的搜索方式。本文中主要讲解Minmax搜索+alpha-beta剪枝搜索</p>
<p>对抗搜索（Adversarial Search）也称为博弈搜索（Game Search），在一个竞争的环境中，智能体（agents）之间通过竞争实现相反的利益，一方最大化这个利益，另外一方最小化这个利益。</p>
</blockquote>
<blockquote>
<p>本课程目前主要讨论在确定的、全局可观察的、竞争对手轮流行动、零和游戏（zero-sum）下的对抗搜索</p>
<ul>
<li><strong>零和博弈</strong>是博弈论的一个概念，属非合作博弈。指参与博弈的各方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”，双方不存在合作的可能。</li>
<li>与“零和”对应，“<strong>双赢博弈</strong>”的基本理论就是“利己”不“损人”，通过谈判、合作达到皆大欢喜的结果。</li>
</ul>
</blockquote>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/%E5%AF%B9%E6%8A%97%E6%90%9C%E7%B4%A2%E5%86%85%E5%AE%B9.jpg" alt="对抗搜索内容"></p>
<h4 id="最大最小搜索"><a class="markdownIt-Anchor" href="#最大最小搜索"></a> 最大最小搜索</h4>
<p>给定一个游戏搜索树，minimax算法通过每个节点的minimax值来决定最优策略。当然，MAX节点希望最大化minimax值，而MIN节点则相反，希望最小化minimax值—&gt;让自己的收益最大，让对方的收益or己方的损失最小</p>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/minmax.jpg" alt="minmax"></p>
<p>优点：</p>
<ul>
<li>算法是一种简单有效的对抗搜索手段</li>
<li>在对手也“尽力而为”前提下，算法可返回最优结果</li>
</ul>
<p>缺点：</p>
<ul>
<li>如果搜索树极大，则无法在有效时间内返回结果</li>
</ul>
<p>改善：</p>
<ul>
<li>使用alpha-beta pruning算法来减少搜索节点</li>
<li>对节点进行采样、而非逐一搜索（ie.，MCTS）</li>
</ul>
<h4 id="alpha-beta剪枝搜索"><a class="markdownIt-Anchor" href="#alpha-beta剪枝搜索"></a> alpha-beta剪枝搜索</h4>
<blockquote>
<p>一种对最小最大搜索进行改进的算法，即在搜索过程中可剪除无需搜索的分支节点，且不影响搜索结果。.</p>
</blockquote>
<p><img src="/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/abpruncing.jpg" alt="abpruncing"></p>
<h4 id="p623蒙特卡洛树搜索"><a class="markdownIt-Anchor" href="#p623蒙特卡洛树搜索"></a> <a href="https://www.bilibili.com/video/BV1c7411n7EY?p=6" target="_blank" rel="noopener">P62.3蒙特卡洛树搜索</a></h4>
<blockquote>
<p>alphaGo三大法宝：深度学习、强化学习、MCTS</p>
</blockquote>
<blockquote>
<p>通过采样而非穷举方法来实现搜索，从而跟上述两种搜索有本质上的区别。</p>
</blockquote>
<blockquote>
<p>多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。</p>
<ul>
<li>利用（exploitation）：保证在过去决策中得到最佳回报</li>
<li>探索（exploration）：寄希望在未来能够得到更大回报</li>
</ul>
</blockquote>
<h5 id="exploitation-component利用"><a class="markdownIt-Anchor" href="#exploitation-component利用"></a> exploitation component(利用)</h5>
<p>第一部分是<span class="mjpage mjpage__block">undefined</span>​ ，也称作exploitation component。 Q(Vi)为子节点获胜次数，N(Vi)为子节点参与模拟的次数</p>
<p>可以看做是子节点Vi的胜率估计（总收益/总次数=平均每次的收益）。但是不能只选择胜率高的下一步，因为这种贪婪方式的搜索会很快导致游戏结束，这往往会导致搜索不充分，错过最优解。</p>
<p>举个简单的例子。现在假设MCTS的UCT函数只用了探索成分，从根节点开始，我们对所有子节点进行了一次模拟，然后在下一步中只访问至少赢了一次的子节点。那么在第一次模拟中那些不幸未被选中的节点（实际中rollout策略函数通常是随机的）将会被立刻抛弃</p>
<h5 id="exploration-component探索"><a class="markdownIt-Anchor" href="#exploration-component探索"></a> exploration component(探索)</h5>
<p class="katex-block katex-error" title="ParseError: KaTeX parse error: Can't use function '$' in math mode at position 39: …))}{N(v_{i})} }$̲$，这个成分更倾向于那些想对较…">c* \sqrt{\frac{\log(N(v))}{N(v_{i})} }$$，这个成分更倾向于那些想对较少被探索的节点N(Vi)小。

参数c是exploitation和exploration之间的折中系数。

##### MCTS的终止

终止条件(or)：

- 达到一定的迭代次数
- 达到规定的搜索时间

当MSCT程序结束时，最佳的移动通常是访问次数最多的那个节点，也是UCT最大的点。



将上限置信区间算法UCB应用于游戏树的搜索方法，由Kocsis和Szepesvari在2006年提出包括了四个步骤：**选举（selection）**，**扩展（expansion）**，**模拟（simulation）**，**反向传播（Back-Propagation）**

**选择**

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记\MCTS1.jpg" alt="MCTS1" style="zoom:67%;" /&gt;



**拓展**

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记\MCTS2.jpg" alt="MCTS2" style="zoom:67%;" /&gt;

**模拟、反向传播**

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记\MCTS3.jpg" alt="MCTS3" style="zoom:67%;" /&gt;



##### MCTS学习策略：

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记\MCTS学习策略.jpg" alt="MCTS学习策略" style="zoom:67%;" /&gt;



##### MCTS算法执行



&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记\processure.png" alt="processure" style="zoom:67%;" /&gt;



&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记\MCTS算法执行.jpg" alt="MCTS算法执行" style="zoom:67%;" /&gt;

# [P125.1机器学习基本概念](https://www.bilibili.com/video/BV1c7411n7EY?p=12)

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/监督学习.jpg" alt="监督学习" style="zoom:67%;" /&gt;

**机器学习的目的：**

1.原始数据中提取特征
2.学习映射函数f
3.通过映射函数f将&lt;u&gt;原始数据映射到语义空间&lt;/u&gt;，即寻找&lt;u&gt;数据和任务目标&lt;/u&gt;之间的关系

## 监督学习

### 监督学习的两种方法：

- 判别模型

  - 判别方法直接学习判别函数f（X）或者条件概率分布P（YIX）作为预测的模型，即判别模型。
  - 判别模型关心在给定输入数据下，预测该数据的输出是什么。
  - 典型判别模型包括回归模型、神经网络、支持向量机和Ada boosting等。

- 生成模型

  - 生成模型从数据中学习联合概率分布P（X，Y）（通过似然概率P(X|Y)-&gt;从输入数据产生输出、类概率P(Y)的乘积来求取）

    $P(Y|X）= \frac{P(X,Y)}{P(x)}$或者$P(Y|X）= \frac{P(X|Y)*P(Y)}{P(x)}$

  - 典型方法为贝叶斯方法、隐马尔可夫链授之于鱼、不如授之于“渔”

  - 联合分布概率P（X，Y）或似然概率P（YIX）求取很困难

## [P135.2线性回归分析](https://www.bilibili.com/video/BV1c7411n7EY?p=13)

线性回归定义：分析不同变量之间存在关系的研究叫回归分析，刻画不同变量之间关系的模型被称为回归模型。如果这个模型是线性的，则称为线性回归模型。

例如y = k*x + b，就是一个回归模型，其中的参数k和b需要从标注的数据中学习得到（监督学习）

**线性回归模型例子**
背景：给出了莫纳罗亚山（夏威夷岛的活火山）从1970年到2005年每5年的二氧化碳浓度，单位是百万分比浓度（Parts Per Million，ppm）。

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/线性回归.jpg" alt="线性回归" style="zoom:67%;" /&gt;

问题Q：1）给出1984年二氧化碳浓度值；2）预测2010年二氧化碳浓度值

解答A

1. 目标：建立回归模型y = a*x + b， 通过最佳回归模型求解参数a和b， 最佳回归模型是最小化残差平方和的均值，即要求8组（x，y）数据得到的残差平均值$\frac{1}{N} \sum(y-\tilde{y})^{2}$最小。残差平均值最小只与参数a和b有关，最优解即是使得残差最小所对应的a和b的值。
2. 具体步骤：
   - 记在当前参数下第i个训练样本xi的预测值为$\hat{y_i}$；
   - xi的标注值（实际值）yi，与预测值$\hat{y_i}$，之差记为$\left(y_{i}-\hat{y}_{i}\right)^{2}$
   - 训练集中n个样本所产生误差总和为$L(a, b)=\sum_{i=1}^{n}\left(y_{i}-a \times x_{i}-b\right)^{2}$--》**误差函数**
   - 目标：寻找一组a和b，使得误差总和L（a，b）值最小。在线性回归中，解决如此目标的方法叫**最小二乘法**。
     一般而言，要使函数具有最小值，可&lt;u&gt;对L(a, b)参数a和b分别求导，令其导数值为零--&gt;偏导&lt;/u&gt;，再求取参数a和b的取值。

▲线性回归，可以从已标注数据出发，找寻两组变量之间的线性关系，并且可拓展为多维变量





## [P145.3提升算法（boosting）](https://www.bilibili.com/video/BV1c7411n7EY?p=14)

对于一个复杂的分类任务，可以将其分解为若干子任务，然后将若干子任务完成方法**综合**，最终完成该复杂任务。即将弱分类器（weak classifiers）**组合**起来，形成强分类器（strong classifier）

### 为什么这样是能work的呢？

&gt; 计算学习理论（Computational Learning Theory）
&gt; - 可计算：什么任务是可以计算的？Ans: 图灵可停机
&gt; - 可学习：什么任务是可以被学习的、从而被学习模型来完成？
&gt;
&gt; 学习任务：统计某个电视节目在全国的收视率。
&gt; 方法：不可能去统计整个国家中每个人是否观看电视节目、进而算出收视率。只能**抽样**一部分人口，然后将抽样人口中观看该电视节目的比例作为该电视节目的全国收视率。
&gt; 霍夫丁不等式：全国人口中看该电视节目的人口比例（记作x）与抽样人口中观看该电视节目的人口比例（记作y）满足如下关系：
&gt;
&gt; &lt;mark&gt;当N足够大时，“全国人口中电视节目收视率”与“样本人口中电视节目收视率”差值超过误差范围e的概率非常小。&lt;/mark&gt;
&gt;
&gt; 对于统计电视节目收视率这样的任务，可以通过&lt;u&gt;不同的采样方法（即不同模型）&lt;/u&gt;来计算收视率。每个模型会产生不同的误差。

问题：如果得到完成该任务的若干“弱模型”，是否可以将这些弱模型组合起来，形成一个“强模型”。该“强模型”产生误差很小呢？
这就是**概率近似正确（PAC）**要回答的问题。

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/PAC.jpg" alt="PAC" style="zoom:67%;" /&gt;

### adaboosting

&gt; 将一系列弱分类器组合成强分类器

Ada Boosting算法中两个核心问题：

- 在每个弱分类器学习过程中，如何改变训练数据的权重：提高在上一轮中分类错误样本的权重。
- 如何将一系列弱分类器组合成强分类器：通过加权多数表决方法来提高分类误差小的弱分类器的权重，让其在最终分类中起到更大作用。同时减少分类误差大的弱分类器的权重，让其在最终分类中仅起到较小作用。



算法步骤：

1. 数据样本权重初始化——初始化每个训练样本的权重
  
- $D_{1}=\left(w_{11}, \ldots, w_{1 i}, \ldots, w_{1 N}\right),$ 其中 $w_{1 i}=\frac{1}{N}(1 \leq i \leq N)$，初始情况下每个分类器的权重是一样的
  
2. -第m个弱分类器训练

   $\quad$ 对 $m=1,2, \ldots, M$
   a) 使用具有分布权重 $D_{m}$ 的训练数据来学习得到第m个基分类器（弱分类器） $G_{m}$ :

</p>
<p>G_{m}(x): X \rightarrow{-1,1}</p>
<p class="katex-block katex-error" title="ParseError: KaTeX parse error: Can't use function '$' in math mode at position 6: ​	b) $̲\quad$ 计算 $G_{m…">​	b) <span class="mjpage">undefined</span> 计算 <span class="mjpage">undefined</span> 在训练数据集上的分类误差<span class="mjpage">undefined</span> 这里: <span class="mjpage">undefined</span> 如果 <span class="mjpage">undefined</span> 否则为 0
​	c) <span class="mjpage">undefined</span> &lt;u&gt;计算弱分类器 <span class="mjpage">undefined</span> 的权重&lt;/u&gt; <span class="mjpage">undefined</span>，如果<span class="mjpage">undefined</span>意味着每个样本都分类错，则<span class="mjpage">undefined</span>，当<span class="mjpage">undefined</span>=1/2， 则性能相当于随机分类；<span class="mjpage">undefined</span>权重随分类误差errm减小而增大，也就是说分类越少，分类器的权重越大。
​	d) <span class="mjpage">undefined</span> 更新训练样本数据的分布权重: <span class="mjpage">undefined</span> 其中 <span class="mjpage">undefined</span> 是归一化因子以使得 <span class="mjpage">undefined</span> 为概率分布, <span class="mjpage">undefined</span>

- 对数据不断划重点：<span class="mjpage">undefined</span>

  可见，如果某个样本无法被第m个弱分类器Gm（x）分类成功，则需要增大该样本权重，否则减少该样本权重。这样，被错误分类样本会在训练第m+1个弱分类器Gm+1（x）时会被“重点关注”。
  在每一轮学习过程中，Ada Boosting算法均在划重点（重视当前尚未被正确分类的样本）

3. 弱分类器组合成强分类器

   以线性加权形式来组合弱分类器 <span class="mjpage">undefined</span>
</p>
<p>f(x)=\sum_{i=1}^{M} \alpha_{m} G_{m}(x)</p>
<p class="katex-block katex-error" title="ParseError: KaTeX parse error: Can't use function '$' in math mode at position 8: 得到强分类器 $̲G(x)$
">得到强分类器 <span class="mjpage">undefined</span>
</p>
<p>G(x)=\operatorname{sign}(f(x))=\operatorname{sign}\left(\sum_{i=1}^{M} \alpha_{m} G_{m}(x)\right)</p>
<p class="katex-block katex-error" title="ParseError: KaTeX parse error: Can't use function '$' in math mode at position 48: …的弱分类器具有更大权重。
- $̲\alpha_{m}$ 累加之…">
- f（x）是M个弱分类器的**加权线性**累加。分类能力越强的弱分类器具有更大权重。
- <span class="mjpage">undefined</span> 累加之和并不等于1。
- <span class="mjpage">undefined</span> 符号决定样本 <span class="mjpage">undefined</span> 分类为 1 或- 1 。如果 <span class="mjpage">undefined</span> 为正，则强分类器 <span class="mjpage">undefined</span> 将样本 <span class="mjpage">undefined</span> 分类为1 ; 否则为-1。

**回看霍夫丁不等式**

&gt; 假设有M个弱分类器Gm（1sm≤M），则M个弱分类器线性组合所产生误差满足如下条件：

<span class="mjpage">undefined</span>

- <span class="mjpage">undefined</span>是真实分类函数、∈（0，1）。上式表明，如果所“组合”弱分类器越多，则学习分类误差呈指数级下降，直至为零。
- 上述不等式成立有两个前提条件：1）每个弱分类器产生的误差相互独立；2）每个弱分类器的误差率小于50%。因为每个弱分类器均是在同一个训练集上产生，条件1）难以满足。也就说，“准确性（对分类结果而言）”和“差异性（对每个弱分类器而言）”难以同时满足。----&gt;Ada Boosting采取了序列化学习机制。

#### 优化目标

Ada Boost实际在最小化如下指数损失函数（minimization of exponential loss）：

<span class="mjpage">undefined</span>

Ada Boost的分类误差上界如下所示：

<span class="mjpage">undefined</span>

在第m次迭代中，Ada Boosting总是趋向于将具有最小误差的学习模型选做本轮生成的弱分类器Gm，使得累积误差快速下降。



## 无监督学习

&gt; 无监督学习中，由于数据本身没有语义标签，因此我们对聚类结果无法知道到底代表的是怎样的高层语义

![无监督学习](./《人工智能-模型与算法——浙江大学公开课》笔记/无监督学习.jpg)



&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/无监督相似度.jpg" alt="无监督相似度" style="zoom:67%;" /&gt;

数据特征和相似度函数都很重要



### [P156.1K均值聚类](https://www.bilibili.com/video/BV1c7411n7EY?p=15)-kmeans

输入：n个数据（无任何标注信息）
输出：k个聚类结果
目的：将n个数据聚类到k个集合（也称为类簇）

**算法描述**

若干定义:
n个m 维数据 <span class="mjpage">undefined</span>

- 两个 <span class="mjpage">undefined</span> 维数据之间的欧氏距离为 

</p>
<p>d\left(x_{i}, x_{j}\right)=\sqrt{\left(x_{i 1}-x_{j 1}\right)^{2}+\left(x_{i 2}-x_{j 2}\right)^{2}+\cdots+\left(x_{i m}-x_{j m}\right)^{2}}</p>
<p class="katex-block katex-error" title="ParseError: KaTeX parse error: Can't use function '$' in math mode at position 1: $̲d\left(x_{i}, x…"><span class="mjpage">undefined</span> 值越小，表示 <span class="mjpage">undefined</span> 和 <span class="mjpage">undefined</span> 越相似; 反之越不相似

- 聚类集合数目 <span class="mjpage">undefined</span>

问题：如何将n个数据依据其相似度大小将它们分别聚类到 <span class="mjpage">undefined</span> 个集合，使得每个数据仅属于一个聚类集合。

1. 初始化——初始化聚类质心
   - 初始化 <span class="mjpage">undefined</span> 个聚类质心 <span class="mjpage">undefined</span>
   - 每个聚类质心 <span class="mjpage">undefined</span> 所在集合记为 <span class="mjpage">undefined</span>

2. 对数据进行聚类——将每个待聚类数据放入唯一一个聚类集合中
   - 计算待聚类数据 <span class="mjpage">undefined</span> 和质心 <span class="mjpage">undefined</span> 之间的欧氏距离 <span class="mjpage">undefined</span>
   - 将每个 <span class="mjpage">undefined</span> &lt;u&gt;放入与之距离最近聚类质心所在聚类集合&lt;/u&gt;中,即 <span class="mjpage">undefined</span>

3. 更新聚类质心——根据聚类结果、更新聚类质心
  
- 根据每个聚类集合中所包含的数据，更新该聚类集合质心值, 即 <span class="mjpage">undefined</span>
  
4. 继续迭代——算法循环迭代，直到满足条件

   聚类迭代满足如下任意一个条件，则聚类停止：

   - 已经达到了迭代次数上限
   - 前后两次迭代中，聚类质心基本保持不变

**K均值聚类算法的另一个视角：最小化每个类簇的方差**

- 欧氏距离与方差量纲相同
- **最小化每个类簇方差**将使得最终**聚类结果中每个聚类集合中所包含数据呈现出来差异性最小**。

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/kmeans.jpg" alt="kmeans" style="zoom:67%;" /&gt;

#### K均值聚类算法的不足

- 需要事先确定聚类数目，很多时候我们并不知道数据应被聚类的数目
- 需要初始化聚类质心，初始化聚类中心对聚类结果有较大的影响
- 算法是迭代执行，时间开销非常大
- 欧氏距离假设数据每个维度之间的重要性是一样的

### [P166.2主成分分析](https://www.bilibili.com/video/BV1c7411n7EY?p=16)——Principle Component Analysis (PCA)

&gt; 主成份分析是一种特征降维方法。人类在认知过程中会主动“化繁为简”
&gt; 奥卡姆剃刀定律（Occam's Razor）：“如无必要，勿增实体”，即“简单有效原理”

主成份分析：**降维后的结果要保持原始数据固有结构**
啥是原始数据中的结构？: 1.图像数据中结构：视觉对象区域构成的空间分布  ; 2.文本数据中结构：单词之间的（共现）相似或不相似

若干相关概念：方差和协方差、皮尔逊相关系数

**方差**

- 方差等于各个数据与样本均值之差的平方和之平均数
- 方差描述了样本数据的波动程度

**协方差**

- 衡量两个变量之间的相关度

**皮尔逊相关系数**

我们可通过皮尔逊相关系数（Pearson Correlation coefficient）将两组变量之间的关联度规整到一定的取值范围内。

注：相关系数表达的是线性相关程度

**皮尔逊相关系数所具有的性质**如下：

- | corr（X，Y）| ≤1
- corr（X，Y）=1的充要条件是存在常数a和b，使得Y=ax+b
- 皮尔逊相关系数是对称的，即corr（X，Y）=corr（Y，X）
- 由此衍生出如下性质：皮尔逊相关系数刻画了变量x和Y之间线性相关程度，如果| corr（x，Y）|的取值越大，则两者在线性相关的意义下相关程度越大。Icorr（x，Y）l=0表示两者不存在线性相关关系（可能存在其他非线性相关的关系）。
- 正线性相关意味着变量x增加的情况下，变量Y也随之增加；负线性相关意味着变量X减少的情况下，变量Y也随之增加。

**相关性（correlation）与独立性（independence）**

- 如果X和Y的线性不相关，则|corr（xX，Y）l=0
- 如果X和Y的彼此独立，则一定lcorr（x，Y）l=0，且x和Y不存在任何线性或非线性关系
- “不相关”是一个比“独立”要弱的概念，即&lt;u&gt;独立一定不相关，但是不相关不一定相互独立&lt;/u&gt;（可能存在其他复杂的关联关系）。独立指两个变量彼此之间不相互影响。





#### 算法动机

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/PCA1.jpg" alt="PCA1" style="zoom:67%;" /&gt;

- 主成份分析思想是将n维特征数据映射到l维空间（n &gt;&gt; l），去除原始数据之间的冗余性（通过去除相关性手段达到这一目的）。
- 将原始数据向这些**数据方差最大的方向**进行投影。一旦发现了方差最大的投影方向，则继续寻找保持方差第二的方向且进行投影。
- 将每个数据从n维高维空间映射到l维低维空间，每个数据所得到最好的k维特征就是使得每一维上样本方差都尽可能大。

#### 算法描述

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/PCA2.jpg" alt="PCA2" style="zoom:67%;" /&gt;

#### 算法步骤

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/PCA3.jpg" alt="PCA3" style="zoom:67%;" /&gt;

具体如何运行的可以看下节的特征人脸算法，讲的还是比较清楚的。

### [P176.3特征人脸算法](https://www.bilibili.com/video/BV1c7411n7EY?p=17)

&gt; 特征人脸方法是一种&lt;u&gt;应用主成份分析PCA&lt;/u&gt;来实现&lt;u&gt;人脸图像降维&lt;/u&gt;的方法，其本质是用一种称为“特征人脸（eigenface）”的特征向量按照线性组合形式来表达每一张原始人脸图像，进而实现人脸识别。==》将原有的像素点降维后，提取出新的PCA变量，即人脸特征===&gt;用（特征）人脸表示人脸，而非用像、素点表示人脸
&gt;
&gt; 由此可见，这一方法的关键之处在于如何得到特征人脸。

**PCA降维计算：**

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/特征人脸-PCA运算.jpg" alt="特征人脸-PCA运算" style="zoom:67%;" /&gt;



#### 人脸对比方法：聚类、主成分分析、非负矩阵分解

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/人脸对比.jpg" alt="人脸对比" style="zoom:67%;" /&gt;



#### 特征人脸识别流程

![人脸识别](./《人工智能-模型与算法——浙江大学公开课》笔记/人脸识别.jpg)





## 统计机器学习算法应用

### [P187.1逻辑斯蒂回归与分类](https://www.bilibili.com/video/BV1c7411n7EY?p=18)

#### 分类和回归的区别：

- 在回归分析中，学习得到一个函数将输入变量映射到连续输出空间，如价格和温度等，即值域是连续空间。
- 在分类模型中，学习得到一个函数将输入变量映射到离散输出空间，如人脸和汽车等，即值域是离散空间。

问题：回归与分类可否统一，即用回归模型来完成分类任务？--&gt;demo：逻辑斯蒂回归（logistic regression）

回归分析：线性-&gt;非线性，线性回归模型难以刻画数据的复杂分布，需要寻找非线性回归模型——demo：逻辑斯蒂回归（logistic regression）

#### sigmod函数性质

概率形式输出。sigmoid函数是单调递增的，其值域为**（0，1）**，因此使sigmoid函数输出可作为**概率值**。在前面介绍的线性回归中，回归函数的值域一般为（一oo，+oo）

数据特征加权累加。对输入z取值范围没有限制，但当z大于一定数值后，函数输出无限趋近于1，而小于一定数值后，函数输出无限趋近于0。特别地，当z=0时，函数输出为0.5。

这里z是输入数据x和回归函数的参数w相乘结果（可视为x各维度进行加权叠加）非线性变化。x各维度加权叠加之和结果取值在0附近时，函数输出值的变化幅度比较大（函数值变化陡峭），且是非线性变化。但是，各维度加权叠加之和结果取值很大或很小时，函数输出值几乎不变化，这是基于概率的一种认识与需要。

缺点：梯度消失

#### 概率输出：从回归到分类

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/logistics.jpg" alt="logistics" style="zoom:67%;" /&gt;

对p(y=1|x)取对数的结果为<span class="mjpage">undefined</span>

- 对x作为正例可能性取对数得到线性回归模型
- x为正例的概率越大，几率取值就越大
- 线性回归模型输出结果去逼近（拟合）真实标记结果的对数几率
- 逻辑斯蒂回归函数被称为“对数几率回归（log-odds regression）"。



- 对数几率回归模型的输出y可作为将输入数据x分类为某一类别概率的大小。
- 输出值越接近1，说明输入数据x分类为该类别的可能性越大。与此相反，输出值越接近0，输入数据x不属于该类别的概率越大。
- 根据具体应用设置一个阈值，将大于该阀值的输入数据x都归属到某个类别，小于该阈值的输入数据x都归属到另外一个类别。

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/logistics2.jpg" alt="logistics2" style="zoom:67%;" /&gt;

★.从这里可以看出，logistic回归是一个线性模型。在预测时，可以通过计算线性函数wx+b取值是否大于0来判断输入数据x的类别归属。

#### 参数求解：最大似然函数

假定数据都是独立同分布的--&gt;最大似然函数==得到了--&gt;交叉熵(求解交叉熵，如果是线性模型可以用最小二乘法，但是logistics不行，可以考虑使用迭代算法——梯度下降法)



**多分类**——output的时候加个softmax，并做归一化，概率最大的那个类别就是分类结果

### [P197.2基于矩阵分解的潜在语义分析](https://www.bilibili.com/video/BV1c7411n7EY?p=19)——LSA

潜在语义分析（Latent Semantic Analysis，LSA或者Latent Semantic Indexing，LSI）是一种从海量文本数据中学习&lt;u&gt;单词-单词、单词-文档以及文档-文档之间&lt;/u&gt;隐性关系，进而得到文档和单词表达特征的方法。该方法的基本思想是综合考虑某些单词在哪些文档中同时出现，以此来决定该词语的含义与其他的词语的相似度。

潜在语义分析先构建一个单词-文档（term-document）矩阵A，进而寻找该矩阵的低秩逼近（low rank approximation）矩阵，从而来挖掘单词-单词、单词-文档以及文档·文档之间的关联关系。

举例

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/LSA.jpg" alt="LSA" style="zoom:67%;" /&gt;

- 当用户输入“optimization"这一检索请求，由于文档a3标题中不包含这一单词，则文档a3被认为是不相关文档，但实际上文档a3所涉及“minimization"内容与优化问题相关。出现这一问题是因为**单词-文档矩阵**只是刻画了&lt;u&gt;单词是否在文档中出现与否这一现象，而无法对单词-单词、单词-文档以及文档-文档之间语义关系进行建模。&lt;/u&gt;
- 如果用户检索“eat an apple"，则文档"Apple is a great company”会被检索出来，而实际上该文档中单词"Apple"所指苹果公司、而非水果，造成这一结果的原因是一些单词具有“一词多义”。
- 因此需要一种方法能够建模单词-单词、单词-文档以及文档-文档之间语义关系，解决包括“异词同义”和“一词多义”
  在内的诸多挑战。

#### 单词-文档矩阵（term-document）：构造与分解

&gt; 基于Latent Dirichlet Allocation（隐狄利克雷分配模型）

![LSA-TD](./《人工智能-模型与算法——浙江大学公开课》笔记/LSA-TD.jpg)

选取最大的前两个特征根及其对应的特征向量对矩阵A进行重建。下面给出了选取矩阵U、矩阵D和矩阵V的子部分重建所得矩阵A，效果如下：

- 回到之前举的一个例子，用户输入“optimization”来检索与之相关的文档。尽管单词“optimization"在文档a3中没有出现，但是在重建矩阵A2中，对应的位置被0.68取代，说明单词"optimization"对表征文档a3所蕴含内容具有重要作用，这也符合文档a3描述的minimization问题是一个optimization问题的事实。
- 在单词-矩阵A中，文档b3所对应network、gene和human三个单词取值为1，在重建矩阵A2中，network、gene和human三个单词取值分别为0.32、0.66和0.53。可见，network在表征文档b3时重要性降低，因为算法认为这一单词在机器学习所相关文档表达中更具有区别性。

为什么work？

通过单词-文档矩阵（term-document）的构造与分解，可以将每个单词映射到维度为R的隐性空间、将每个文档映射到维度为R的隐性空间：统一空间，隐性空间可视为“主题空间（topic），因此就可以比较两个单词、两篇文章的主题是否一致了。



### [P207.3线性区别分析及分类](https://www.bilibili.com/video/BV1c7411n7EY?p=20)

线性区别分析（linear discriminant analysis，LDA）是一种基于监督学习的降维方法，也称为Fisher线性区别分析（Fisher's Discriminant analysis，FDA）。
对于一组具有标签信息的高维数据样本，LDA利用其类别信息，将其线性投影到一个低维空间上，&lt;u&gt;在低维空间中同一类别样本尽可能靠近，不同类别样本尽可能彼此远离&lt;/u&gt;。==&gt;**为了获得“类内汇聚、类间间隔”的最佳投影结果**



&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/LDA.jpg" alt="LDA" style="zoom:67%;" /&gt;

#### PCA和LDA区别

- 主成分分析（PCA）是一种&lt;u&gt;无监督学习&lt;/u&gt;的降维方法（无需样本类别标签），线性区别分析（LDA）是一种&lt;u&gt;监督学习&lt;/u&gt;的降维方法（需要样本类别标签。PCA和LDA均是优化寻找一定特征向量w来实现降维，其中PCA寻找投影后&lt;u&gt;数据之间方差最大&lt;/u&gt;的投影方向、LDA寻找“&lt;u&gt;类内方差小、类间距离大&lt;/u&gt;”投影方向。
- PCA对高维数据降维后的维数是与原始数据特征维度相关（&lt;u&gt;与数据类别标签无关&lt;/u&gt;）。假设原始数据维度为d，那么PCA所得数据的降维维度可以为小于d的任意维度；LDA降维后所得到维度是与&lt;u&gt;数据样本的类别个数K有关&lt;/u&gt;（与数据本身维度无关）。假设原始数据一共有K个类别，那么LDA所得数据的降维维度小于或等于K-1。



## [P218.1深度学习基本概念](https://www.bilibili.com/video/BV1c7411n7EY?p=21)——深度学习

&gt; 浅层学习Versus深度学习：从分段学习到端到端学习；传统学习需要人工提取特征，深度神经网络可以通过层层网络自动提取特征。
&gt;
&gt; &lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/DL.jpg" alt="DL" style="zoom: 67%;" /&gt;



#### 神经元数学模型

1. 对相邻的前向神经元输入通过加权累加 : In <span class="mjpage">undefined</span>
2. 对累加结果进行非线性变换（通过激活函数）： <span class="mjpage">undefined</span>
3. 神经元的输出: <span class="mjpage">undefined</span>

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/神经元.jpg" alt="神经元" style="zoom:67%;" /&gt;

注：神经元越多，非线性表达能力越强，同时参数也很变得更多，网络也会变得越复杂





### [P228.2前馈神经网络](https://www.bilibili.com/video/BV1c7411n7EY?p=22)、[P238.3误差后向传播(BP)](https://www.bilibili.com/video/BV1c7411n7EY?p=23)

- 各个神经元&lt;u&gt;接受前一级的输入&lt;/u&gt;，&lt;u&gt;并输出到下一级&lt;/u&gt;，模型中没有反馈
- 层与层之间通过“&lt;u&gt;全连接&lt;/u&gt;”进行链接，即两个相邻层之间的神经元完全成对连接，但层内的神经元不相互连接。

#### 感知机网络

感知机网络（Perceptron Networks）是一种特殊的前馈神经网络：

- 无隐藏层，只有输入层/输出层
- 无法拟合复杂的数据

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/FNN-感知机.jpg" alt="FNN-感知机" style="zoom:67%;" /&gt;



#### 参数优化

##### 1.GD梯度下降法

梯度下降算法是一种使得损失函数最小化的方法。一元变量所构成函数f在x处梯度为：<span class="mjpage">undefined</span>

- 在多元函数中，梯度是对每一变量所求导数组成的向量
- 梯度的反方向是函数值下降最快的方向

##### 2.误差反向传播BP

- BP算法是一种将输出层误差反向传播给隐藏层进行参数更新的方法。=
- 将误差从后向前传递，将误差分摊给各层所有单元，从而获得各层单元所产生的误差，进而依据这个误差来让各层单元负起各自责任、修正各单元参数。









### [P249.1卷积神经网络](https://www.bilibili.com/video/BV1c7411n7EY?p=24)

#### 卷积

Q：什么是卷积操作？A：可以理解为是滤波器，图像经过特定卷积矩阵滤波后，所得到的卷积结果可认为是保留了像素点所构成的特定空间分布模式

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/卷积.jpg" alt="卷积" style="zoom:67%;" /&gt;





&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/卷积-神经元.jpg" alt="卷积-神经元" style="zoom:67%;" /&gt;

实际上卷积操作可以理解为是神经元的变形。



有一张32*32*3（RGB）的图像，使用5*5*3的卷积核Wi，步长为1对其进行卷积操作。卷积核W，在原始图像上从左到右、从上到下进行计算，改变5*5子块区域中的中心像素点值，得到28*28的特征图m1。

特征图：在深度学习里被定义为，原始图像经过卷积后得到的结果

▲卷积参数的确定都是通过数据驱动来确定的

#### 池化

对输入的特征图进行**下采样**，以在区域内获得最主要信息
常用的池化操作有：最大池化、平均池化

![平均池化](./《人工智能-模型与算法——浙江大学公开课》笔记/平均池化.jpg)

#### 全连接、输出层

将特征从卷积操作后向输出层映射，最后通过输出层进行输出

&lt;img src="./《人工智能-模型与算法——浙江大学公开课》笔记/全连接.jpg" alt="全连接" style="zoom:67%;" /&gt;

综上：所需学习参数：卷积核、全连接层权重、激活函数参数

AlexNet：经典用来分类（识别）图像的卷积神经网络，包含5个卷积层、3个全连接层，有六千多万个参数，最终吧一个RGB的图像转换成了一个4096维的特征向量，接着将这个特征向量输入给分类函数，最后输出一个一千维的向量，一千维向量每个维度上的值表示该图像被识别为该维度指代对象的概率大小。

### [P259.2-自然语言理解与视觉分析](https://www.bilibili.com/video/BV1c7411n7EY?p=25)



## [P2610.1强化学习定义](https://www.bilibili.com/video/BV1c7411n7EY?p=26)





### [P2710.2策略优化与策略评估](https://www.bilibili.com/video/BV1c7411n7EY?p=27)



[P2810.3强化学习求解QLearning](https://www.bilibili.com/video/BV1c7411n7EY?p=28)





### [P2910.4深度强化学习](https://www.bilibili.com/video/BV1c7411n7EY?p=29)





## [P3011.1博弈相关概念](https://www.bilibili.com/video/BV1c7411n7EY?p=30)——人工智能博弈



### [P3111.2遗憾最小化算法](https://www.bilibili.com/video/BV1c7411n7EY?p=31)



### [P3211.3虚拟遗憾最小化算法](https://www.bilibili.com/video/BV1c7411n7EY?p=32)</p>
</body></html></article><!-- lincense--><div class="license-wrapper"><p> <span>Author:  </span><a href="https://nymrli.top">Mrli</a></p><p> <span>Link:  </span><a href="https://nymrli.top/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/">https://nymrli.top/2020/12/08/《人工智能-模型与算法——浙江大学公开课》笔记/</a></p><p> <span>Copyright:  </span><span>All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/3.0">CC BY-NC-SA 3.0</a> unless stating additionally.</span></p></div><div class="post-paginator"><a class="prevSlogan" href="/2020/12/09/浙大2020春夏-人工智能习题3——图像恢复/" title="浙大2020春夏-人工智能习题3——图像恢复"><span>< PreviousPost</span><br><span class="prevTitle">浙大2020春夏-人工智能习题3——图像恢复</span></a><a class="nextSlogan" href="/2020/12/05/枚举类的优雅写法Java-Python/" title="枚举类的优雅写法Java-&gt;Python"><span>NextPost ></span><br><span class="nextTitle">枚举类的优雅写法Java-&gt;Python</span></a><div class="clear"></div></div><div id="comment"><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  id: '《人工智能:模型与算法——浙江大学公开课》笔记',
  owner: 'Freedomisgood',
  repo: 'Freedomisgood.github.io',
  oauth: {
    client_id: 'bc5a81fe36017dcd8b63',
    client_secret: '949cec3a1b91742c6249c47259791e4b80a6fa69',
  },
})
gitment.render('container')</script></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><p class="beian"><span>备案号:苏ICP备18015439号</span></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 60vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#p111可计算思想起源与发展"><span class="toc-number">1.</span> <span class="toc-text"> P11.1可计算思想起源与发展</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#p212人工智能的发展简史"><span class="toc-number">2.</span> <span class="toc-text"> P21.2人工智能的发展简史</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#p313人工智能研究的基本内容"><span class="toc-number">3.</span> <span class="toc-text"> P31.3人工智能研究的基本内容</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#授课基本内容"><span class="toc-number">3.1.</span> <span class="toc-text"> 授课基本内容：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#搜索求解"><span class="toc-number">4.</span> <span class="toc-text"> 搜索求解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#p421启发式搜索"><span class="toc-number">4.1.</span> <span class="toc-text"> P42.1启发式搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#p522对抗搜索"><span class="toc-number">4.2.</span> <span class="toc-text"> P52.2对抗搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#最大最小搜索"><span class="toc-number">4.2.1.</span> <span class="toc-text"> 最大最小搜索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#alpha-beta剪枝搜索"><span class="toc-number">4.2.2.</span> <span class="toc-text"> alpha-beta剪枝搜索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#p623蒙特卡洛树搜索"><span class="toc-number">4.2.3.</span> <span class="toc-text"> P62.3蒙特卡洛树搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#exploitation-component利用"><span class="toc-number">4.2.3.1.</span> <span class="toc-text"> exploitation component(利用)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#exploration-component探索"><span class="toc-number">4.2.3.2.</span> <span class="toc-text"> exploration component(探索)</span></a></li></ol></li></ol></li></ol></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>